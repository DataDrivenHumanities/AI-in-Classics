trigger: none
pr: none

variables:
  - group: latin-secrets
  - name: SCRAPER_PATH
    value: src/Lemmatizer-LTN/tools/scrape_tables.py
  - name: LEMMA_CSV
    value: src/Lemmatizer-LTN/out/lemmas.csv
  - name: FORM_CSV
    value: src/Lemmatizer-LTN/out/forms.csv

pool:
  name: Default
  # optional: ensure it lands on Windows
  # demands:
  #   - Agent.OS -equals Windows_NT

steps:
  # Use Python 3.11 (x64) on Windows
  - task: UsePythonVersion@0
    inputs:
      versionSpec: "3.11"
      addToPath: true

  # Create venv + install deps (PowerShell on Windows)
  - pwsh: |
      python -m venv .venv
      .\.venv\Scripts\Activate.ps1
      python -m pip install -U pip
      pip install -r etl/requirements.txt
    displayName: "Create venv + install deps"

  # Materialize Google SA JSON from base64 (PowerShell; cross-platform safe)
  - pwsh: |
      $bytes = [Convert]::FromBase64String($env:GOOGLE_SERVICE_ACCOUNT_JSON_BASE64)
      [IO.File]::WriteAllBytes("service_account.json", $bytes)
    displayName: "Materialize Google SA JSON"
    env:
      GOOGLE_SERVICE_ACCOUNT_JSON_BASE64: $(GOOGLE_SERVICE_ACCOUNT_JSON_BASE64)

  # Step 1: run scraper → CSVs
  - pwsh: |
      .\.venv\Scripts\Activate.ps1
      python "$(SCRAPER_PATH)" --lemmas-out "$(LEMMA_CSV)" --forms-out "$(FORM_CSV)"
    displayName: "Scrape → CSVs"

  # Step 2: upload CSVs to Drive
  - pwsh: |
      .\.venv\Scripts\Activate.ps1
      python etl/upload_to_drive.py `
        --service-account-json service_account.json `
        --folder-id "$(GOOGLE_DRIVE_FOLDER_ID)" `
        --files "$(LEMMA_CSV)" "$(FORM_CSV)"
    displayName: "Upload CSVs to Google Drive"
    env:
      GOOGLE_DRIVE_FOLDER_ID: $(GOOGLE_DRIVE_FOLDER_ID)

  # Step 3: init schema + load into Postgres (no sudo/apt; use psycopg2)
  # Assumes psycopg2 (or psycopg[binary]) is in etl/requirements.txt
  - pwsh: |
      .\.venv\Scripts\Activate.ps1
      python - << 'PY'
      import os, psycopg2
      from psycopg2.extras import execute_batch

      dsn = os.environ["DATABASE_URL"]
      sql_path = os.path.join("ops","init_db.sql")

      with open(sql_path, "r", encoding="utf-8") as f:
          init_sql = f.read()

      with psycopg2.connect(dsn) as conn:
          with conn.cursor() as cur:
              cur.execute(init_sql)
      print("Schema initialized.")

      # Now run your loader
      import subprocess, sys
      subprocess.check_call([
          sys.executable, "etl/load_to_postgres.py",
          "--db", dsn,
          "--lemmas", os.environ["LEMMA_CSV"],
          "--forms",  os.environ["FORM_CSV"],
      ])
      PY
    displayName: "Init schema + Load CSVs"
    env:
      DATABASE_URL: $(DATABASE_URL)
      LEMMA_CSV: $(LEMMA_CSV)
      FORM_CSV: $(FORM_CSV)
