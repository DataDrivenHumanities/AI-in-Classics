trigger: none
pr: none

variables:
  - group: latin-secrets
  - name: SCRAPER_PATH
    value: src/Lemmatizer-LTN/tools/scrape_tables.py
  - name: LEMMA_CSV
    value: src/Lemmatizer-LTN/out/lemmas.csv
  - name: FORM_CSV
    value: src/Lemmatizer-LTN/out/forms.csv

pool:
  name: Default

steps:
  # 1) Seed Python 3.11.9 into the agent tool cache from NuGet (no Store, no admin)
  - powershell: |
      $ErrorActionPreference = 'Stop'
      $version   = '3.11.9'
      $cacheRoot = Join-Path "$(Agent.ToolsDirectory)" "Python\$version"
      $toolsDir  = Join-Path $cacheRoot 'x64'

      if (-not (Test-Path (Join-Path $toolsDir 'python.exe'))) {
        New-Item -ItemType Directory -Force -Path $toolsDir | Out-Null
        $pkgUrl  = "https://www.nuget.org/api/v2/package/python/$version"
        $pkgFile = Join-Path $env:TEMP "python-$version.nupkg"
        $tmpDir  = Join-Path $env:TEMP "py-extract-$version"

        Invoke-WebRequest -Uri $pkgUrl -OutFile $pkgFile
        if (Test-Path $tmpDir) { Remove-Item -Recurse -Force $tmpDir }
        New-Item -ItemType Directory -Force -Path $tmpDir | Out-Null

        # Extract .nupkg without renaming
        Add-Type -AssemblyName System.IO.Compression.FileSystem
        [IO.Compression.ZipFile]::ExtractToDirectory($pkgFile, $tmpDir)

        Copy-Item -Path (Join-Path $tmpDir 'tools\*') -Destination $toolsDir -Recurse -Force

        # Mark the cache as ready for UsePythonVersion
        New-Item -ItemType File -Path (Join-Path $cacheRoot 'x64.complete') -Force | Out-Null
      }

      & "$toolsDir\python.exe" -V
      & "$toolsDir\python.exe" -m ensurepip --upgrade
      & "$toolsDir\python.exe" -m pip install -U pip
    displayName: "Seed Python 3.11.9 (NuGet) + ensurepip"

  # 2) Put that Python on PATH
  - task: UsePythonVersion@0
    displayName: "Use Python 3.11.9"
    inputs:
      versionSpec: "3.11.9"
      architecture: "x64"

  # 3) Create venv + deps (fix requirements path)
  - powershell: |
      $ErrorActionPreference = 'Stop'
      python -m venv .venv
      .\.venv\Scripts\python.exe -m pip install -U pip
      .\.venv\Scripts\python.exe -m pip install -r "$(Build.SourcesDirectory)\src\Lemmatizer-LTN\etl\requirements.txt"
    displayName: "Create venv + deps"

  # 4) Materialize Google SA JSON (unchanged)
  - powershell: |
      $ErrorActionPreference = 'Stop'
      $bytes = [Convert]::FromBase64String($env:GOOGLE_SERVICE_ACCOUNT_JSON_BASE64)
      [IO.File]::WriteAllBytes("service_account.json", $bytes)
    displayName: "Materialize Google SA JSON"
    env:
      GOOGLE_SERVICE_ACCOUNT_JSON_BASE64: $(GOOGLE_SERVICE_ACCOUNT_JSON_BASE64)

  # 5) Scrape → CSVs (ensure out/ exists; call script via absolute path)
  - powershell: |
      $ErrorActionPreference = 'Stop'
      $out = "$(Build.SourcesDirectory)\src\Lemmatizer-LTN\out"
      New-Item -ItemType Directory -Force -Path $out | Out-Null
      .\.venv\Scripts\python.exe "$(Build.SourcesDirectory)\src\Lemmatizer-LTN\tools\scrape_tables.py" `
        --outdir "$out" `
        --start 1 --step 1 --end 2 `
        --index-concurrency 4 --lemma-concurrency 8 --delay 0.05
    displayName: "Scrape → per-lemma CSVs (100 pages test)"
    timeoutInMinutes: 120

  # 6) Upload CSVs to Google Drive (fix script path)
  - powershell: |
      $ErrorActionPreference = 'Stop'
      .\.venv\Scripts\python.exe "$(Build.SourcesDirectory)\src\Lemmatizer-LTN\etl\upload_to_drive.py" `
        --service-account-json service_account.json `
        --folder-id "$(GOOGLE_DRIVE_FOLDER_ID)" `
        --files "$(Build.SourcesDirectory)\src\Lemmatizer-LTN\out"
    displayName: "Upload all CSVs under out/ to Google Drive"
    env:
      GOOGLE_DRIVE_FOLDER_ID: $(GOOGLE_DRIVE_FOLDER_ID)

  # 7) Init schema + Load CSVs (point to ops/init_db.sql under the subfolder)
  - powershell: |
      $ErrorActionPreference = 'Stop'
      $py = ".\.venv\Scripts\python.exe"
      $proj = "$(Build.SourcesDirectory)\src\Lemmatizer-LTN"
      $code = @'
      import csv, os, re, unicodedata, glob
      from urllib.parse import urlparse, parse_qs

      OUT_DIR   = os.path.join(r"$(Build.SourcesDirectory)", r"src\Lemmatizer-LTN\out")
      LEMMA_CSV = os.path.join(OUT_DIR, "lemmas.csv")
      FORM_CSV  = os.path.join(OUT_DIR, "forms.csv")

      def strip_accents(s):
          if not s: return ""
          nf = unicodedata.normalize("NFD", s)
          return "".join(ch for ch in nf if not unicodedata.combining(ch))

      def norm(s):
          s = strip_accents(s).lower()
          s = re.sub(r"[^a-z0-9]+", "", s)
          return s

      def lemma_code_from_url(u):
          try:
              q = parse_qs(urlparse(u).query)
              return (q.get("lemma") or [None])[0]
          except Exception:
              return None

      # Collect inputs
      paths = sorted(glob.glob(os.path.join(OUT_DIR, "*.csv")))
      # Exclude outputs if they exist (re-entrancy)
      paths = [p for p in paths if os.path.basename(p) not in ("lemmas.csv", "forms.csv")]

      lemmas = {}  # key: lemma_nod -> (lemma_code, lemma_nod, lemma_diac, pos, gender, page_url)
      forms  = []  # rows for forms.csv

      for p in paths:
          with open(p, newline="", encoding="utf-8") as f:
              r = csv.DictReader(f)
              rows = list(r)
              if not rows:
                  continue
              # Use the first row to get lemma-level attributes
              r0 = rows[0]
              lemma_text = r0.get("lemma_text","") or ""
              pos        = r0.get("pos","") or ""
              page_url   = r0.get("page_url","") or ""
              lnod       = norm(lemma_text)
              lcode      = lemma_code_from_url(page_url)
              # Keep first-seen; update sparsely if new info appears
              if lnod not in lemmas:
                  lemmas[lnod] = (lcode, lnod, lemma_text, pos, "", page_url)

              # Build forms rows: one row per table entry
              for rr in rows:
                  val = (rr.get("value") or "").strip()
                  if not val:
                      continue
                  label = rr.get("label","") or ""
                  # We’ll keep morphology mostly blank for now; label/contexts retained
                  forms.append((
                      lnod,                 # lemma_nod
                      norm(val),            # form_nod
                      val,                  # form_diac
                      label,                # label
                      "", "", "",           # mood, tense, voice
                      "", "", "",           # person, number, gender
                      "", "",               # case, degree
                      rr.get("page_url","") # page_url
                  ))

      # Write lemmas.csv
      os.makedirs(OUT_DIR, exist_ok=True)
      with open(LEMMA_CSV, "w", newline="", encoding="utf-8") as f:
          w = csv.writer(f)
          w.writerow(["lemma_code","lemma_nod","lemma_diac","pos","gender","page_url"])
          for tup in lemmas.values():
              w.writerow(tup)

      # Write forms.csv
      with open(FORM_CSV, "w", newline="", encoding="utf-8") as f:
          w = csv.writer(f)
          w.writerow(["lemma_nod","form_nod","form_diac","label",
                      "mood","tense","voice","person","number","gender","case","degree","page_url"])
          w.writerows(forms)

      print(f"Wrote {len(lemmas)} lemmas -> {LEMMA_CSV}")
      print(f"Wrote {len(forms)}  forms  -> {FORM_CSV}")
      '@
          Set-Content -Path aggregate_out_to_csvs.py -Value $code -Encoding UTF8
          & $py aggregate_out_to_csvs.py
    displayName: "Aggregate per-lemma CSVs → lemmas.csv + forms.csv"
