{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perseus Atlas Greek Lemmas and Conjugations Scraper\n",
    "\n",
    "This notebook scrapes Greek lemmas and their conjugated/inflected forms from the Perseus Atlas.\n",
    "It uses `requests` and `BeautifulSoup` to fetch paginated lemma lists and individual lemma pages.\n",
    "\n",
    "**References:**\n",
    "- Atlas Lemma List: https://atlas.perseus.tufts.edu/lemmas/?lang=grc\n",
    "- Individual lemma pages contain inflected forms and morphological data\n",
    "\n",
    "**Data Collected:**\n",
    "- Lemma headwords\n",
    "- Atlas IDs and URLs\n",
    "- Inflected forms (conjugations for verbs, declensions for nouns)\n",
    "- Morphological information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies into the active environment\n",
    "%pip -q install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Iterable, Optional, Dict\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "ATLAS_BASE = \"https://atlas.perseus.tufts.edu\"\n",
    "LEMMA_LIST_URL = f\"{ATLAS_BASE}/lemmas/?lang=grc\"\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'AI-in-Classics Atlas Scraper (+https://github.com/DataDrivenHumanities/AI-in-Classics)'\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Lemma:\n",
    "    lemma: str\n",
    "    atlas_id: str\n",
    "    url: str\n",
    "    source: str = 'Atlas'\n",
    "    language: str = 'grc'\n",
    "\n",
    "@dataclass\n",
    "class InflectedForm:\n",
    "    lemma: str\n",
    "    atlas_id: str\n",
    "    inflected_form: str\n",
    "    morphology: Optional[str] = None\n",
    "    frequency: Optional[int] = None\n",
    "\n",
    "def fetch(url: str, session: Optional[requests.Session] = None, timeout: int = 30) -> str:\n",
    "    \"\"\"Fetch URL with error handling.\"\"\"\n",
    "    s = session or requests.Session()\n",
    "    r = s.get(url, headers=HEADERS, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def to_abs(href: str) -> str:\n",
    "    \"\"\"Convert relative URL to absolute.\"\"\"\n",
    "    if href.startswith('http'):\n",
    "        return href\n",
    "    return urljoin(ATLAS_BASE, href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scrape Lemma List Pages\n",
    "\n",
    "The Atlas has paginated lemma lists. We'll scrape multiple pages to collect lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_lemma_list_page(page_num: int, session: requests.Session) -> List[Lemma]:\n",
    "    \"\"\"Scrape lemmas from a single page of the Atlas.\"\"\"\n",
    "    url = f\"{LEMMA_LIST_URL}&page={page_num}\"\n",
    "    print(f'Fetching page {page_num}: {url}')\n",
    "    \n",
    "    html = fetch(url, session=session)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    lemmas = []\n",
    "    \n",
    "    # Find all lemma links (format: /lemma/XXXXX/)\n",
    "    for link in soup.find_all('a', href=re.compile(r'/lemma/\\d+/')):\n",
    "        lemma_text = link.get_text(strip=True)\n",
    "        \n",
    "        # Skip navigation elements\n",
    "        if lemma_text in ['start', 'prev', 'next', 'end', '[', ']'] or not lemma_text:\n",
    "            continue\n",
    "        \n",
    "        href = link['href']\n",
    "        \n",
    "        # Extract Atlas ID from URL\n",
    "        match = re.search(r'/lemma/(\\d+)/', href)\n",
    "        if not match:\n",
    "            continue\n",
    "        \n",
    "        atlas_id = match.group(1)\n",
    "        lemma_url = to_abs(href)\n",
    "        \n",
    "        lemmas.append(Lemma(\n",
    "            lemma=lemma_text,\n",
    "            atlas_id=atlas_id,\n",
    "            url=lemma_url\n",
    "        ))\n",
    "    \n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape multiple pages\n",
    "session = requests.Session()\n",
    "\n",
    "# Configuration\n",
    "START_PAGE = 1\n",
    "NUM_PAGES = 5  # Change this to scrape more pages (total: ~19,673)\n",
    "DELAY = 0.5  # seconds between requests\n",
    "\n",
    "all_lemmas = []\n",
    "\n",
    "for page in range(START_PAGE, START_PAGE + NUM_PAGES):\n",
    "    try:\n",
    "        lemmas = scrape_lemma_list_page(page, session)\n",
    "        all_lemmas.extend(lemmas)\n",
    "        print(f'  -> Collected {len(lemmas)} lemmas (total: {len(all_lemmas)})')\n",
    "        time.sleep(DELAY)\n",
    "    except Exception as e:\n",
    "        print(f'ERROR on page {page}: {e}', file=sys.stderr)\n",
    "\n",
    "print(f'\\nTotal lemmas collected: {len(all_lemmas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview collected lemmas\n",
    "print(f'First 10 lemmas:')\n",
    "for lemma in all_lemmas[:10]:\n",
    "    print(f'  {lemma.lemma} (ID: {lemma.atlas_id})')\n",
    "\n",
    "print(f'\\nLast 10 lemmas:')\n",
    "for lemma in all_lemmas[-10:]:\n",
    "    print(f'  {lemma.lemma} (ID: {lemma.atlas_id})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scrape Individual Lemma Pages for Conjugations\n",
    "\n",
    "Now we'll visit individual lemma pages to extract inflected forms (conjugations/declensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_lemma_inflections(lemma: Lemma, session: requests.Session) -> List[InflectedForm]:\n",
    "    \"\"\"Scrape inflected forms from an individual lemma page.\"\"\"\n",
    "    try:\n",
    "        html = fetch(lemma.url, session=session)\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        inflections = []\n",
    "        seen_forms = set()  # Avoid duplicates\n",
    "        \n",
    "        # Look for links to form pages (format: /form/NUMBER/)\n",
    "        # These contain the inflected forms\n",
    "        for link in soup.find_all('a', href=re.compile(r'/form/\\d+/')):\n",
    "            form_text = link.get_text(strip=True)\n",
    "            \n",
    "            # Clean up the form text (remove brackets, special chars from OCR errors)\n",
    "            form_text = re.sub(r'[<>[\\]{}]', '', form_text)\n",
    "            form_text = form_text.strip()\n",
    "            \n",
    "            # Skip if empty, same as lemma, or already seen\n",
    "            if not form_text or form_text == lemma.lemma or form_text in seen_forms:\n",
    "                continue\n",
    "            \n",
    "            # Skip if it's just punctuation or numbers\n",
    "            if re.match(r'^[.,;:·\\[\\]\\d\\s]+$', form_text):\n",
    "                continue\n",
    "            \n",
    "            # Skip very short non-Greek forms (OCR errors)\n",
    "            if len(form_text) <= 2 and not re.search(r'[α-ωΑ-Ω]', form_text):\n",
    "                continue\n",
    "            \n",
    "            seen_forms.add(form_text)\n",
    "            \n",
    "            # Try to find morphological info nearby (in parent or sibling elements)\n",
    "            morph_info = None\n",
    "            parent = link.find_parent(['td', 'div', 'li'])\n",
    "            if parent:\n",
    "                # Look for morphology tags or description\n",
    "                morph_tag = parent.find(['span', 'small'], class_=re.compile(r'morph|parse|grammar'))\n",
    "                if morph_tag:\n",
    "                    morph_info = morph_tag.get_text(strip=True)\n",
    "            \n",
    "            inflections.append(InflectedForm(\n",
    "                lemma=lemma.lemma,\n",
    "                atlas_id=lemma.atlas_id,\n",
    "                inflected_form=form_text,\n",
    "                morphology=morph_info\n",
    "            ))\n",
    "        \n",
    "        return inflections\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'ERROR scraping inflections for {lemma.lemma}: {e}', file=sys.stderr)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape inflections for a sample of lemmas\n",
    "# (Set SAMPLE_SIZE to None to scrape all)\n",
    "SAMPLE_SIZE = 10  # Start with just 10 for testing\n",
    "INFLECTION_DELAY = 1.0  # Be more polite when fetching individual pages\n",
    "\n",
    "all_inflections = []\n",
    "sample_lemmas = all_lemmas[:SAMPLE_SIZE] if SAMPLE_SIZE else all_lemmas\n",
    "\n",
    "for i, lemma in enumerate(sample_lemmas, 1):\n",
    "    print(f'[{i}/{len(sample_lemmas)}] Scraping inflections for: {lemma.lemma}')\n",
    "    \n",
    "    inflections = scrape_lemma_inflections(lemma, session)\n",
    "    all_inflections.extend(inflections)\n",
    "    \n",
    "    print(f'  -> Found {len(inflections)} inflected forms')\n",
    "    \n",
    "    time.sleep(INFLECTION_DELAY)\n",
    "\n",
    "print(f'\\nTotal inflected forms collected: {len(all_inflections)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview inflections\n",
    "if all_inflections:\n",
    "    print('Sample inflected forms:')\n",
    "    for infl in all_inflections[:20]:\n",
    "        morph = f' [{infl.morphology}]' if infl.morphology else ''\n",
    "        print(f'  {infl.lemma} → {infl.inflected_form}{morph}')\n",
    "else:\n",
    "    print('No inflections found. The Atlas page structure may have changed.')\n",
    "    print('You may need to inspect the HTML manually and update the scraping logic.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lemmas_csv(path: str, items: Iterable[Lemma]) -> None:\n",
    "    \"\"\"Write lemmas to CSV.\"\"\"\n",
    "    with open(path, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['language', 'source', 'lemma', 'atlas_id', 'url'])\n",
    "        for it in items:\n",
    "            w.writerow([it.language, it.source, it.lemma, it.atlas_id, it.url])\n",
    "\n",
    "def write_inflections_csv(path: str, items: Iterable[InflectedForm]) -> None:\n",
    "    \"\"\"Write inflected forms to CSV.\"\"\"\n",
    "    with open(path, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['lemma', 'atlas_id', 'inflected_form', 'morphology', 'frequency'])\n",
    "        for it in items:\n",
    "            w.writerow([it.lemma, it.atlas_id, it.inflected_form, it.morphology or '', it.frequency or ''])\n",
    "\n",
    "def write_json(path: str, items: Iterable, item_type: str) -> None:\n",
    "    \"\"\"Write data to JSON.\"\"\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump([asdict(i) for i in items], f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lemmas\n",
    "LEMMAS_CSV = 'database/atlas_lemmas.csv'\n",
    "LEMMAS_JSON = 'database/atlas_lemmas.json'\n",
    "\n",
    "write_lemmas_csv(LEMMAS_CSV, all_lemmas)\n",
    "print(f'✓ Wrote {len(all_lemmas)} lemmas to {LEMMAS_CSV}')\n",
    "\n",
    "write_json(LEMMAS_JSON, all_lemmas, 'lemmas')\n",
    "print(f'✓ Wrote {len(all_lemmas)} lemmas to {LEMMAS_JSON}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inflections\n",
    "if all_inflections:\n",
    "    INFLECTIONS_CSV = 'database/atlas_inflections.csv'\n",
    "    INFLECTIONS_JSON = 'database/atlas_inflections.json'\n",
    "    \n",
    "    write_inflections_csv(INFLECTIONS_CSV, all_inflections)\n",
    "    print(f'✓ Wrote {len(all_inflections)} inflected forms to {INFLECTIONS_CSV}')\n",
    "    \n",
    "    write_json(INFLECTIONS_JSON, all_inflections, 'inflections')\n",
    "    print(f'✓ Wrote {len(all_inflections)} inflected forms to {INFLECTIONS_JSON}')\n",
    "else:\n",
    "    print('⚠ No inflections to save')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create DataFrame for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "lemmas_df = pd.DataFrame([asdict(l) for l in all_lemmas])\n",
    "print(f'Lemmas DataFrame:')\n",
    "print(lemmas_df.head(10))\n",
    "print(f'\\nShape: {lemmas_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_inflections:\n",
    "    inflections_df = pd.DataFrame([asdict(i) for i in all_inflections])\n",
    "    print(f'Inflections DataFrame:')\n",
    "    print(inflections_df.head(10))\n",
    "    print(f'\\nShape: {inflections_df.shape}')\n",
    "    \n",
    "    # Show summary by lemma\n",
    "    print(f'\\nInflections per lemma:')\n",
    "    print(inflections_df.groupby('lemma').size().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('SCRAPING SUMMARY')\n",
    "print('='*60)\n",
    "print(f'Pages scraped: {NUM_PAGES}')\n",
    "print(f'Total lemmas collected: {len(all_lemmas)}')\n",
    "print(f'Lemmas with inflections checked: {len(sample_lemmas)}')\n",
    "print(f'Total inflected forms found: {len(all_inflections)}')\n",
    "if all_inflections:\n",
    "    print(f'Average forms per lemma: {len(all_inflections) / len(sample_lemmas):.1f}')\n",
    "print('\\nOutput files:')\n",
    "print(f'  - {LEMMAS_CSV}')\n",
    "print(f'  - {LEMMAS_JSON}')\n",
    "if all_inflections:\n",
    "    print(f'  - {INFLECTIONS_CSV}')\n",
    "    print(f'  - {INFLECTIONS_JSON}')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
