{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSJ Greek Headwords (Lemmas) Scraper\n",
    "\n",
    "This notebook scrapes Greek lexicon headwords (lemmas) from Perseus (LSJ: Liddell–Scott–Jones).\n",
    "It uses `requests` and `BeautifulSoup` to fetch LSJ index/letter pages, discover entry-group pages,\n",
    "and extract headwords from LSJ entry links. Results can be saved to CSV or JSON.\n",
    "\n",
    "References:\n",
    "- LSJ Index: https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0057\n",
    "- Example entry-group page: https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0057:alphabetic+letter=*a:entry+group=221\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies into the active environment\n",
    "%pip -q install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Iterable, Optional\n",
    "from urllib.parse import unquote\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "BASE_INDEX_URL = \"https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0057\"\n",
    "# Simple substring targets (after URL-decoding)\n",
    "ENTRY_GROUP_SUBSTR = \"Perseus:text:1999.04.0057:alphabetic+letter=\"\n",
    "ENTRY_GROUP_AND = \":entry+group=\"\n",
    "ENTRY_PAGE_SUBSTR = \"Perseus:text:1999.04.0057:entry=\"\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'AI-in-Classics LSJ Notebook Scraper (+https://github.com/ahulloli/AI-in-Classics)'\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Headword:\n",
    "    lemma: str\n",
    "    url: str\n",
    "    source: str = 'LSJ'\n",
    "    language: str = 'grc'\n",
    "\n",
    "def fetch(url: str, session: Optional[requests.Session] = None, timeout: int = 45) -> str:\n",
    "    s = session or requests.Session()\n",
    "    r = s.get(url, headers=HEADERS, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def to_abs(href: str) -> str:\n",
    "    if href.startswith('/hopper/'):\n",
    "        return f'https://www.perseus.tufts.edu{href}'\n",
    "    if href.startswith('http'):\n",
    "        return href\n",
    "    return f'https://www.perseus.tufts.edu/hopper/{href.lstrip('/')}'\n",
    "\n",
    "def extract_entry_group_links(html: str) -> List[str]:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links: List[str] = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        href_dec = unquote(href)\n",
    "        if (ENTRY_GROUP_SUBSTR in href_dec) and (ENTRY_GROUP_AND in href_dec):\n",
    "            links.append(to_abs(href))\n",
    "    # dedupe preserve order\n",
    "    seen = set()\n",
    "    uniq = []\n",
    "    for u in links:\n",
    "        if u not in seen:\n",
    "            seen.add(u)\n",
    "            uniq.append(u)\n",
    "    return uniq\n",
    "\n",
    "def generate_letter_urls() -> List[str]:\n",
    "    letters = ['*a','*b','*g','*d','*e','*z','*h','*q','*i','*k','*l','*m',\n",
    "               '*n','*c','*o','*p','*r','*s','*t','*u','*f','*x','*y','*w']\n",
    "    return [f\"https://www.perseus.tufts.edu/hopper/text?doc=Perseus:text:1999.04.0057:alphabetic+letter={ltr}\" for ltr in letters]\n",
    "\n",
    "def extract_headwords_from_entry_group(html: str) -> List[Headword]:\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    out: List[Headword] = []\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        href_dec = unquote(href)\n",
    "        if ENTRY_PAGE_SUBSTR in href_dec:\n",
    "            text = a.get_text(strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            out.append(Headword(lemma=text, url=to_abs(href)))\n",
    "    return out\n",
    "\n",
    "def write_csv(path: str, items: Iterable[Headword]) -> None:\n",
    "    with open(path, 'w', newline='', encoding='utf-8') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(['language','source','lemma','url'])\n",
    "        for it in items:\n",
    "            w.writerow([it.language, it.source, it.lemma, it.url])\n",
    "\n",
    "def write_json(path: str, items: Iterable[Headword]) -> None:\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump([asdict(i) for i in items], f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect LSJ entry-group links via letter pages\n",
    "We generate letter pages deterministically and extract entry-group links from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "\n",
    "# Fetch index (optional; we don't rely on it, but useful to ensure connectivity)\n",
    "_ = fetch(BASE_INDEX_URL, session=session)\n",
    "\n",
    "letter_urls = generate_letter_urls()\n",
    "print(f'Letters: {len(letter_urls)}')\n",
    "\n",
    "entry_groups = []\n",
    "for i, u in enumerate(letter_urls, 1):\n",
    "    try:\n",
    "        html = fetch(u, session=session)\n",
    "        groups = extract_entry_group_links(html)\n",
    "        print(f'[{i}/{len(letter_urls)}] {u} -> {len(groups)} groups')\n",
    "        for g in groups:\n",
    "            if g not in entry_groups:\n",
    "                entry_groups.append(g)\n",
    "        time.sleep(0.4)  # polite delay\n",
    "    except Exception as e:\n",
    "        print('WARN letter', u, e, file=sys.stderr)\n",
    "\n",
    "print('Total entry groups collected:', len(entry_groups))\n",
    "entry_groups[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract headwords from entry-group pages\n",
    "This step visits each entry-group and collects headword links (lemmas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headwords: List[Headword] = []\n",
    "for i, g in enumerate(entry_groups, 1):\n",
    "    try:\n",
    "        html = fetch(g, session=session)\n",
    "        items = extract_headwords_from_entry_group(html)\n",
    "        headwords.extend(items)\n",
    "        if i % 10 == 0:\n",
    "            print(f'Processed {i}/{len(entry_groups)} groups; total headwords so far: {len(headwords)}')\n",
    "        time.sleep(0.4)\n",
    "    except Exception as e:\n",
    "        print('WARN group', g, e, file=sys.stderr)\n",
    "\n",
    "# Deduplicate by lemma (keep first)\n",
    "seen = set()\n",
    "unique_hw: List[Headword] = []\n",
    "for hw in headwords:\n",
    "    if hw.lemma in seen:\n",
    "        continue\n",
    "    seen.add(hw.lemma)\n",
    "    unique_hw.append(hw)\n",
    "\n",
    "len(unique_hw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to CSV (and optionally JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_CSV = 'src/Lemmatizer-GRK/lsj_headwords.csv'\n",
    "OUT_JSON = 'src/Lemmatizer-GRK/lsj_headwords.json'\n",
    "\n",
    "write_csv(OUT_CSV, unique_hw)\n",
    "print('Wrote CSV:', OUT_CSV)\n",
    "# Optionally also write JSON\n",
    "# write_json(OUT_JSON, unique_hw)\n",
    "# print('Wrote JSON:', OUT_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview a few headwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(hw.lemma, hw.url) for hw in unique_hw[:15]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
