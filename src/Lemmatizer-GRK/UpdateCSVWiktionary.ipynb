{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_variations(word,language='ancient greek',show=False):\n",
    "        # CREATES A FOLDER STRUCTURE THAT CONTAINS WORDS ORGANIZED BY PART OF SPEECH\n",
    "        #DECLINES WORDS\n",
    "        #test words\n",
    "        #'á¿¬Ï‰Î¼Î±á¿–Î¿Ï‚' romans\n",
    "        #'Î¼Î¹ÏƒÏŽ' hate\n",
    "        #'Îµá¼°Î¼Î¯' are to be\n",
    "        #word = word.lower()\n",
    "\n",
    "        #https://en.wiktionary.org/wiki/á¿¬Ï‰Î¼Î±á¿–Î¿Ï‚\n",
    "        url= requests.get(f'https://en.wiktionary.org/wiki/{quote(word)}')\n",
    "        soup = bs(url.text, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #remove unwanted classes\n",
    "        for tag in soup.find_all('span',{'class':'tr Latn'}):\n",
    "                tag.decompose()\n",
    "        for tag in soup.find_all('table',{'class':'audiotable'}):\n",
    "                tag.decompose()\n",
    "        for tag in soup.find_all('sup'):\n",
    "                tag.decompose()\n",
    "\n",
    "        #HEADERS ---------------------------\n",
    "        #find all tables\n",
    "        tables =soup.find_all('table')\n",
    "        part_of_speech_h3 =[]\n",
    "        h4 =[] # inflection/declension\n",
    "        nav=[] #verb tense, noun first second third delcension\n",
    "        gender =None\n",
    "\n",
    "        #DATAFRAMES----------------------------------\n",
    "\n",
    "        dataframes = pd.read_html(soup.prettify())\n",
    "        for i,df in enumerate(dataframes):\n",
    "                \n",
    "                header = tables[i].find_previous('h2').text.replace('[edit]','').lower().strip()\n",
    "\n",
    "                #GENDER\n",
    "                gender = tables[i].find_previous(class_=\"gender\")\n",
    "                if(gender != None):\n",
    "                        gender = gender.find_next(\"abbr\").text\n",
    "                elif (gender ==None):\n",
    "                        gender = \"n\"\n",
    "\n",
    "\n",
    "                #if(header =='greek'):\n",
    "                        # if modern greek ignore\n",
    "                        #header = ''\n",
    "                #if(header =='ancient greek'):\n",
    "                        #header = 'greek'\n",
    "                if(header != language):\n",
    "                        part_of_speech_h3.append('')\n",
    "                        h4.append('')\n",
    "                        nav.append('')\n",
    "                        continue\n",
    "                if(tables[i].find_previous('h3')):\n",
    "                        part_of_speech_h3.append(tables[i].find_previous('h3').text\n",
    "                        .replace('[edit]','')\n",
    "                        .replace('\\\"','').lower().strip())\n",
    "                else:\n",
    "                        part_of_speech_h3.append('')\n",
    "                if(tables[i].find_previous('h4')):\n",
    "                        h4.append(tables[i].find_previous('h4').text\n",
    "                        .replace('[edit]','')\n",
    "                        .replace('\\\"','').lower().strip())\n",
    "                else:\n",
    "                        h4.append('')\n",
    "                if(tables[i].find_previous('div',{'class':'NavHead'})):\n",
    "                        nav.append(tables[i].find_previous('div',{'class':'NavHead'}).text\n",
    "                        .replace('hide â–²','')\n",
    "                        .replace('\\\"','').replace('/','-').replace(':','-').strip().lower())\n",
    "                        #  : / mess up the file path so you need to remove them\n",
    "                else:\n",
    "                        nav.append('')\n",
    "                ##display(df)\n",
    "                ##CREATE A FOLDER\n",
    "                root_dir = f'{header.replace(\" \", \"_\")}_variations'\n",
    "                os.makedirs(f'{root_dir}/{part_of_speech_h3[i]}/{word.lower()}/',exist_ok=True)\n",
    "                print(f'{root_dir}/{part_of_speech_h3[i]}/{word}/')\n",
    "                ##os.makedirs(nav[i],exist_ok=True)\n",
    "\n",
    "                #print(h3[i])\n",
    "                #print(h4[i])\n",
    "                #print(nav[i])\n",
    "                #print('WRITE TO FILE--------------------------------')\n",
    "\n",
    "                df=dataframes[i]\n",
    "\n",
    "             \n",
    " \n",
    "                # for appending df2 at the end of df1\n",
    "                df.loc[len(df)]= gender\n",
    "                df.iat[-1,0] = \"gender\"\n",
    "\n",
    "                df=df.dropna(how='all',axis=0)\n",
    "\n",
    "\n",
    "\n",
    "                df=df.replace({'âž¤':''}, regex=True)\n",
    "                df=df.replace({'â€¢':''}, regex=True)\n",
    "                df=df.replace({'\\[':'','\\]':'','\\{':'','\\}':''}, regex=True)\n",
    "                df=df.replace({'â€”':' '}, regex=True)\n",
    "                #df=df.replace({'/':' ',r'\\\\':' '}, regex=True)\n",
    "                df=df.replace(r\"\\([^)]*\\)\",\"\",regex=True)\n",
    "\n",
    "\n",
    "\n",
    "                # make lowercase\n",
    "                df = df.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "                #df = df.applymap(lambda s:strip_accents_and_lowercase(s) if type(s) == str else s)\n",
    "                df = df.applymap(lambda s: s.split(',', 1)[0] if type(s) == str else s)\n",
    "                df = df.applymap(lambda s: s.split('-', 1)[0] if type(s) == str else s)\n",
    "                df = df.applymap(lambda s: s.split('+', 1)[0] if type(s) == str else s)\n",
    "                df = df.applymap(lambda s: s.split('/', 1)[0] if type(s) == str else s)\n",
    "                df = df.applymap(lambda s: s.split('\\\\', 1)[0] if type(s) == str else s)\n",
    "\n",
    "                df= df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "                df = df.replace(r\"\\s\\s+\",\" \",regex=True)\n",
    "              \n",
    "                # NAN ARE REPALCED BY SPACE CHARACTER\n",
    "                df.to_csv(f'{root_dir}/{part_of_speech_h3[i]}/{word.lower()}/{nav[i]}_{h4[i]}.csv',index=False)\n",
    "\n",
    "                return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreekWord:\n",
    "    def __init__(self, id, text, bare_text, sequence_num, morph_code, base_form, bare_base_form, definition):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.bare_text = bare_text\n",
    "        self.sequence_num = sequence_num\n",
    "        self.morph_code = morph_code\n",
    "        self.base_form = base_form\n",
    "        self.bare_base_form = bare_base_form\n",
    "        self.definition = definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_forms(word):\n",
    "    generate_variations(word) #generates the csv table dataframe from wiktionary\n",
    "    #1. read the csv\n",
    "    #2. take in block of all with base word\n",
    "    #3. load all words into custom object, hashmap from morph code to object\n",
    "    #4. scrape the chart, take the chart and figure out what morph codes are missing\n",
    "    #5. generate object with all the words from the missing morph codes\n",
    "    #6. edit csv in place or edit to dataframe to spit into csv\n",
    "    #7. termination function that stores current location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def update_csv_with_wiktionary(csv_path, base_word):\n",
    "    # 1: Read the CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # 2: Scrape the chart to obtain all morph codes\n",
    "    scraped_dataframe = generate_variations(base_word)  # Assuming generate_variations returns the DataFrame\n",
    "    all_morph_codes = generate_all_morph_codes(base_word, csv_path, scraped_dataframe)\n",
    "\n",
    "    # 3: Take in block of all words with the base word\n",
    "    base_word_block = df[df['base_form'] == base_word]\n",
    "\n",
    "    # 4: Load all words into custom object (hashmap from morph code to object)\n",
    "    greek_word = {}\n",
    "    for index, row in base_word_block.iterrows():\n",
    "        morph_code = row['morph_code']\n",
    "        word = GreekWord(\n",
    "            id=row['id'],\n",
    "            text=row['text'],\n",
    "            bare_text=row['bare_text'],\n",
    "            sequence_num=row['sequence_num'],\n",
    "            morph_code=row['morph_code'],\n",
    "            base_form=row['base_form'],\n",
    "            bare_base_form=row['bare_base_form'],\n",
    "            definition=row['definition']\n",
    "        )\n",
    "        greek_word[morph_code] = word\n",
    "\n",
    "    # 5: Determine missing morph codes\n",
    "    missing_morph_codes = []\n",
    "    for morph_code in all_morph_codes:\n",
    "        if morph_code not in greek_word:\n",
    "            missing_morph_codes.append(morph_code)\n",
    "\n",
    "    # 6: Generate object with all words from the missing morph codes\n",
    "    missing_words = []\n",
    "    for morph_code in missing_morph_codes:\n",
    "        # Call generate_variations to get word details from Wiktionary\n",
    "        word_details_from_wiktionary = generate_variations(base_word, morph_code)\n",
    "        word = GreekWord(\n",
    "            id=word_details_from_wiktionary['id'],\n",
    "            text=word_details_from_wiktionary['text'],\n",
    "            bare_text=word_details_from_wiktionary['bare_text'],\n",
    "            sequence_num=word_details_from_wiktionary['sequence_num'],\n",
    "            morph_code=morph_code,\n",
    "            base_form=word_details_from_wiktionary['base_form'],\n",
    "            bare_base_form=word_details_from_wiktionary['bare_base_form'],\n",
    "            definition=word_details_from_wiktionary['definition']\n",
    "        )\n",
    "        greek_word[morph_code] = word\n",
    "        missing_words.append(word)\n",
    "\n",
    "    # 7: Edit DataFrame to export to CSV\n",
    "    for word in missing_words:\n",
    "        df = df.append(word.__dict__, ignore_index=True)\n",
    "\n",
    "    # 8: Termination function that stores current location\n",
    "    updated_csv_path = csv_path.replace('.csv', '_updated.csv')\n",
    "    df.to_csv(updated_csv_path, index=False)\n",
    "\n",
    "    return updated_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_morph_codes(word, original_csv_path, dataframe):\n",
    "    # Read the original CSV containing Greek forms\n",
    "    original_df = pd.read_csv(original_csv_path)\n",
    "\n",
    "    # Extract all Greek forms from the DataFrame\n",
    "    greek_forms = set(original_df['text'])\n",
    "\n",
    "    # Compare with forms of the base word in the generated DataFrame\n",
    "    base_word_forms = set(dataframe['text'])\n",
    "    missing_forms = greek_forms - base_word_forms\n",
    "\n",
    "    # Generate new entries for missing forms\n",
    "    new_entries = []\n",
    "    for form in missing_forms:\n",
    "        # Retrieve the details of the missing form from the original CSV\n",
    "        missing_entry = original_df[original_df['text'] == form].iloc[0]\n",
    "\n",
    "        # Generate morph code based on the format provided in the chart\n",
    "        morph_code = generate_morph_code(dataframe, form)\n",
    "\n",
    "        new_entry = {\n",
    "            'id': '',  # You may generate an appropriate ID here if needed\n",
    "            'text': missing_entry['text'],\n",
    "            'bare_text': missing_entry['bare_text'],\n",
    "            'sequence_num': missing_entry['sequence_num'],\n",
    "            'morph_code': morph_code,\n",
    "            'base_form': word,\n",
    "            'bare_base_form': missing_entry['bare_base_form'],\n",
    "            'definition': missing_entry['definition']\n",
    "        }\n",
    "        new_entries.append(new_entry)\n",
    "\n",
    "    return new_entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_morph_code(dataframe, form):\n",
    "    # Initialize variables to store morph code components\n",
    "    part_of_speech = ''  # First letter of morph code\n",
    "    number = ''           # Third letter of morph code\n",
    "    gender = ''           # Seventh letter of morph code\n",
    "    case = ''             # Eighth letter of morph code\n",
    "\n",
    "    # Iterate through the DataFrame rows to find the form and extract its morph code components\n",
    "    for index, row in dataframe.iterrows():\n",
    "        # Check if the current row contains the form we are looking for\n",
    "        if row['text'] == form:\n",
    "            # Extract morph code components based on the row's position and content\n",
    "            # For example, if it's an adjective in singular masculine nominative case, set the corresponding variables\n",
    "            if row['1'] == 'adjective':\n",
    "                part_of_speech = 'a'  # Assuming 'a' for adjective\n",
    "                # Check the number (singular, dual, plural) and set the number variable accordingly\n",
    "                # For example:\n",
    "                # if row['2'] == 'singular':\n",
    "                #     number = 's'\n",
    "                # elif row['2'] == 'dual':\n",
    "                #     number = 'd'\n",
    "                # else:\n",
    "                #     number = 'p'\n",
    "                # Similarly, check other columns (gender, case) and set the corresponding variables\n",
    "                # For example:\n",
    "                # if row['7'] == 'masculine':\n",
    "                #     gender = 'm'\n",
    "                # elif row['7'] == 'feminine':\n",
    "                #     gender = 'f'\n",
    "                # else:\n",
    "                #     gender = 'n'\n",
    "                # Similarly for the case:\n",
    "                # if row['8'] == 'nominative':\n",
    "                #     case = 'n'\n",
    "                # elif row['8'] == 'genitive':\n",
    "                #     case = 'g'\n",
    "                # and so on...\n",
    "            # Add other conditions for different parts of speech and their variations\n",
    "\n",
    "    # Combine morph code components to form the complete morph code\n",
    "    morph_code = part_of_speech + '-' + number + '---' + gender + case + '-'\n",
    "\n",
    "    return morph_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: Compile all of the morph codes of a single word from the Perseus database\n",
    "# Then take this list of morph codes and compare it to a list of generate morph codes\n",
    "# For the same word in the Wiktionary database. If the lists don't match, then we are missing forms\n",
    "# and we can add them from the Wiktionary database\n",
    "\n",
    "# Things to do:\n",
    "# Get all forms of words of all base words from Wiktionary using generate_variations function\n",
    "# 1. Function to scrape Wiktionary for variations of a word\n",
    "# 2. Function to add variations to a CSV file (Wiktionary Dataframe)\n",
    "# 3. Function to compare get_perseus_morphcodes and get_wiktionary_morphcodes\n",
    "# 4. Function to add missing forms to Perseus Dataframe \n",
    "# 5. Celebrate ðŸŽ‰\n",
    "\n",
    "# Wiktionary CSV\n",
    "\n",
    "\n",
    "\n",
    "# Function to get all morph codes for a given word from the Perseus database\n",
    "def get_perseus_morphcodes(perseus_df_path, base_word):\n",
    "\n",
    "    # Read the Perseus CSV file\n",
    "    perseus_df = pd.read_csv(perseus_df_path)\n",
    "\n",
    "    morph_codes = []\n",
    "    # Extract all morph codes for the given word\n",
    "    for index, row in perseus_df.iterrows():\n",
    "        if row['base_form'] == base_word:\n",
    "            morph_codes.append(row['morph_code'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all morph codes for a given word from the Wiktionary database\n",
    "def get_wiktionary_morphcodes(wiktionary_df_path, base_word):\n",
    "    # Read the Wiktionary DataFrame\n",
    "    wiktionary_df = pd.read_csv(wiktionary_df_path)\n",
    "\n",
    "    morph_codes = []\n",
    "    # Extract all morph codes for the given word\n",
    "    for index, row in wiktionary_df.iterrows():\n",
    "        if row['base_form'] == base_word:\n",
    "            morph_codes.append(row['morph_code'])\n",
    "\n",
    "    return morph_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gen_varitions(base_word):\n",
    "    # Jacob and Guhan\n",
    "    # Scrapes the wiktionary page for the word and all of its variations\n",
    "\n",
    "    # I'm thinking we put this in a map? Where the key is the plurality/grammatical case and the value is the form\n",
    "    # Ex. Singular Nominative = SN -> map['SN'] = 'Singualar Nominative Form'\n",
    "\n",
    "    # Set up the url to scrape\n",
    "    url= requests.get(f'https://en.wiktionary.org/wiki/{quote(word)}')\n",
    "    soup = bs(url.text, 'html.parser')\n",
    "\n",
    "    # Initialize storage map\n",
    "    variations = {}\n",
    "\n",
    "    # Remove unwanted html classes and stuff\n",
    "    for tag in soup.find_all('span',{'class':'tr Latn'}):\n",
    "            tag.decompose()\n",
    "    for tag in soup.find_all('table',{'class':'audiotable'}):\n",
    "            tag.decompose()\n",
    "    for tag in soup.find_all('sup'):\n",
    "            tag.decompose()\n",
    "\n",
    "    tables = soup.find_all('table') \n",
    "    part_of_speech_h3 = [] # Stores part of speech headers (noun, verb, etc.)\n",
    "    declension_h4 = [] # Stores inflection/declension headers (first, second, etc.)\n",
    "    nav = [] # Stores something idk it was in his code\n",
    "    gender = None # Init gender by default to None (gonna have it check for a neutral case, then check for f and m) (Also no clue if we need this his code is pretty messy lol)\n",
    "\n",
    "    # Not sure about this yet still working on it\n",
    "    #for i, table in enumerate(tables):\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the currentDataFrame header  Index(['id', 'text', 'bare_text', 'sequence_num', 'morph_code', 'base_form',\n",
      "       'bare_base_form', 'definition'],\n",
      "      dtype='object')\n",
      "Data has been transferred to the greek_dictionary/temp_output.csv\n"
     ]
    }
   ],
   "source": [
    "# possible variation w/ input path and output file path \n",
    "# takes in all of greek_words.csv file and returns them to the output path \n",
    "# iterate throught the index, row in the current Dataframe (in this case it would be greek_words.csv)\n",
    "# and add them to the csv superset \n",
    "# %pip install pandas\n",
    "# ignore, issues w/ local system\n",
    "import pandas as pd\n",
    "def add_variations_wiktionaryDataframes(csv_input_path, csv_output_path):\n",
    "    # Johnny\n",
    "\n",
    "    try: \n",
    "        # input csv read in \n",
    "        currDataframe = pd.read_csv(csv_input_path, header=0, on_bad_lines='skip', engine='python')\n",
    "        currDataframe.columns = currDataframe.columns.str.strip()  # get rid of extra whitespacing issues like \\t or bnrt\n",
    "        print(\"this is the currentDataFrame header \", currDataframe.columns)\n",
    "        # new df for output w same structure as original greek_words.csv file\n",
    "        outputDataframe = pd.DataFrame(columns=['id', 'text', 'bare_text', 'sequence_num', 'morph_code',\n",
    "                                                'base_form', 'bare_base_form', 'definition'])\n",
    "\n",
    "        # just move the row from the greek_words.csv to the output df\n",
    "        for index, row in currDataframe.iterrows():\n",
    "            newRow = pd.DataFrame([{\n",
    "                'id': row['id'],\n",
    "                'text': row['text'],\n",
    "                'bare_text': row['bare_text'],\n",
    "                'sequence_num': row['sequence_num'],\n",
    "                'morph_code': row['morph_code'],\n",
    "                'base_form': row['base_form'],\n",
    "                'bare_base_form': row['bare_base_form'],\n",
    "                'definition': row['definition']\n",
    "            }])\n",
    "            # append the new row to the output df\n",
    "            # append became deprecated \n",
    "            outputDataframe = pd.concat([outputDataframe, newRow], ignore_index=True)\n",
    "\n",
    "        # write the output df to the new csv file\n",
    "        outputDataframe.to_csv(csv_output_path, index=False)\n",
    "        print(f\"Data has been transferred to the {csv_output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred: \", e)\n",
    "\n",
    "\n",
    "\n",
    "inputPath = \"greek_dictionary/temp_input.csv\"\n",
    "outputPath = \"greek_dictionary/temp_output.csv\"\n",
    "\n",
    "add_variations_wiktionaryDataframes(inputPath, outputPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_morph_codes(perseus_morph_codes, wiktionary_morph_codes, return_indices=False):\n",
    "    # returns values by default but returns indices if specified\n",
    "    # Krish - compare and find differences\n",
    "    if return_indices:\n",
    "        # find indices unique to perseus list\n",
    "        unique_to_perseus = [i for i, x in enumerate(perseus_morph_codes) if x not in wiktionary_morph_codes]\n",
    "        # find indices unique to wiktionary list\n",
    "        unique_to_wiktionary = [i for i, x in enumerate(wiktionary_morph_codes) if x not in perseus_morph_codes]\n",
    "    else:\n",
    "        # find values unique to perseus list\n",
    "        unique_to_perseus = [x for x in perseus_morph_codes if x not in wiktionary_morph_codes]\n",
    "        # find values unique to wiktionary list\n",
    "        unique_to_wiktionary = [x for x in wiktionary_morph_codes if x not in perseus_morph_codes]\n",
    "    result = {\"perseus\" : unique_to_perseus, \"wiktionary\" : unique_to_wiktionary}\n",
    "    # returns a dictionary with unique values/indices in each respective list\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
