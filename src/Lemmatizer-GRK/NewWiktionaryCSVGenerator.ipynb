{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from io import StringIO\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    \"\"\"Remove accents from a given string.\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def generate_morph_code(part_of_speech, person, number, tense, mood, voice, gender, case, degree):\n",
    "    \"\"\"\n",
    "    Generate a morph code based on grammatical features according to the 9-position key.\n",
    "    \"\"\"\n",
    "    morph_code = f\"{part_of_speech}{person}{number}{tense}{mood}{voice}{gender}{case}{degree}\"\n",
    "    return morph_code\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"Normalize a Greek word by removing accents and converting to lowercase.\"\"\"\n",
    "    return strip_accents(word).lower()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_noun_table(df, base_word, gender, sequence_num_start, definition):\n",
    "    \"\"\"Parse the noun inflection table and extract forms along with their grammatical features.\"\"\"\n",
    "    # Map for cases and numbers\n",
    "    case_map = {\n",
    "        'nominative': 'n',\n",
    "        'genitive': 'g',\n",
    "        'dative': 'd',\n",
    "        'accusative': 'a',\n",
    "        'vocative': 'v',\n",
    "        'ablative': 'b',\n",
    "        'locative': 'l',\n",
    "        'instrumental': 'i',\n",
    "        'notes:': ''\n",
    "    }\n",
    "    number_map = {\n",
    "        'singular': 's',\n",
    "        'dual': 'd',\n",
    "        'plural': 'p'\n",
    "    }\n",
    "\n",
    "    exclude_texts = set([\n",
    "        'adverb', 'comparative', 'superlative', '-', '—', '', ' ', 'notes:'\n",
    "    ])\n",
    "\n",
    "    forms = []\n",
    "    sequence_num = sequence_num_start\n",
    "\n",
    "    # Clean the DataFrame headers\n",
    "    df.columns = [str(col).lower().strip() for col in df.columns]\n",
    "\n",
    "    # The first column should be cases\n",
    "    cases = df.iloc[:, 0].astype(str).str.lower().str.strip()\n",
    "    # The rest of the columns are numbers\n",
    "    numbers = [str(col).lower().strip() for col in df.columns[1:]]\n",
    "\n",
    "    for idx, case in enumerate(cases):\n",
    "        case = case.strip()\n",
    "        case_key = case_map.get(case, '-')\n",
    "        if case_key == '' or case in exclude_texts:\n",
    "            continue  # Skip notes or irrelevant rows\n",
    "        for col_idx, num_col in enumerate(numbers):\n",
    "            num_col = num_col.strip()\n",
    "            number_key = number_map.get(num_col, '-')\n",
    "            if number_key == '-' or num_col in exclude_texts:\n",
    "                continue\n",
    "            form = df.iloc[idx, col_idx + 1]\n",
    "            if isinstance(form, str):\n",
    "                form = form.strip()\n",
    "                if form != '' and form not in exclude_texts:\n",
    "                    bare_text = strip_accents(form)\n",
    "                    bare_base_form = strip_accents(base_word)\n",
    "                    morph_code = generate_morph_code(\n",
    "                        part_of_speech='n',\n",
    "                        person='-',\n",
    "                        number=number_key,\n",
    "                        tense='-',\n",
    "                        mood='-',\n",
    "                        voice='-',\n",
    "                        gender=gender,\n",
    "                        case=case_key,\n",
    "                        degree='-'\n",
    "                    )\n",
    "                    form_entry = {\n",
    "                        'id': sequence_num,\n",
    "                        'text': form,\n",
    "                        'bare_text': bare_text,\n",
    "                        'sequence_num': sequence_num,\n",
    "                        'morph_code': morph_code,\n",
    "                        'base_form': base_word,\n",
    "                        'bare_base_form': bare_base_form,\n",
    "                        'definition': definition\n",
    "                    }\n",
    "                    forms.append(form_entry)\n",
    "                    sequence_num += 1\n",
    "    return forms\n",
    "\n",
    "def parse_adjective_table(df, base_word, sequence_num_start, definition):\n",
    "    \"\"\"\n",
    "    Parse a multi-level Ancient Greek adjective table.\n",
    "    \n",
    "    If the table’s header isn’t properly detected as multirow (for example,\n",
    "    if the first column reads \"number\"), this function will assume the first row\n",
    "    contains the actual header and reassign them.\n",
    "    \n",
    "    IMPORTANT:\n",
    "      When calling pd.read_html on Wiktionary adjective tables, consider using:\n",
    "          header=[0,1,2]\n",
    "      to see if you obtain a proper MultiIndex. If not, this function will try to adjust.\n",
    "    \"\"\"\n",
    "    import unicodedata\n",
    "\n",
    "    def strip_accents(s):\n",
    "        \"\"\"Remove accents from a given string.\"\"\"\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                       if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    # Mapping definitions.\n",
    "    case_map = {\n",
    "        'nominative': 'n',\n",
    "        'genitive': 'g',\n",
    "        'dative': 'd',\n",
    "        'accusative': 'a',\n",
    "        'vocative': 'v',\n",
    "        'ablative': 'b',\n",
    "        'locative': 'l',\n",
    "        'instrumental': 'i',\n",
    "    }\n",
    "    number_map = {\n",
    "        'singular': 's',\n",
    "        'dual': 'd',\n",
    "        'plural': 'p'\n",
    "    }\n",
    "    gender_map = {\n",
    "        'masculine': 'm',\n",
    "        'feminine': 'f',\n",
    "        'masculine / feminine': 'mf',\n",
    "        'masculine/feminine': 'mf',\n",
    "        'neuter': 'n',\n",
    "    }\n",
    "    # Skip only these keywords.\n",
    "    skip_keywords = {'derived forms', 'notes', 'adverb', 'comparative', 'superlative', '—', '-'}\n",
    "\n",
    "    forms = []\n",
    "    sequence_num = sequence_num_start\n",
    "\n",
    "    # === Step 1: Examine and Flatten Headers ===\n",
    "    print(\"=== DEBUG: Original DF.columns ===\")\n",
    "    print(df.columns)\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [\n",
    "            \" \".join(str(x).strip().lower() for x in col if str(x).lower() != 'nan')\n",
    "            for col in df.columns\n",
    "        ]\n",
    "    else:\n",
    "        df.columns = [str(col).strip().lower() for col in df.columns]\n",
    "        # If the first header is \"number\" (which indicates the header wasn't read properly),\n",
    "        # use the first row as the header and drop it.\n",
    "        if df.columns[0] == \"number\":\n",
    "            print(\"=== DEBUG: Detected 'number' as first header. Using first row as new header.\")\n",
    "            new_header = df.iloc[0].str.lower().tolist()\n",
    "            df = df.iloc[1:].reset_index(drop=True)\n",
    "            df.columns = new_header\n",
    "\n",
    "    print(\"=== DEBUG: Processed columns ===\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "    # === Step 2: Identify Columns for (Number, Gender) ===\n",
    "    # Assume the first column now is the row label (which contains the case name).\n",
    "    col_info = {}\n",
    "    for idx, col_name in enumerate(df.columns):\n",
    "        col_name = col_name.strip()\n",
    "        if idx == 0:\n",
    "            continue  # skip row label column\n",
    "\n",
    "        # Skip columns with skip keywords.\n",
    "        if any(kw in col_name for kw in skip_keywords):\n",
    "            print(f\"=== DEBUG: Skipping column '{col_name}' due to skip keyword\")\n",
    "            continue\n",
    "\n",
    "        # Check for the number (singular, dual, plural) in the column name.\n",
    "        number_val = None\n",
    "        for pat, nm_code in number_map.items():\n",
    "            if pat in col_name:\n",
    "                number_val = nm_code\n",
    "                break\n",
    "\n",
    "        # Check for gender information.\n",
    "        gender_val = None\n",
    "        if 'masculine' in col_name and 'feminine' in col_name:\n",
    "            gender_val = 'mf'\n",
    "        else:\n",
    "            for pat, g_code in gender_map.items():\n",
    "                if pat in col_name:\n",
    "                    gender_val = g_code\n",
    "                    break\n",
    "\n",
    "        if number_val and gender_val:\n",
    "            col_info[idx] = (number_val, gender_val)\n",
    "            print(f\"=== DEBUG: Column index {idx} '{col_name}' mapped to ({number_val}, {gender_val})\")\n",
    "        else:\n",
    "            print(f\"=== DEBUG: Column index {idx} '{col_name}' did not match number/gender criteria.\")\n",
    "\n",
    "    if not col_info:\n",
    "        print(\"=== DEBUG: No columns with both number and gender info were found. Check header reading.\")\n",
    "        return forms  # return empty if nothing found\n",
    "\n",
    "    # === Step 3: Process Each Row to Extract Forms ===\n",
    "    for row_idx in range(len(df)):\n",
    "        row_label = str(df.iloc[row_idx, 0]).lower().strip()\n",
    "        if not row_label or row_label in skip_keywords:\n",
    "            print(f\"=== DEBUG: Skipping row {row_idx} with label '{row_label}'\")\n",
    "            continue\n",
    "\n",
    "        # Determine the morphological case from the row label.\n",
    "        case_found = None\n",
    "        for cm_key, cm_val in case_map.items():\n",
    "            if cm_key in row_label:\n",
    "                case_found = cm_val\n",
    "                break\n",
    "\n",
    "        if not case_found:\n",
    "            print(f\"=== DEBUG: Row {row_idx} with label '{row_label}' did not match any expected case.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"=== DEBUG: Row {row_idx} with case '{row_label}' mapped to case code '{case_found}'\")\n",
    "\n",
    "        # Process each (number, gender) column.\n",
    "        for col_idx, (num_code, gen_code) in col_info.items():\n",
    "            cell_val = df.iat[row_idx, col_idx]\n",
    "            if not isinstance(cell_val, str):\n",
    "                print(f\"=== DEBUG: Cell at row {row_idx}, col {col_idx} is not a string; skipping.\")\n",
    "                continue\n",
    "            cell_val = cell_val.strip()\n",
    "            if not cell_val or cell_val in skip_keywords:\n",
    "                print(f\"=== DEBUG: Cell at row {row_idx}, col {col_idx} is empty or skipped.\")\n",
    "                continue\n",
    "\n",
    "            # If there are multiple forms in one cell, separate them.\n",
    "            sub_forms = [x.strip() for x in cell_val.split('/') if x.strip()]\n",
    "            for sf in sub_forms:\n",
    "                bare_text = strip_accents(sf)\n",
    "                bare_base_form = strip_accents(base_word)\n",
    "                degree_key = 'p'  # assuming positive degree by default\n",
    "                morph_code = generate_morph_code(\n",
    "                    part_of_speech='a',   # adjective\n",
    "                    person='-',\n",
    "                    number=num_code,\n",
    "                    tense='-',\n",
    "                    mood='-',\n",
    "                    voice='-',\n",
    "                    gender=gen_code,\n",
    "                    case=case_found,\n",
    "                    degree=degree_key\n",
    "                )\n",
    "                form_entry = {\n",
    "                    'id': sequence_num,\n",
    "                    'text': sf,\n",
    "                    'bare_text': bare_text,\n",
    "                    'sequence_num': sequence_num,\n",
    "                    'morph_code': morph_code,\n",
    "                    'base_form': base_word,\n",
    "                    'bare_base_form': bare_base_form,\n",
    "                    'definition': definition\n",
    "                }\n",
    "                print(f\"=== DEBUG: Adding form: {form_entry}\")\n",
    "                forms.append(form_entry)\n",
    "                sequence_num += 1\n",
    "\n",
    "    if not forms:\n",
    "        print(\"=== DEBUG: No adjective forms were parsed from the table.\")\n",
    "    else:\n",
    "        print(f\"=== DEBUG: Parsed {len(forms)} adjective forms.\")\n",
    "    return forms\n",
    "\n",
    "\n",
    "\n",
    "def parse_verb_table(df, base_word, sequence_num_start, definition):\n",
    "    \"\"\"\n",
    "    Parse the verb inflection table and extract forms along with their grammatical features.\n",
    "    Handles tables with multiple header levels.\n",
    "    \"\"\"\n",
    "    # Maps for grammatical features\n",
    "    person_map = {\n",
    "        'first person': '1',\n",
    "        'second person': '2',\n",
    "        'third person': '3',\n",
    "    }\n",
    "    \n",
    "    number_map = {\n",
    "        'singular': 's',\n",
    "        'dual': 'd',\n",
    "        'plural': 'p'\n",
    "    }\n",
    "    \n",
    "    mood_map = {\n",
    "        'indicative': 'i',\n",
    "        'subjunctive': 's',\n",
    "        'optative': 'o',\n",
    "        'imperative': 'm',\n",
    "        'infinitive': 'n',\n",
    "        'participle': 'p'\n",
    "    }\n",
    "    \n",
    "    voice_map = {\n",
    "        'active': 'a',\n",
    "        'middle': 'm',\n",
    "        'passive': 'p',\n",
    "        'mediopassive': 'e'\n",
    "    }\n",
    "    \n",
    "    tense_map = {\n",
    "        'present': 'p',\n",
    "        'imperfect': 'i',\n",
    "        'future': 'f',\n",
    "        'aorist': 'a',\n",
    "        'perfect': 'r',\n",
    "        'pluperfect': 'l',\n",
    "        'future perfect': 't'\n",
    "    }\n",
    "        # Set of unwanted entries to exclude\n",
    "    exclude_texts = set([\n",
    "        'adverb', 'comparative', 'superlative', '-', '—', '', ' ', 'notes:'\n",
    "    ])\n",
    "\n",
    "    forms = []\n",
    "    sequence_num = sequence_num_start\n",
    "\n",
    "    # Flatten MultiIndex columns\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        df.columns = [' '.join([str(s).strip().lower() for s in col if str(s) != 'nan']) for col in df.columns]\n",
    "    else:\n",
    "        df.columns = [col.lower().strip() for col in df.columns]\n",
    "\n",
    "    # Identify the grammatical features from the table\n",
    "    for col in df.columns:\n",
    "        if ' ' in col:\n",
    "            continue\n",
    "        if col in ['person', 'number', 'person / number']:\n",
    "            # This is the row index column\n",
    "            df.rename(columns={col: 'person_number'}, inplace=True)\n",
    "            break\n",
    "\n",
    "    # Extract mood, voice, tense from the table caption or nearby headers\n",
    "    # For this example, let's assume we have mood, voice, tense information passed to this function\n",
    "    # Alternatively, you can modify the code to extract these from the HTML as needed\n",
    "\n",
    "    # Since the tables are complex, we need to get mood, voice, tense from the column headers\n",
    "    columns_info = []\n",
    "    for col in df.columns:\n",
    "        if col == 'person_number':\n",
    "            columns_info.append({'column': col})\n",
    "            continue\n",
    "        features = {'column': col}\n",
    "        # Split the column name to extract features\n",
    "        tokens = col.split()\n",
    "        for token in tokens:\n",
    "            if token in mood_map:\n",
    "                features['mood'] = mood_map[token]\n",
    "            elif token in voice_map:\n",
    "                features['voice'] = voice_map[token]\n",
    "            elif token in tense_map:\n",
    "                features['tense'] = tense_map[token]\n",
    "        columns_info.append(features)\n",
    "\n",
    "    # Iterate over the rows to extract forms\n",
    "    for idx, row in df.iterrows():\n",
    "        person_number = str(row.get('person_number', '')).lower()\n",
    "        person = '-'\n",
    "        number = '-'\n",
    "        # Extract person and number from row labels\n",
    "        for pn in person_map:\n",
    "            if pn in person_number:\n",
    "                person = person_map[pn]\n",
    "                break\n",
    "        for num in number_map:\n",
    "            if num in person_number:\n",
    "                number = number_map[num]\n",
    "                break\n",
    "        if person == '-' and number == '-':\n",
    "            # Try splitting the person_number string\n",
    "            tokens = person_number.split()\n",
    "            for token in tokens:\n",
    "                if token in person_map:\n",
    "                    person = person_map[token]\n",
    "                elif token in number_map:\n",
    "                    number = number_map[token]\n",
    "        # Iterate over the columns to get forms\n",
    "        for col_info in columns_info:\n",
    "            col = col_info['column']\n",
    "            if col == 'person_number':\n",
    "                continue\n",
    "            form = row[col]\n",
    "            if isinstance(form, str) and form.strip() != '':\n",
    "                form = form.strip()\n",
    "                bare_text = strip_accents(form)\n",
    "                bare_base_form = strip_accents(base_word)\n",
    "                mood = col_info.get('mood', '-')\n",
    "                voice = col_info.get('voice', '-')\n",
    "                tense = col_info.get('tense', '-')\n",
    "                # Handle non-finite forms\n",
    "                if person == '-' and number == '-' and 'person_number' in df.columns:\n",
    "                    if 'singular' in person_number:\n",
    "                        number = 's'\n",
    "                    elif 'dual' in person_number:\n",
    "                        number = 'd'\n",
    "                    elif 'plural' in person_number:\n",
    "                        number = 'p'\n",
    "                    else:\n",
    "                        number = '-'\n",
    "                    if 'first' in person_number:\n",
    "                        person = '1'\n",
    "                    elif 'second' in person_number:\n",
    "                        person = '2'\n",
    "                    elif 'third' in person_number:\n",
    "                        person = '3'\n",
    "                    else:\n",
    "                        person = '-'\n",
    "                morph_code = generate_morph_code(\n",
    "                    part_of_speech='v',\n",
    "                    person=person,\n",
    "                    number=number,\n",
    "                    tense=tense,\n",
    "                    mood=mood,\n",
    "                    voice=voice,\n",
    "                    gender='-',\n",
    "                    case='-',\n",
    "                    degree='-'\n",
    "                )\n",
    "                form_entry = {\n",
    "                    'id': sequence_num,\n",
    "                    'text': form,\n",
    "                    'bare_text': bare_text,\n",
    "                    'sequence_num': sequence_num,\n",
    "                    'morph_code': morph_code,\n",
    "                    'base_form': base_word,\n",
    "                    'bare_base_form': bare_base_form,\n",
    "                    'definition': definition\n",
    "                }\n",
    "                forms.append(form_entry)\n",
    "                sequence_num += 1\n",
    "\n",
    "    return forms\n",
    "\n",
    "def parse_verb_table_specific(table, base_word, sequence_num_start, definition):\n",
    "    \"\"\"\n",
    "    Parse the specific 'Present (uncontracted)' verb table for ἀάω.\n",
    "    \"\"\"\n",
    "    forms = []\n",
    "    sequence_num = sequence_num_start\n",
    "\n",
    "    # Initialize grammatical feature mappings\n",
    "    person_number_list = [\n",
    "        ('1', 's'),  # First person singular\n",
    "        ('2', 's'),  # Second person singular\n",
    "        ('3', 's'),  # Third person singular\n",
    "        ('2', 'd'),  # Second person dual\n",
    "        ('3', 'd'),  # Third person dual\n",
    "        ('1', 'p'),  # First person plural\n",
    "        ('2', 'p'),  # Second person plural\n",
    "        ('3', 'p'),  # Third person plural\n",
    "    ]\n",
    "\n",
    "    mood_map = {\n",
    "        'indicative': 'i',\n",
    "        'subjunctive': 's',\n",
    "        'optative': 'o',\n",
    "        'imperative': 'm',\n",
    "        'infinitive': 'n',\n",
    "        'participle': 'p'\n",
    "    }\n",
    "\n",
    "    voice_map = {\n",
    "        'active': 'a',\n",
    "        'middle': 'm',\n",
    "        'passive': 'p',\n",
    "        'mediopassive': 'e'\n",
    "    }\n",
    "\n",
    "    tense_map = {\n",
    "        'present': 'p',\n",
    "        # Add other tenses if needed\n",
    "    }\n",
    "\n",
    "    # Initialize current grammatical features\n",
    "    current_voice = '-'\n",
    "    current_mood = '-'\n",
    "    current_tense = 'p'  # Present tense\n",
    "\n",
    "    # Iterate over table rows\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all(['th', 'td'])\n",
    "        if not cells:\n",
    "            continue\n",
    "        first_cell_text = cells[0].get_text(strip=True).lower()\n",
    "        # Check for voice labels\n",
    "        if first_cell_text in voice_map:\n",
    "            current_voice = voice_map[first_cell_text]\n",
    "            continue\n",
    "        # Check for mood labels\n",
    "        elif first_cell_text in mood_map:\n",
    "            current_mood = mood_map[first_cell_text]\n",
    "            continue\n",
    "        # Check for participle gender labels\n",
    "        elif first_cell_text in ['m', 'f', 'n']:\n",
    "            current_gender = first_cell_text\n",
    "            # Process participle forms\n",
    "            if len(cells) > 1:\n",
    "                form = cells[1].get_text(strip=True)\n",
    "                if form and form not in ['-', '—']:\n",
    "                    bare_text = strip_accents(form)\n",
    "                    bare_base_form = strip_accents(base_word)\n",
    "                    morph_code = generate_morph_code(\n",
    "                        part_of_speech='v',\n",
    "                        person='-',\n",
    "                        number='-',\n",
    "                        tense=current_tense,\n",
    "                        mood='p',  # Participle\n",
    "                        voice=current_voice,\n",
    "                        gender=current_gender,\n",
    "                        case='-',\n",
    "                        degree='-'\n",
    "                    )\n",
    "                    form_entry = {\n",
    "                        'id': sequence_num,\n",
    "                        'text': form,\n",
    "                        'bare_text': bare_text,\n",
    "                        'sequence_num': sequence_num,\n",
    "                        'morph_code': morph_code,\n",
    "                        'base_form': base_word,\n",
    "                        'bare_base_form': bare_base_form,\n",
    "                        'definition': definition\n",
    "                    }\n",
    "                    forms.append(form_entry)\n",
    "                    sequence_num += 1\n",
    "            continue\n",
    "        # Check for infinitive\n",
    "        elif first_cell_text == 'infinitive':\n",
    "            # Process infinitive form\n",
    "            if len(cells) > 1:\n",
    "                form = cells[1].get_text(strip=True)\n",
    "                if form and form not in ['-', '—']:\n",
    "                    bare_text = strip_accents(form)\n",
    "                    bare_base_form = strip_accents(base_word)\n",
    "                    morph_code = generate_morph_code(\n",
    "                        part_of_speech='v',\n",
    "                        person='-',\n",
    "                        number='-',\n",
    "                        tense=current_tense,\n",
    "                        mood='n',  # Infinitive\n",
    "                        voice=current_voice,\n",
    "                        gender='-',\n",
    "                        case='-',\n",
    "                        degree='-'\n",
    "                    )\n",
    "                    form_entry = {\n",
    "                        'id': sequence_num,\n",
    "                        'text': form,\n",
    "                        'bare_text': bare_text,\n",
    "                        'sequence_num': sequence_num,\n",
    "                        'morph_code': morph_code,\n",
    "                        'base_form': base_word,\n",
    "                        'bare_base_form': bare_base_form,\n",
    "                        'definition': definition\n",
    "                    }\n",
    "                    forms.append(form_entry)\n",
    "                    sequence_num += 1\n",
    "            continue\n",
    "        # Skip notes and empty rows\n",
    "        elif first_cell_text in ['notes:', 'note:', '']:\n",
    "            continue\n",
    "\n",
    "        # Process rows with verb forms\n",
    "        forms_in_row = [cell.get_text(strip=True) for cell in cells]\n",
    "        # Remove empty forms and labels\n",
    "        forms_in_row = [form for form in forms_in_row if form not in ['', '-', '—']]\n",
    "        # Skip rows with insufficient data\n",
    "        if len(forms_in_row) < 2:\n",
    "            continue\n",
    "\n",
    "        # For moods with person and number\n",
    "        if current_mood in ['i', 's', 'o', 'm']:  # Indicative, Subjunctive, Optative, Imperative\n",
    "            for idx_form, form in enumerate(forms_in_row[1:], start=0):\n",
    "                if form in ['', '-', '—']:\n",
    "                    continue\n",
    "                if idx_form < len(person_number_list):\n",
    "                    person, number = person_number_list[idx_form]\n",
    "                    bare_text = strip_accents(form)\n",
    "                    bare_base_form = strip_accents(base_word)\n",
    "                    morph_code = generate_morph_code(\n",
    "                        part_of_speech='v',\n",
    "                        person=person,\n",
    "                        number=number,\n",
    "                        tense=current_tense,\n",
    "                        mood=current_mood,\n",
    "                        voice=current_voice,\n",
    "                        gender='-',\n",
    "                        case='-',\n",
    "                        degree='-'\n",
    "                    )\n",
    "                    form_entry = {\n",
    "                        'id': sequence_num,\n",
    "                        'text': form,\n",
    "                        'bare_text': bare_text,\n",
    "                        'sequence_num': sequence_num,\n",
    "                        'morph_code': morph_code,\n",
    "                        'base_form': base_word,\n",
    "                        'bare_base_form': bare_base_form,\n",
    "                        'definition': definition\n",
    "                    }\n",
    "                    forms.append(form_entry)\n",
    "                    sequence_num += 1\n",
    "        else:\n",
    "            # Handle other moods if necessary\n",
    "            pass\n",
    "\n",
    "    return forms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s):\n",
    "    \"\"\"Remove accents from a given string.\"\"\"\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"Clean the DataFrame by removing unwanted characters and standardizing the data.\"\"\"\n",
    "    df = df.dropna(how='all', axis=0)  # Remove empty rows\n",
    "\n",
    "    # Remove unwanted characters and symbols\n",
    "    df = df.replace({'➤': '', '•': '', '\\[': '', '\\]': '', '\\{': '', '\\}': '', '—': ' '}, regex=True)\n",
    "    df = df.replace(r\"\\([^)]*\\)\", \"\", regex=True)  # Remove content within parentheses\n",
    "\n",
    "    # Make all strings lowercase\n",
    "    df = df.map(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "\n",
    "    # Split strings to remove extraneous information\n",
    "    df = df.map(lambda s: s.split(',', 1)[0] if isinstance(s, str) else s)\n",
    "    df = df.map(lambda s: s.split('-', 1)[0] if isinstance(s, str) else s)\n",
    "    df = df.map(lambda s: s.split('+', 1)[0] if isinstance(s, str) else s)\n",
    "    df = df.map(lambda s: s.split('/', 1)[0] if isinstance(s, str) else s)\n",
    "    df = df.map(lambda s: s.split('\\\\', 1)[0] if isinstance(s, str) else s)\n",
    "\n",
    "    df = df.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    df = df.replace(r\"\\s\\s+\", \" \", regex=True)  # Replace multiple spaces with a single space\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_variations(word, show=False, definition_map=None):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup as bs\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 1. Fetch the page\n",
    "    url = f\"https://en.wiktionary.org/wiki/{quote(word)}\"\n",
    "    response = requests.get(url)\n",
    "    soup = bs(response.text, 'html.parser')\n",
    "\n",
    "    # 2. Clean up\n",
    "    for tag in soup.find_all('span', {'class': 'tr Latn'}):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('table', {'class': 'audiotable'}):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all('sup'):\n",
    "        tag.decompose()\n",
    "\n",
    "    # 3. Definition from map\n",
    "    if definition_map is not None:\n",
    "        normalized_word = strip_accents(word).lower()\n",
    "        definition = definition_map.get(normalized_word, '')\n",
    "    else:\n",
    "        definition = ''\n",
    "\n",
    "    forms = []\n",
    "    sequence_num = 1\n",
    "\n",
    "    # 4. Find all tables whose class contains \"inflection\"\n",
    "    #    (since \"inflection-table\" might not be exact)\n",
    "    all_inflect_tables = []\n",
    "    for tbl in soup.find_all(\"table\"):\n",
    "        classes = tbl.get(\"class\", [])\n",
    "        class_str = \" \".join(classes).lower()\n",
    "        if \"inflection\" in class_str:\n",
    "            all_inflect_tables.append(tbl)\n",
    "\n",
    "    if not all_inflect_tables:\n",
    "        print(f\"No inflection tables found on page for {word}\")\n",
    "        return pd.DataFrame(forms)\n",
    "\n",
    "    # 5. For each table, guess the part of speech from preceding <h3> or the table’s caption\n",
    "    for table in all_inflect_tables:\n",
    "        # Check heading above\n",
    "        part_of_speech_tag = table.find_previous('h3')\n",
    "        if part_of_speech_tag:\n",
    "            pos_text = part_of_speech_tag.get_text(strip=True).lower()\n",
    "        else:\n",
    "            pos_text = \"\"\n",
    "\n",
    "        # Also check caption\n",
    "        caption_tag = table.find('caption')\n",
    "        caption_text = caption_tag.get_text(strip=True).lower() if caption_tag else \"\"\n",
    "\n",
    "        # Decide if verb, adjective, noun, etc.\n",
    "        if 'verb' in pos_text or 'verb' in caption_text:\n",
    "            # parse as verb\n",
    "            try:\n",
    "                df_list = pd.read_html(str(table))\n",
    "                if not df_list:\n",
    "                    continue\n",
    "                df = df_list[0]\n",
    "                df = clean_dataframe(df)\n",
    "\n",
    "                # parse with parse_verb_table\n",
    "                parsed_forms = parse_verb_table(df, word, sequence_num, definition)\n",
    "                forms.extend(parsed_forms)\n",
    "                sequence_num += len(parsed_forms)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing verb table for {word}: {e}\")\n",
    "                continue\n",
    "\n",
    "        elif 'adjective' in pos_text or 'adjective' in caption_text:\n",
    "            try:\n",
    "                # Use the proper header parameter so that the real headers are read.\n",
    "                df_list = pd.read_html(str(table), header=[0,1,2])\n",
    "                if not df_list:\n",
    "                    continue\n",
    "                df = df_list[0]\n",
    "                df = clean_dataframe(df)\n",
    "                parsed_forms = parse_adjective_table(\n",
    "                    df, \n",
    "                    base_word=word, \n",
    "                    sequence_num_start=sequence_num, \n",
    "                    definition=definition\n",
    "                )\n",
    "                forms.extend(parsed_forms)\n",
    "                sequence_num += len(parsed_forms)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing adjective table for {word}: {e}\")\n",
    "                continue\n",
    "\n",
    "        elif 'noun' in pos_text or 'noun' in caption_text:\n",
    "            # parse as noun\n",
    "            try:\n",
    "                df_list = pd.read_html(str(table))\n",
    "                if not df_list:\n",
    "                    continue\n",
    "                df = df_list[0]\n",
    "                df = clean_dataframe(df)\n",
    "\n",
    "                parsed_forms = parse_noun_table(\n",
    "                    df, \n",
    "                    base_word=word, \n",
    "                    gender='-', \n",
    "                    sequence_num_start=sequence_num, \n",
    "                    definition=definition\n",
    "                )\n",
    "                forms.extend(parsed_forms)\n",
    "                sequence_num += len(parsed_forms)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing noun table for {word}: {e}\")\n",
    "                continue\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return pd.DataFrame(forms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_csv_with_wiktionary(csv_path, base_word):\n",
    "    # \"\"\"\n",
    "    # Update the CSV file with forms from Wiktionary by comparing existing morph codes\n",
    "    # and adding any missing forms.\n",
    "    # \"\"\"\n",
    "    # # Read the original CSV\n",
    "    # df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # # Get variations from Wiktionary\n",
    "    # scraped_df = generate_variations(base_word)\n",
    "    \n",
    "    # if scraped_df.empty:\n",
    "    #     print(f\"No new forms found for {base_word}.\")\n",
    "    #     return csv_path  # Return the original path if no updates\n",
    "    \n",
    "    # # Ensure morph_code is a string\n",
    "    # df['morph_code'] = df['morph_code'].astype(str)\n",
    "    # scraped_df['morph_code'] = scraped_df['morph_code'].astype(str)\n",
    "    \n",
    "    # # Filter rows corresponding to the base word\n",
    "    # base_word_block = df[df['base_form'] == base_word]\n",
    "    # existing_morph_codes = set(base_word_block['morph_code'])\n",
    "    \n",
    "    # # Get new forms not in the original CSV\n",
    "    # new_forms = scraped_df[~scraped_df['morph_code'].isin(existing_morph_codes)]\n",
    "    \n",
    "    # if new_forms.empty:\n",
    "    #     print(f\"All forms of {base_word} are already present in the dataset.\")\n",
    "    #     return csv_path  # Return the original path if no new forms to add\n",
    "    \n",
    "    # # Append new forms to the DataFrame\n",
    "    # updated_df = pd.concat([df, new_forms], ignore_index=True)\n",
    "    \n",
    "    # # Save the updated DataFrame to a new CSV\n",
    "    # updated_csv_path = csv_path.replace('.csv', '_updated.csv')\n",
    "    # updated_df.to_csv(updated_csv_path, index=False)\n",
    "    \n",
    "    # print(f\"Updated CSV saved to {updated_csv_path}\")\n",
    "    # return updated_csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 'προπονέω'…"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sampled_words:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m…\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_variations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefinition_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefinition_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m no forms found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[34], line 37\u001b[0m, in \u001b[0;36mgenerate_variations\u001b[1;34m(word, show, definition_map)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 1. Fetch the page\u001b[39;00m\n\u001b[0;32m     36\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://en.wiktionary.org/wiki/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(word)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m soup \u001b[38;5;241m=\u001b[39m bs(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 2. Clean up\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\jhopp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# --- 1. Load your word bank (and old morphological entries) ---\n",
    "greek_words_df = pd.read_csv('greek_words.csv',\n",
    "                             encoding='utf-8', sep='\\t', index_col=False)\n",
    "\n",
    "# Ensure we have the columns we need\n",
    "desired_columns = [\n",
    "    'id', 'text', 'bare_text', 'sequence_num',\n",
    "    'morph_code', 'base_form', 'bare_base_form', 'definition'\n",
    "]\n",
    "for col in desired_columns:\n",
    "    if col not in greek_words_df.columns:\n",
    "        greek_words_df[col] = \"\"\n",
    "\n",
    "# Build a definition map for the scraper\n",
    "greek_words_df['normalized_base_form'] = greek_words_df['base_form'].apply(normalize_word)\n",
    "definition_map = greek_words_df.set_index('normalized_base_form')['definition'].to_dict()\n",
    "\n",
    "# --- 2. Pick 100 *random* unique headwords to test ---\n",
    "unique_words = list(greek_words_df['base_form'].unique())\n",
    "sampled_words = random.sample(unique_words, k=100)\n",
    "\n",
    "# Extract the “old” entries for those sampled words\n",
    "old_entries = greek_words_df[greek_words_df['base_form'].isin(sampled_words)][desired_columns].copy()\n",
    "\n",
    "# --- 3. Scrape Wiktionary for each of those 100 headwords ---\n",
    "all_scraped = []\n",
    "total_scraped = 0\n",
    "next_milestone = 1000\n",
    "\n",
    "for word in sampled_words:\n",
    "    print(f\"Scraping '{word}'…\", end='')\n",
    "    df = generate_variations(word, definition_map=definition_map)\n",
    "    if df.empty:\n",
    "        print(\" no forms found.\")\n",
    "        continue\n",
    "\n",
    "    # Ensure correct columns & order\n",
    "    for col in desired_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    df = df[desired_columns]\n",
    "\n",
    "    count = len(df)\n",
    "    total_scraped += count\n",
    "    all_scraped.append(df)\n",
    "\n",
    "    print(f\" found {count} forms (total so far: {total_scraped}).\")\n",
    "    if total_scraped >= next_milestone:\n",
    "        print(f\"=== Progress: {total_scraped} total forms scraped so far ===\")\n",
    "        next_milestone += 1000\n",
    "\n",
    "scraped_df = pd.concat(all_scraped, ignore_index=True) if all_scraped else pd.DataFrame(columns=desired_columns)\n",
    "\n",
    "# --- 4. Merge old + new, drop duplicates, re-number full set ---\n",
    "combined = pd.concat([old_entries, scraped_df], ignore_index=True)\n",
    "combined = combined.drop_duplicates(subset=['text', 'morph_code'], keep='first')\n",
    "combined = combined.reset_index(drop=True)\n",
    "combined['id'] = combined.index + 1\n",
    "combined['sequence_num'] = combined['id']\n",
    "final_df = combined[desired_columns]\n",
    "final_df.to_csv('greek_words_updated.csv', sep='\\t', index=False, encoding='utf-8')\n",
    "print(f\"Done! {len(final_df)} unique forms written to greek_words_updated.csv.\")\n",
    "\n",
    "# --- 5. Identify *new* forms (in scraped_df but not in old_entries) ---\n",
    "# Compare by (text, morph_code)\n",
    "old_set = set(zip(old_entries['text'], old_entries['morph_code']))\n",
    "scraped_new = scraped_df[~scraped_df.apply(lambda r: (r['text'], r['morph_code']) in old_set, axis=1)]\n",
    "\n",
    "# Re-number just the new ones\n",
    "scraped_new = scraped_new.reset_index(drop=True)\n",
    "scraped_new['id'] = scraped_new.index + 1\n",
    "scraped_new['sequence_num'] = scraped_new['id']\n",
    "\n",
    "# Save out the “new only” CSV\n",
    "scraped_new.to_csv('greek_words_new_only.csv', sep='\\t', index=False, encoding='utf-8')\n",
    "print(f\"Done! {len(scraped_new)} new forms written to greek_words_new_only.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
