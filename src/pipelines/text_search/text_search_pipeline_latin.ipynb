{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Search Pipeline for Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for documents or indexes of word occurrences inside documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import other required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, csv, multiprocessing as mp, numpy as np, os, pandas as pd, pickle, requests, tqdm\n",
    "from inspect import signature\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from termcolor import colored, cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipycache\n",
    "from IPython.utils.traitlets import Unicode\n",
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coloring options\n",
    "line_color = 'blue'\n",
    "text_color = 'magenta'\n",
    "reference_color = 'white'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "metadata_path=os.getenv(key='metadata_path')\n",
    "metadata_df_path=os.getenv(key='metadata_df_path')\n",
    "corpus_path=os.getenv(key='corpus_path')\n",
    "\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text=f'Metadata path: {metadata_path}', color='magenta')\n",
    "cprint(text=f'Metadata dataframe path: {metadata_df_path}', color='magenta')\n",
    "cprint(text=f'Corpus path: {corpus_path}', color='magenta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = widgets.Text('Testing...')\n",
    "display(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load metadata for First1KGreek project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = None\n",
    "\n",
    "try:\n",
    "    metadata_df = pickle.load(file=open(file='metadata_df.pkl', mode='rb'))['metadata_df']\n",
    "\n",
    "except:\n",
    "    metadata_csv_reader = csv.reader(open(file=metadata_path, mode='r', encoding='utf-8'))\n",
    "    columns = next(metadata_csv_reader)\n",
    "    metadata_df = pd.DataFrame(\n",
    "        data=np.asarray(a=list(metadata_csv_reader)),\n",
    "        columns=columns\n",
    "    )\n",
    "\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text='metadata_df:', color='magenta')\n",
    "cprint(text='-' * 100, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache metadata_df.pkl metadata_df --force\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Latin text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *From TXT File*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt(filename: str):\n",
    "    \"\"\"\n",
    "    Extract text from a .txt file.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Path of file to load.\n",
    "\n",
    "    Returns:\n",
    "        str: Text loaded from file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path=filename):\n",
    "        raise ValueError(f'The path {filename} does not exist.')\n",
    "\n",
    "    text = open(file=filename, mode='r').read()\n",
    "    return text\n",
    "\n",
    "filename = '/mnt/d/share/Using-AI-to-Trace-the-History-of-Race-and-Inequality/src/sample_text/latin/urn_cts_greekLit_stoa0146d.stoa001.opp-lat11.txt'\n",
    "text = load_txt(filename=filename)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='Loading from TXT file:', color=text_color)\n",
    "cprint(text=filename, color=reference_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=text, color=text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *From URI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uri(uri: str):\n",
    "    \"\"\"\n",
    "    Load text from URI.\n",
    "\n",
    "    Parameters:\n",
    "        uri (str): URI link to text online.\n",
    "\n",
    "    Returns:\n",
    "        str: Text loaded from URI.\n",
    "    \"\"\"\n",
    "    req = requests.get(url=uri)\n",
    "    return req.text\n",
    "\n",
    "uri = 'https://scaife.perseus.org/library/passage/urn:cts:greekLit:stoa0146d.stoa001.opp-lat1:1/text/'\n",
    "text = load_uri(uri=uri)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='Loading from URI:', color=text_color)\n",
    "cprint(text=uri, color=reference_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=text, color=text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *From URN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_urn(urn: str):\n",
    "    \"\"\"\n",
    "    Load text from URN.\n",
    "\n",
    "    Parameters:\n",
    "        urn (str): URN link that identifies specific work.\n",
    "\n",
    "    Returns:\n",
    "        str: Text loaded from URI.\n",
    "    \"\"\"\n",
    "    idx = metadata_df.index[metadata_df['URN'] == urn][0]\n",
    "    url = f'https://scaife.perseus.org/library/passage/{metadata_df.at[idx, \"URL\"].split(\"/\")[-2]}/text/'\n",
    "    req = requests.get(url=url)\n",
    "    \n",
    "    return req.text\n",
    "\n",
    "urn = 'urn:cts:greekLit:stoa0146d.stoa001.opp-lat1'\n",
    "text = load_urn(urn=urn)\n",
    "url = f'https://scaife.perseus.org/library/passage/{metadata_df.at[0, \"URL\"].split(\"/\")[-2]}/text/'\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='Loading from URN:', color=text_color)\n",
    "cprint(text=urn, color=reference_color)\n",
    "cprint(text=url, color=text_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=text, color=text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create analyzer for text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.lang import has_stemmer, languages\n",
    "from whoosh.analysis import SpaceSeparatedTokenizer, LowercaseFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for Latin availability for analyzer\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text='Available languages for LanguageAnalyzer:', color='magenta')\n",
    "cprint(text='-' * 100, color='green')\n",
    "pprint(languages)\n",
    "\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text=f'Latin is available? {has_stemmer(lang=\"lat\")}', color='magenta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SpaceSeparatedTokenizer() | LowercaseFilter()\n",
    "for token in analyzer(value=text):\n",
    "    pprint(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import create_in\n",
    "from whoosh.fields import Schema, ID, KEYWORD, NUMERIC, TEXT\n",
    "from whoosh.qparser import QueryParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(\n",
    "    index = NUMERIC(\n",
    "        bits=64,\n",
    "        numtype='int',\n",
    "        signed=False,\n",
    "        sortable=True,\n",
    "        stored=True,\n",
    "        unique=True\n",
    "    ),\n",
    "    url = ID(\n",
    "        sortable=False,\n",
    "        stored=True,\n",
    "        unique=True\n",
    "    ),\n",
    "    title=TEXT(\n",
    "        chars=True,\n",
    "        phrase=True,\n",
    "        sortable=True,\n",
    "        stored=True\n",
    "    ),\n",
    "    author=TEXT(\n",
    "        chars=True,\n",
    "        phrase=True,\n",
    "        sortable=True,\n",
    "        stored=True\n",
    "    ),\n",
    "    languages=KEYWORD(\n",
    "        commas=True,\n",
    "        lowercase=True,\n",
    "        scorable=False,\n",
    "        sortable=False,\n",
    "        stored=True,\n",
    "        unique=False\n",
    "    ),\n",
    "    keywords=KEYWORD(\n",
    "        commas=True,\n",
    "        lowercase=True,\n",
    "        scorable=True,\n",
    "        sortable=True,\n",
    "        stored=True,\n",
    "        unique=False\n",
    "    ),\n",
    "    content=TEXT(\n",
    "        chars=True,\n",
    "        phrase=True,\n",
    "        sortable=False,\n",
    "        stored=False,\n",
    "        analyzer=analyzer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = create_in(dirname='indexes/',\n",
    "              schema=schema,\n",
    "              indexname='latin_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing does not work with document indexing."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "writer = ix.writer()\n",
    "\n",
    "pool.starmap(\n",
    "    func=writer.add_document,\n",
    "    iterable=tqdm.tqdm(\n",
    "        np.asarray(\n",
    "            a=list([\n",
    "                (\n",
    "                    index,\n",
    "                    row['URL'],\n",
    "                    row['Work'],\n",
    "                    row['Workgroup'],\n",
    "                    row['Language'],\n",
    "                    str(),\n",
    "                    requests.get(url=row['URL']).text\n",
    "                )\n",
    "                for index, row in tqdm.tqdm(metadata_df.iterrows())\n",
    "            ])\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conventional for-loop approach to document indexing. The multiprocessing portion is built-in inside writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.writing import AsyncWriter, BufferedWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pickle.load(file=open(file=corpus_path, mode='rb'))['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ix.writer()\n",
    "insertions = np.asarray(a=list([metadata_df.loc[:10, col_name].to_numpy().astype(dtype=np.unicode_) for col_name in metadata_df.columns]))\n",
    "insertions[0] = insertions[0].astype(dtype=np)\n",
    "#     pprint(type(u'%s' % row['URL']))\n",
    "\n",
    "    \n",
    "writer.add_document(\n",
    "    index = insertions[0].tolist(),\n",
    "    url = insertions[1].tolist(),\n",
    "    title = insertions[2].tolist(),\n",
    "    author = insertions[3].tolist()\n",
    "    languages = insertions[4].tolist(),\n",
    "    keywords = u'',\n",
    "    content = corpus[:10].astype(dtype=np.unicode_).tolist()\n",
    ")\n",
    "\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ix.searcher() as searcher:\n",
    "    query = QueryParser(\n",
    "        fieldname='author',\n",
    "        schema=ix.schema\n",
    "    ).parse(\n",
    "        text='Workgroup',\n",
    "    )\n",
    "    results = searcher.search(q=query)\n",
    "    pprint(results)\n",
    "#     pprint(signature(obj=query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
