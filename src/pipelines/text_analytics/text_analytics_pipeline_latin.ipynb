{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics Pipeline for Latin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzes text from Latin sources in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently used Python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bidict, cltk, multiprocessing as mp, numpy as np, pandas as pd, pickle, os, requests, tqdm\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "from termcolor import colored, cprint\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipycache\n",
    "from IPython.utils.traitlets import Unicode\n",
    "%load_ext ipycache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print coloring options\n",
    "line_color = 'blue'\n",
    "text_color = 'magenta'\n",
    "reference_color = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set floating point precision and printout length for numpy\n",
    "np.set_printoptions(\n",
    "    precision=10,\n",
    "    threshold=20\n",
    ")\n",
    "\n",
    "# set floating point precision for pandas\n",
    "pd.set_option('display.float_format', lambda x: '%.10f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load environment variables from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "metadata_path=os.getenv(key='metadata_path')\n",
    "metadata_df_path=os.getenv(key='metadata_df_path')\n",
    "\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text=f'Metadata path: {metadata_path}', color='magenta')\n",
    "cprint(text=f'Metadata dataframe path: {metadata_df_path}', color='magenta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load metadata for First1KGreek project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = None\n",
    "\n",
    "try:\n",
    "    metadata_df = pickle.load(file=open(file=metadata_df_path, mode='rb'))['metadata_df']\n",
    "\n",
    "except:\n",
    "    metadata_csv_reader = csv.reader(open(file=metadata_path, mode='r', encoding='utf-8'))\n",
    "    columns = next(metadata_csv_reader)\n",
    "    metadata_df = pd.DataFrame(\n",
    "        data=np.asarray(a=list(metadata_csv_reader)),\n",
    "        columns=columns\n",
    "    )\n",
    "\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text='metadata_df:', color='magenta')\n",
    "cprint(text='-' * 100, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache metadata_df.pkl metadata_df --force\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Latin text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *From TXT FIles*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_txt(filename: str):\n",
    "    \"\"\"\n",
    "    Extract text from a .txt file.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str): Path of file to load.\n",
    "\n",
    "    Returns:\n",
    "        str: Text loaded from file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path=filename):\n",
    "        raise ValueError(f'The path {filename} does not exist.')\n",
    "\n",
    "    text = open(file=filename, mode='r').read()\n",
    "    return text\n",
    "\n",
    "filename = '/mnt/d/share/Using-AI-to-Trace-the-History-of-Race-and-Inequality/src/sample_text/latin/urn_cts_greekLit_stoa0146d.stoa001.opp-lat11.txt'\n",
    "text = load_txt(filename=filename)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='Loading from TXT file:', color=text_color)\n",
    "cprint(text=filename, color=reference_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=text, color=text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Form URI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_uri(uri: str):\n",
    "    \"\"\"\n",
    "    Load text from URI.\n",
    "\n",
    "    Parameters:\n",
    "        uri (str): URI link to text online.\n",
    "\n",
    "    Returns:\n",
    "        str: Text loaded from URI.\n",
    "    \"\"\"\n",
    "    req = requests.get(url=uri)\n",
    "    return req.text\n",
    "\n",
    "uri = 'https://scaife.perseus.org/library/passage/urn:cts:greekLit:stoa0146d.stoa001.opp-lat1:1/text/'\n",
    "text = load_uri(uri=uri)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='Loading from URI:', color=text_color)\n",
    "cprint(text=uri, color=reference_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=text, color=text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *From URN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_urn(urn: str):\n",
    "    \"\"\"\n",
    "    Load text from URN.\n",
    "\n",
    "    Parameters:\n",
    "        urn (str): URN link that identifies specific work.\n",
    "\n",
    "    Returns:\n",
    "        str: Text loaded from URI.\n",
    "    \"\"\"\n",
    "    idx = metadata_df.index[metadata_df['URN'] == urn][0]\n",
    "    url = f'https://scaife.perseus.org/library/passage/{metadata_df.at[idx, \"URL\"].split(\"/\")[-2]}/text/'\n",
    "    req = requests.get(url=url)\n",
    "    \n",
    "    return req.text\n",
    "\n",
    "urn = 'urn:cts:greekLit:stoa0146d.stoa001.opp-lat1'\n",
    "text = load_urn(urn=urn)\n",
    "url = f'https://scaife.perseus.org/library/passage/{metadata_df.at[0, \"URL\"].split(\"/\")[-2]}/text/'\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='Loading from URN:', color=text_color)\n",
    "cprint(text=urn, color=reference_color)\n",
    "cprint(text=url, color=text_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=text, color=text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect and extract potentially important terms from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize NLP model\n",
    "nlp_model = cltk.NLP(language='lat')\n",
    "doc = nlp_model.analyze(text=text)\n",
    "pprint(doc)\n",
    "\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='NLP model:', color=text_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=nlp_model, color=text_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text='Document', color=text_color)\n",
    "cprint(text='-' * 100, color=line_color)\n",
    "cprint(text=doc, color=text_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate text corpus from URNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = None\n",
    "\n",
    "try:\n",
    "    corpus = pickle.load(file=open(file='corpus.pkl', mode='rb'))['corpus']\n",
    "\n",
    "except:\n",
    "    pool = mp.Pool(processes=mp.cpu_count())\n",
    "    corpus = np.asarray(\n",
    "        a=pool.starmap(\n",
    "            func=load_urn,\n",
    "            iterable=tqdm.tqdm(np.asarray(a=list((urn,) for urn in metadata_df['URN'])))\n",
    "        )\n",
    "    )\n",
    "\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text='Corpus:', color='magenta')\n",
    "cprint(text='-' * 100, color='green')\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cache corpus.pkl corpus --force\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create count vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = None\n",
    "vector = None\n",
    "doc_term_matrix = None\n",
    "\n",
    "try:\n",
    "    raise Exception()\n",
    "    vectorizer = pickle.load(file=open(file='vectorizer.pkl', mode='rb'))['vectorizer']\n",
    "    vector = pickle.load(file=open(file='vector.pkl', mode='rb'))['vector']\n",
    "    doc_term_matrix = pickle.load(file=open(file='doc_term_matrix.pkl', mode='rb'))['doc_term_matrix']\n",
    "\n",
    "except:\n",
    "    vectorizer = CountVectorizer(input='content')\n",
    "    vector = vectorizer.fit_transform(raw_documents=corpus)\n",
    "    doc_term_matrix = pd.DataFrame(\n",
    "        data=np.matrix(data=vector.toarray()),\n",
    "        columns=np.asarray(a=list(vectorizer.vocabulary_.keys()))\n",
    "    ) \n",
    "\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text='Vocabulary:', color='magenta')\n",
    "cprint(text='-' * 100, color='green')\n",
    "pprint(vectorizer.vocabulary_)\n",
    "cprint(text='-' * 100, color='green')\n",
    "cprint(text='Document term matrix:', color='magenta')\n",
    "cprint(text='-' * 100, color='green')\n",
    "pprint(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cache vectorizer.pkl vectorizer --force\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%cache vector.pkl vector --force\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cache doc_term_matrix.pkl doc_term_matrix --force\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document identification via keywords present in content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def docs_from_keywords(keywords: Iterable[str]):\n",
    "    \"\"\"\n",
    "    Find documents that contain at least one (1) of any provided keywords in content.\n",
    "    \n",
    "    Parameters:\n",
    "        keywords (str): Keywords to search for to locate relevant documents.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Subset of metadata dataframe for only documents containing any keyword.\n",
    "    \"\"\"\n",
    "    keywords = set([x for x in keywords if x in doc_term_matrix.columns])\n",
    "    indexes = doc_term_matrix.index[doc_term_matrix[keywords].any(axis=1)]\n",
    "    return metadata_df.loc[indexes, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = np.asarray(a=list([\n",
    "    'Ξεν',\n",
    "    'Ξενιζ',\n",
    "    'ξενισ',\n",
    "    'Ξενικ-ος',\n",
    "    'Βαρβαρ-ος',\n",
    "    'Βαρβαριζ',\n",
    "    'Ελληνιζ',\n",
    "    'ελλην',\n",
    "    'ελληνικ',\n",
    "    'σολοικ',\n",
    "    'αρχαιζ'\n",
    "]))\n",
    "docs_from_keywords(keywords=keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
