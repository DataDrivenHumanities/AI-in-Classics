{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "1.13.1+cu117\n",
      "[2023-10-03 14:50:54] (2.3.3) Set Model Device: cuda:0\n",
      "[2023-10-03 14:50:54] (2.3.3) Device Name: NVIDIA GeForce RTX 3080 Ti\n",
      "2023-10-03 14:50:54,541 INFO: PyABSA version: 2.3.3\n",
      "2023-10-03 14:50:54,542 INFO: Transformers version: 4.33.3\n",
      "2023-10-03 14:50:54,542 INFO: Torch version: 1.13.1+cu117+cuda11.7\n",
      "2023-10-03 14:50:54,543 INFO: Device: NVIDIA GeForce RTX 3080 Ti\n",
      "2023-10-03 14:50:54,543 INFO: Greek in the trainer is not a exact path, will search dataset in current working directory\n",
      "2023-10-03 14:50:54,547 INFO: You can set load_aug=True in a trainer to augment your dataset (English only yet) and improve performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\multiprocessing\\pool.py:265: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
      "  _warn(f\"unclosed running multiprocessing pool {self!r}\",\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n",
      "convert examples to features: 100%|██████████| 1996/1996 [00:01<00:00, 1075.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-03 14:50:57,725 INFO: Dataset Label Details: {'Negative': 998, 'Positive': 998, 'Sum': 1996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "convert examples to features: 100%|██████████| 1999/1999 [00:01<00:00, 1089.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-03 14:51:00,006 INFO: Dataset Label Details: {'Negative': 1000, 'Positive': 999, 'Sum': 1999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-03 14:51:00,895 INFO: cuda memory allocated:965359616\n",
      "2023-10-03 14:51:00,896 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,897 INFO: IOB_label_to_index:{'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,897 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x000002F751AF6490>\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,898 INFO: PyABSAVersion:2.3.3\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,898 INFO: SRD:3\t-->\tCalling Count:7990\n",
      "2023-10-03 14:51:00,899 INFO: TorchVersion:1.13.1+cu117+cuda11.7\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,899 INFO: TransformersVersion:4.33.3\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,900 INFO: auto_device:True\t-->\tCalling Count:3\n",
      "2023-10-03 14:51:00,900 INFO: batch_size:16\t-->\tCalling Count:4\n",
      "2023-10-03 14:51:00,901 INFO: cache_dataset:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,902 INFO: checkpoint_save_mode:1\t-->\tCalling Count:4\n",
      "2023-10-03 14:51:00,902 INFO: cross_validate_fold:-1\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,902 INFO: dataset_file:{'train': ['integrated_datasets\\\\atepc_datasets\\\\200.Greek\\\\greek.train.txt.atepc'], 'test': ['integrated_datasets\\\\atepc_datasets\\\\200.Greek\\\\greek.test.txt.atepc'], 'valid': []}\t-->\tCalling Count:6\n",
      "2023-10-03 14:51:00,903 INFO: dataset_name:Greek\t-->\tCalling Count:3\n",
      "2023-10-03 14:51:00,903 INFO: device:cuda:0\t-->\tCalling Count:4\n",
      "2023-10-03 14:51:00,904 INFO: device_name:NVIDIA GeForce RTX 3080 Ti\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,904 INFO: dropout:0.5\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,905 INFO: dynamic_truncate:True\t-->\tCalling Count:7990\n",
      "2023-10-03 14:51:00,905 INFO: embed_dim:768\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,906 INFO: evaluate_begin:0\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,906 INFO: from_checkpoint:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,907 INFO: gradient_accumulation_steps:1\t-->\tCalling Count:3\n",
      "2023-10-03 14:51:00,907 INFO: hidden_dim:768\t-->\tCalling Count:6\n",
      "2023-10-03 14:51:00,908 INFO: index_to_IOB_label:{1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,908 INFO: index_to_label:{0: 'Negative', 1: 'Positive'}\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,909 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,910 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,910 INFO: l2reg:1e-05\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,911 INFO: label_list:['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]']\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,912 INFO: label_to_index:{'Negative': 0, 'Positive': 1}\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,912 INFO: lcf:cdw\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,913 INFO: learning_rate:2e-05\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,913 INFO: load_aug:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,914 INFO: log_step:50\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,914 INFO: logger:<Logger fast_lcf_atepc (INFO)>\t-->\tCalling Count:8\n",
      "2023-10-03 14:51:00,915 INFO: max_seq_len:80\t-->\tCalling Count:27967\n",
      "2023-10-03 14:51:00,915 INFO: model:<class 'pyabsa.tasks.AspectTermExtraction.models.__lcf__.fast_lcf_atepc.FAST_LCF_ATEPC'>\t-->\tCalling Count:5\n",
      "2023-10-03 14:51:00,916 INFO: model_name:fast_lcf_atepc\t-->\tCalling Count:3997\n",
      "2023-10-03 14:51:00,917 INFO: model_path_to_save:checkpoints\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,917 INFO: num_epoch:1\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,918 INFO: num_labels:6\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,919 INFO: optimizer:adamw\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,919 INFO: output_dim:2\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,919 INFO: overwrite_cache:False\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,920 INFO: path_to_save:None\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,921 INFO: patience:99999\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,921 INFO: pretrained_bert:nlpaueb/bert-base-greek-uncased-v1\t-->\tCalling Count:5\n",
      "2023-10-03 14:51:00,921 INFO: save_mode:1\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,922 INFO: seed:52\t-->\tCalling Count:7\n",
      "2023-10-03 14:51:00,922 INFO: sep_indices:102\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,923 INFO: show_metric:False\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,923 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:3\n",
      "2023-10-03 14:51:00,924 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,925 INFO: task_code:ATEPC\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,925 INFO: task_name:Aspect Term Extraction and Polarity Classification\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,926 INFO: use_amp:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,926 INFO: use_bert_spc:True\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,927 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:3995\n",
      "2023-10-03 14:51:00,927 INFO: warmup_step:-1\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,927 INFO: window:lr\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,930 INFO: cuda memory allocated:965359616\n",
      "2023-10-03 14:51:00,933 INFO: Model Architecture:\n",
      " FAST_LCF_ATEPC(\n",
      "  (bert4global): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (SA1): Encoder(\n",
      "    (encoder): ModuleList(\n",
      "      (0): SelfAttention(\n",
      "        (SA): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (SA2): Encoder(\n",
      "    (encoder): ModuleList(\n",
      "      (0): SelfAttention(\n",
      "        (SA): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (linear_double): Linear(in_features=1536, out_features=768, bias=True)\n",
      "  (linear_triple): Linear(in_features=2304, out_features=768, bias=True)\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      ")\n",
      "2023-10-03 14:51:00,933 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,934 INFO: IOB_label_to_index:{'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,934 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x000002F751AF6490>\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,935 INFO: PyABSAVersion:2.3.3\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,935 INFO: SRD:3\t-->\tCalling Count:7990\n",
      "2023-10-03 14:51:00,935 INFO: TorchVersion:1.13.1+cu117+cuda11.7\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,936 INFO: TransformersVersion:4.33.3\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,936 INFO: auto_device:True\t-->\tCalling Count:4\n",
      "2023-10-03 14:51:00,937 INFO: batch_size:16\t-->\tCalling Count:4\n",
      "2023-10-03 14:51:00,938 INFO: cache_dataset:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,939 INFO: checkpoint_save_mode:1\t-->\tCalling Count:4\n",
      "2023-10-03 14:51:00,939 INFO: cross_validate_fold:-1\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,940 INFO: dataset_file:{'train': ['integrated_datasets\\\\atepc_datasets\\\\200.Greek\\\\greek.train.txt.atepc'], 'test': ['integrated_datasets\\\\atepc_datasets\\\\200.Greek\\\\greek.test.txt.atepc'], 'valid': []}\t-->\tCalling Count:6\n",
      "2023-10-03 14:51:00,940 INFO: dataset_name:Greek\t-->\tCalling Count:3\n",
      "2023-10-03 14:51:00,941 INFO: device:cuda:0\t-->\tCalling Count:8\n",
      "2023-10-03 14:51:00,941 INFO: device_name:NVIDIA GeForce RTX 3080 Ti\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,941 INFO: dropout:0.5\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,942 INFO: dynamic_truncate:True\t-->\tCalling Count:7990\n",
      "2023-10-03 14:51:00,942 INFO: embed_dim:768\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,942 INFO: evaluate_begin:0\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,943 INFO: from_checkpoint:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,943 INFO: gradient_accumulation_steps:1\t-->\tCalling Count:3\n",
      "2023-10-03 14:51:00,944 INFO: hidden_dim:768\t-->\tCalling Count:6\n",
      "2023-10-03 14:51:00,944 INFO: index_to_IOB_label:{1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,945 INFO: index_to_label:{0: 'Negative', 1: 'Positive'}\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,945 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,946 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,947 INFO: l2reg:1e-05\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,947 INFO: label_list:['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]']\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,948 INFO: label_to_index:{'Negative': 0, 'Positive': 1}\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,948 INFO: lcf:cdw\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,949 INFO: learning_rate:2e-05\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,949 INFO: load_aug:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,950 INFO: log_step:50\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,951 INFO: logger:<Logger fast_lcf_atepc (INFO)>\t-->\tCalling Count:9\n",
      "2023-10-03 14:51:00,951 INFO: max_seq_len:80\t-->\tCalling Count:27967\n",
      "2023-10-03 14:51:00,951 INFO: model:<class 'pyabsa.tasks.AspectTermExtraction.models.__lcf__.fast_lcf_atepc.FAST_LCF_ATEPC'>\t-->\tCalling Count:5\n",
      "2023-10-03 14:51:00,952 INFO: model_name:fast_lcf_atepc\t-->\tCalling Count:3997\n",
      "2023-10-03 14:51:00,952 INFO: model_path_to_save:checkpoints\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,953 INFO: num_epoch:1\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,953 INFO: num_labels:6\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,954 INFO: optimizer:adamw\t-->\tCalling Count:2\n",
      "2023-10-03 14:51:00,954 INFO: output_dim:2\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,954 INFO: overwrite_cache:False\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,955 INFO: path_to_save:None\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,955 INFO: patience:99999\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,956 INFO: pretrained_bert:nlpaueb/bert-base-greek-uncased-v1\t-->\tCalling Count:5\n",
      "2023-10-03 14:51:00,956 INFO: save_mode:1\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,957 INFO: seed:52\t-->\tCalling Count:7\n",
      "2023-10-03 14:51:00,958 INFO: sep_indices:102\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,958 INFO: show_metric:False\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,959 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:3\n",
      "2023-10-03 14:51:00,959 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,959 INFO: task_code:ATEPC\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,960 INFO: task_name:Aspect Term Extraction and Polarity Classification\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,960 INFO: tokenizer:BertTokenizerFast(name_or_path='nlpaueb/bert-base-greek-uncased-v1', vocab_size=35000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,961 INFO: use_amp:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:51:00,962 INFO: use_bert_spc:True\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,962 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:3995\n",
      "2023-10-03 14:51:00,962 INFO: warmup_step:-1\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,963 INFO: window:lr\t-->\tCalling Count:0\n",
      "2023-10-03 14:51:00,963 INFO: ***** Running training for Aspect Term Extraction and Polarity Classification *****\n",
      "2023-10-03 14:51:00,964 INFO:   Num examples = 1996\n",
      "2023-10-03 14:51:00,964 INFO:   Batch size = 16\n",
      "2023-10-03 14:51:00,965 INFO:   Num steps = 124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Smooth Loss: 1.4410:  19%|█▉        | 24/125 [00:06<00:27,  3.70it/s]d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Epoch:  0 | Smooth Loss: 1.1211:  39%|███▉      | 49/125 [00:28<00:21,  3.60it/s,  APC_ACC: 49.97(max:49.97) | APC_F1: 33.32(max:33.32) | ATE_F1: 89.10(max:89.10)]               d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Epoch:  0 | Smooth Loss: 0.9802:  59%|█████▉    | 74/125 [00:52<00:14,  3.62it/s,  APC_ACC: 50.98(max:50.98) | APC_F1: 36.00(max:36.00) | ATE_F1: 100.00(max:100.00)]               d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Epoch:  0 | Smooth Loss: 0.9022:  79%|███████▉  | 99/125 [01:15<00:07,  3.56it/s,  APC_ACC: 51.63(max:51.63) | APC_F1: 36.97(max:36.97) | ATE_F1: 100.00(max:100.00)]               d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Epoch:  0 | Smooth Loss: 0.8092:  99%|█████████▉| 124/125 [01:38<00:00,  3.86it/s,  APC_ACC: 65.98(max:65.98) | APC_F1: 62.32(max:62.32) | ATE_F1: 100.00(max:100.00)]               d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "Epoch:  0| loss_apc:0.4143 | loss_ate:0.0239 |: 100%|██████████| 125/125 [01:56<00:00,  1.08it/s,  APC_ACC: 92.65(max:92.65) | APC_F1: 92.61(max:92.61) | ATE_F1: 100.00(max:100.00)]\n",
      "d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: [SEP] seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-03 14:53:11,837 INFO: \n",
      "--------------------------------------------------------------------- Raw Metric Records ---------------------------------------------------------------------\n",
      "╒════════════════════════════════╤═════════════════════════════════════════════════════════╤══════════╤═══════════╤══════════╤═══════╤═══════╤═══════╤═══════╕\n",
      "│ Metric                         │ Trial                                                   │ Values   │  Average  │  Median  │  Std  │  IQR  │  Min  │  Max  │\n",
      "╞════════════════════════════════╪═════════════════════════════════════════════════════════╪══════════╪═══════════╪══════════╪═══════╪═══════╪═══════╪═══════╡\n",
      "│ Max-APC-Test-Acc w/o Valid Set │ fast_lcf_atepc-Greek-nlpaueb/bert-base-greek-uncased-v1 │ [92.65]  │   92.65   │  92.65   │   0   │   0   │ 92.65 │ 92.65 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────────────────────┼──────────┼───────────┼──────────┼───────┼───────┼───────┼───────┤\n",
      "│ Max-APC-Test-F1 w/o Valid Set  │ fast_lcf_atepc-Greek-nlpaueb/bert-base-greek-uncased-v1 │ [92.61]  │   92.61   │  92.61   │   0   │   0   │ 92.61 │ 92.61 │\n",
      "├────────────────────────────────┼─────────────────────────────────────────────────────────┼──────────┼───────────┼──────────┼───────┼───────┼───────┼───────┤\n",
      "│ Max-ATE-Test-F1 w/o Valid Set  │ fast_lcf_atepc-Greek-nlpaueb/bert-base-greek-uncased-v1 │ [100.0]  │    100    │   100    │   0   │   0   │  100  │  100  │\n",
      "╘════════════════════════════════╧═════════════════════════════════════════════════════════╧══════════╧═══════════╧══════════╧═══════╧═══════╧═══════╧═══════╛\n",
      "------------------------------------------------------ https://github.com/yangheng95/metric_visualizer ------------------------------------------------------\n",
      "\n",
      "2023-10-03 14:53:11,838 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,838 INFO: IOB_label_to_index:{'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,839 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x000002F751AF6490>\t-->\tCalling Count:4\n",
      "2023-10-03 14:53:11,840 INFO: PyABSAVersion:2.3.3\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,840 INFO: SRD:3\t-->\tCalling Count:7990\n",
      "2023-10-03 14:53:11,841 INFO: TorchVersion:1.13.1+cu117+cuda11.7\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,841 INFO: TransformersVersion:4.33.3\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,841 INFO: auto_device:True\t-->\tCalling Count:129\n",
      "2023-10-03 14:53:11,842 INFO: batch_size:16\t-->\tCalling Count:6\n",
      "2023-10-03 14:53:11,843 INFO: cache_dataset:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,843 INFO: checkpoint_save_mode:1\t-->\tCalling Count:4\n",
      "2023-10-03 14:53:11,843 INFO: cross_validate_fold:-1\t-->\tCalling Count:2\n",
      "2023-10-03 14:53:11,844 INFO: dataset_file:{'train': ['integrated_datasets\\\\atepc_datasets\\\\200.Greek\\\\greek.train.txt.atepc'], 'test': ['integrated_datasets\\\\atepc_datasets\\\\200.Greek\\\\greek.test.txt.atepc'], 'valid': []}\t-->\tCalling Count:6\n",
      "2023-10-03 14:53:11,844 INFO: dataset_name:Greek\t-->\tCalling Count:12\n",
      "2023-10-03 14:53:11,845 INFO: device:cuda:0\t-->\tCalling Count:7888\n",
      "2023-10-03 14:53:11,845 INFO: device_name:NVIDIA GeForce RTX 3080 Ti\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,845 INFO: dropout:0.5\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,845 INFO: dynamic_truncate:True\t-->\tCalling Count:7990\n",
      "2023-10-03 14:53:11,846 INFO: embed_dim:768\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,846 INFO: evaluate_begin:0\t-->\tCalling Count:6\n",
      "2023-10-03 14:53:11,847 INFO: from_checkpoint:None\t-->\tCalling Count:2\n",
      "2023-10-03 14:53:11,847 INFO: gradient_accumulation_steps:1\t-->\tCalling Count:3\n",
      "2023-10-03 14:53:11,847 INFO: hidden_dim:768\t-->\tCalling Count:6\n",
      "2023-10-03 14:53:11,849 INFO: index_to_IOB_label:{1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,849 INFO: index_to_label:{0: 'Negative', 1: 'Positive'}\t-->\tCalling Count:2\n",
      "2023-10-03 14:53:11,850 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,850 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,851 INFO: l2reg:1e-05\t-->\tCalling Count:2\n",
      "2023-10-03 14:53:11,851 INFO: label_list:['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]']\t-->\tCalling Count:7\n",
      "2023-10-03 14:53:11,852 INFO: label_to_index:{'Negative': 0, 'Positive': 1}\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,852 INFO: lcf:cdw\t-->\tCalling Count:1755\n",
      "2023-10-03 14:53:11,852 INFO: learning_rate:2e-05\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,853 INFO: load_aug:False\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,854 INFO: log_step:50\t-->\tCalling Count:126\n",
      "2023-10-03 14:53:11,854 INFO: logger:<Logger fast_lcf_atepc (INFO)>\t-->\tCalling Count:10\n",
      "2023-10-03 14:53:11,854 INFO: loss:0.3787331968545914\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,855 INFO: max_seq_len:80\t-->\tCalling Count:29717\n",
      "2023-10-03 14:53:11,855 INFO: max_test_metrics:{'max_apc_test_acc': 92.65, 'max_apc_test_f1': 92.61, 'max_ate_test_f1': 100.0}\t-->\tCalling Count:50\n",
      "2023-10-03 14:53:11,856 INFO: metrics_of_this_checkpoint:{'apc_acc': 92.65, 'apc_f1': 92.61, 'ate_f1': 100.0}\t-->\tCalling Count:15\n",
      "2023-10-03 14:53:11,856 INFO: model:<class 'pyabsa.tasks.AspectTermExtraction.models.__lcf__.fast_lcf_atepc.FAST_LCF_ATEPC'>\t-->\tCalling Count:5\n",
      "2023-10-03 14:53:11,858 INFO: model_name:fast_lcf_atepc\t-->\tCalling Count:4026\n",
      "2023-10-03 14:53:11,858 INFO: model_path_to_save:checkpoints\t-->\tCalling Count:13\n",
      "2023-10-03 14:53:11,858 INFO: num_epoch:1\t-->\tCalling Count:2\n",
      "2023-10-03 14:53:11,859 INFO: num_labels:6\t-->\tCalling Count:2\n",
      "2023-10-03 14:53:11,859 INFO: optimizer:adamw\t-->\tCalling Count:2\n",
      "2023-10-03 14:53:11,860 INFO: output_dim:2\t-->\tCalling Count:7\n",
      "2023-10-03 14:53:11,860 INFO: overwrite_cache:False\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,861 INFO: path_to_save:None\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,861 INFO: patience:99999\t-->\tCalling Count:6\n",
      "2023-10-03 14:53:11,862 INFO: pretrained_bert:nlpaueb/bert-base-greek-uncased-v1\t-->\tCalling Count:8\n",
      "2023-10-03 14:53:11,862 INFO: save_mode:1\t-->\tCalling Count:10\n",
      "2023-10-03 14:53:11,863 INFO: seed:52\t-->\tCalling Count:7\n",
      "2023-10-03 14:53:11,864 INFO: sep_indices:102\t-->\tCalling Count:25984\n",
      "2023-10-03 14:53:11,864 INFO: show_metric:False\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,865 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:3\n",
      "2023-10-03 14:53:11,865 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,865 INFO: task_code:ATEPC\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,866 INFO: task_name:Aspect Term Extraction and Polarity Classification\t-->\tCalling Count:1\n",
      "2023-10-03 14:53:11,866 INFO: tokenizer:BertTokenizerFast(name_or_path='nlpaueb/bert-base-greek-uncased-v1', vocab_size=35000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\t-->\tCalling Count:0\n",
      "2023-10-03 14:53:11,866 INFO: use_amp:False\t-->\tCalling Count:251\n",
      "2023-10-03 14:53:11,867 INFO: use_bert_spc:True\t-->\tCalling Count:1625\n",
      "2023-10-03 14:53:11,867 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:3995\n",
      "2023-10-03 14:53:11,868 INFO: warmup_step:-1\t-->\tCalling Count:127\n",
      "2023-10-03 14:53:11,868 INFO: window:lr\t-->\tCalling Count:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\metric_visualizer\\utils.py:31: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  self.skewness = stats.skew(self.data, keepdims=True)\n",
      "d:\\ANACONDA\\envs\\pyabsa2.3\\lib\\site-packages\\pyabsa\\framework\\trainer_class\\trainer_template.py:251: ResourceWarning: unclosed file <_io.TextIOWrapper name='f:\\\\PURECLONES\\\\AI-in-Classics\\\\src\\\\PYABSA\\\\PYABSA\\\\logs\\\\fast_lcf_atepc_20231003 143440\\\\trainer.log' mode='a' encoding='utf8'>\n",
      "  self.config.logger.removeHandler(self.config.logger.handlers[0])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-03 14:53:14] (2.3.3) Load aspect extractor from checkpoints/fast_lcf_atepc_Greek_cdw_apcacc_92.65_apcf1_92.61_atef1_100.0/\n",
      "[2023-10-03 14:53:14] (2.3.3) config: checkpoints/fast_lcf_atepc_Greek_cdw_apcacc_92.65_apcf1_92.61_atef1_100.0/fast_lcf_atepc.config\n",
      "[2023-10-03 14:53:14] (2.3.3) state_dict: checkpoints/fast_lcf_atepc_Greek_cdw_apcacc_92.65_apcf1_92.61_atef1_100.0/fast_lcf_atepc.state_dict\n",
      "[2023-10-03 14:53:14] (2.3.3) model: None\n",
      "[2023-10-03 14:53:14] (2.3.3) tokenizer: checkpoints/fast_lcf_atepc_Greek_cdw_apcacc_92.65_apcf1_92.61_atef1_100.0/fast_lcf_atepc.tokenizer\n",
      "[2023-10-03 14:53:14] (2.3.3) Set Model Device: cuda:0\n",
      "[2023-10-03 14:53:14] (2.3.3) Device Name: NVIDIA GeForce RTX 3080 Ti\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\natepc_examples = [\\n    \"But the staff was so nice to us .\",\\n    \"But the staff was so horrible to us .\",\\n    r\"Not only was the food outstanding , but the little ` perks \\' were great .\",\\n    \"It took half an hour to get our check , which was perfect since we could sit , have drinks and talk !\",\\n    \"It was pleasantly uncrowded , the service was delightful , the garden adorable , \"\\n    \"the food -LRB- from appetizers to entrees -RRB- was delectable .\",\\n    \"How pretentious and inappropriate for MJ Grill to claim that it provides power lunch and dinners !\",\\n]\\naspect_extractor.batch_predict(\\n    target_file=atepc_examples,  #\\n    save_result=True,\\n    print_result=True,  # print the result\\n    pred_sentiment=True,  # Predict the sentiment of extracted aspect terms\\n)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from pyabsa import AspectTermExtraction as ATEPC\n",
    "from pyabsa import DatasetItem\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.__version__)\n",
    "\n",
    "config = ATEPC.ATEPCConfigManager.get_atepc_config_multilingual()\n",
    "config.model = ATEPC.ATEPCModelList.FAST_LCF_ATEPC\n",
    "config.num_epoch = 2\n",
    "config.cache_dataset = False\n",
    "config.pretrained_bert = 'nlpaueb/bert-base-greek-uncased-v1'\n",
    "dataset_path =DatasetItem(\"Greek\", \"200.Greek\")\n",
    "\n",
    "#config.evaluate_begin = 0\n",
    "#config.max_seq_len = 128\n",
    "#config.batch_size = 16\n",
    "\n",
    "#config.pretrained_bert = 'yangheng/deberta-v3-base-absa'\n",
    "#config.pretrained_bert = \"microsoft/mdeberta-v3-base\"\n",
    "#config.pretrained_bert = 'nlpaueb/bert-base-greek-uncased-v1'\n",
    "\n",
    "#config.log_step = -1\n",
    "#config.l2reg = 1e-8\n",
    "#config.num_epoch = 20\n",
    "#config.seed = 42\n",
    "#config.use_bert_spc = True\n",
    "#config.use_amp = False\n",
    "#config.cache_dataset = True\n",
    "#config.cross_validate_fold = -1\n",
    "\n",
    "# chinese_sets = ATEPC.ATEPCDatasetList.Chinese_Zhang\n",
    "\n",
    "#dataset_path =DatasetItem(\"Laptop14\", \"113.Laptop14\")\n",
    "#dataset_path = ATEPC.ATEPCDatasetList.Multilingual\n",
    "'''\n",
    "dataset_path = DatasetItem( #multilingual\n",
    "        \"Multilingual\",\n",
    "        [\n",
    "            \"113.Laptop14\",\n",
    "            \"114.Restaurant14\",\n",
    "            \"116.Restaurant16\",\n",
    "            \"101.ACL_Twitter\",\n",
    "            \"109.MAMS\",\n",
    "            \"117.Television\",\n",
    "            \"118.TShirt\",\n",
    "            \"119.Yelp\",\n",
    "            \"107.Phone\",\n",
    "            \"103.Camera\",\n",
    "            \"106.Notebook\",\n",
    "            \"104.Car\",\n",
    "            \"105.MOOC\",\n",
    "            \"129.Kaggle\",\n",
    "            \"120.SemEval2016Task5\",\n",
    "            \"121.MOOC_En\",\n",
    "            \"130.Chinese_Zhang\",\n",
    "        ],\n",
    "    )\n",
    "'''\n",
    "\n",
    "aspect_extractor = ATEPC.ATEPCTrainer(\n",
    "    config=config,\n",
    "    # from_checkpoint=\"\",   # not necessary for most situations\n",
    "    dataset=dataset_path,\n",
    "    checkpoint_save_mode=1,\n",
    "    auto_device=True,\n",
    "    load_aug=False,\n",
    ").load_trained_model()\n",
    "'''\n",
    "atepc_examples = [\n",
    "    \"But the staff was so nice to us .\",\n",
    "    \"But the staff was so horrible to us .\",\n",
    "    r\"Not only was the food outstanding , but the little ` perks \\' were great .\",\n",
    "    \"It took half an hour to get our check , which was perfect since we could sit , have drinks and talk !\",\n",
    "    \"It was pleasantly uncrowded , the service was delightful , the garden adorable , \"\n",
    "    \"the food -LRB- from appetizers to entrees -RRB- was delectable .\",\n",
    "    \"How pretentious and inappropriate for MJ Grill to claim that it provides power lunch and dinners !\",\n",
    "]\n",
    "aspect_extractor.batch_predict(\n",
    "    target_file=atepc_examples,  #\n",
    "    save_result=True,\n",
    "    print_result=True,  # print the result\n",
    "    pred_sentiment=True,  # Predict the sentiment of extracted aspect terms\n",
    ")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyabsa2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
