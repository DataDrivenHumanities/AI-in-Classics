{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JetnfZMs2Kcl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"36342bc2-c5ca-41fc-d1a2-d6cb3c5b4a55","executionInfo":{"status":"ok","timestamp":1695918735126,"user_tz":240,"elapsed":34194,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/sjgallagher2/PyWORDS\n","  Cloning https://github.com/sjgallagher2/PyWORDS to /tmp/pip-req-build-25pc4kvf\n","  Running command git clone --filter=blob:none --quiet https://github.com/sjgallagher2/PyWORDS /tmp/pip-req-build-25pc4kvf\n","  Resolved https://github.com/sjgallagher2/PyWORDS to commit 815f1fad4c560e89c6200df1e0833e2cdf062338\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pywords==0.0.1) (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pywords==0.0.1) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pywords==0.0.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pywords==0.0.1) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pywords==0.0.1) (2023.7.22)\n","Requirement already satisfied: cltk in /usr/local/lib/python3.10/dist-packages (1.1.6)\n","Requirement already satisfied: PyYAML<7.0.0,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (6.0.1)\n","Requirement already satisfied: boltons<22.0.0,>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (21.0.0)\n","Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (4.3.2)\n","Requirement already satisfied: gitpython<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.1.37)\n","Requirement already satisfied: greek-accentuation<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.2.0)\n","Requirement already satisfied: nltk<4.0,>=3.7 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.8.1)\n","Requirement already satisfied: python-Levenshtein<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (0.12.2)\n","Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (2.31.0)\n","Requirement already satisfied: scikit-learn<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.2.2)\n","Requirement already satisfied: scipy<2.0.0,>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.11.2)\n","Requirement already satisfied: spacy<4.0.0,>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.6.1)\n","Requirement already satisfied: stanza<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.5.1)\n","Requirement already satisfied: stringcase<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.2.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from cltk) (4.66.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->cltk) (1.23.5)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->cltk) (6.4.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4.0,>=3.0->cltk) (4.0.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (2023.6.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein<0.13.0,>=0.12.0->cltk) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.2->cltk) (3.2.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (0.10.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.10.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.1.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.3.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (2.8.0)\n","Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (3.20.3)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (2.0.1+cu118)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4.0,>=3.0->cltk) (5.0.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.2.4->cltk) (4.5.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.2.4->cltk) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.2.4->cltk) (0.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.2.4->cltk) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (1.3.0)\n"]}],"source":["!pip install git+https://github.com/sjgallagher2/PyWORDS\n","!pip install cltk\n","import random\n","import requests\n","import csv\n","import os\n","import pandas as pd\n","from bs4 import BeautifulSoup as bs\n","from urllib.parse import quote\n","import re\n","import unicodedata\n","import pywords.lookup as lookup\n","#from urllib.request import urlopen\n","#pip install html5lib\n","#pip install lxml"]},{"cell_type":"code","source":["import pywords.lookup as lookup\n","\n","word = 'lacedaemon'\n","# High level\n","lookup.lookup_word(word)  # Print matching words\n","\n","# Lower level\n","# matches = lookup.match_word(word)\n","# for match in matches:\n","#      print(lookup.get_dictionary_string(match,full_info=True))\n"],"metadata":{"id":"KmRDSvgyW_hb","executionInfo":{"status":"ok","timestamp":1695918735127,"user_tz":240,"elapsed":41,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["for match in lookup.match_word('graecum'): # Match possible words\n","    print(lookup.get_dictionary_string(match, False, True, False).split(' ')[0].replace(',',\"\"))  # Start with the definition of the word\n"],"metadata":{"id":"kfI0Tk-dtMv0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695918735127,"user_tz":240,"elapsed":39,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}},"outputId":"20c6db9c-7b5c-419a-8099-26c687bfff10"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["graecus\n"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)\n","PATH= 'drive/Shareddrives/AI in Classics/GreekABASAModel/DataSetGenerator'\n","\n","os.chdir(PATH)# sets the current active directory to the current folder\n","!pwd\n","# IF YOU ENCOUNTER AN ERROR FIRST TRY RESTARTING RUNTIME"],"metadata":{"id":"GFQI8g-E2QJx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695918755006,"user_tz":240,"elapsed":19914,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}},"outputId":"fdf0c832-198a-4fb0-b50e-e12d5a8df48e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Shareddrives/AI in Classics/GreekABASAModel/DataSetGenerator\n"]}]},{"cell_type":"code","source":["!pip install cltk\n","import pandas as pd\n","from cltk.morphology.lat import CollatinusDecliner\n","from cltk.data.fetch import FetchCorpus\n","corpus_importer = FetchCorpus(\"lat\")\n","print(corpus_importer.list_corpora)\n","corpus_importer.import_corpus(\"lat_models_cltk\")\n","\n","#test2 #allot easier proably just going to use this\n","#creates inflections for nouns and adjectives\n","#creates conjugations for verbs\n","def strip_accents_and_lowercase(s):\n","   return ''.join(c for c in unicodedata.normalize('NFD', s)\n","                  if unicodedata.category(c) != 'Mn').lower()\n","\n","\n","def generate_variations(word,language='greek',show=False):\n","      # CREATES A FOLDER STRUCTURE THAT CONTAINS WORDS ORGANIZED BY PART OF SPEECH\n","      #DECLINES WORDS\n","      #test words\n","      #'Ῥωμαῖος' romans\n","      #'μισώ' hate\n","      #'εἰμί' are to be\n","      #word = word.lower()\n","\n","# takes in a word as a string and uses the collatinus decliner to return\n","# the different forms of the word. returns words which is a list that contains\n","# tuples formated like so: [word, (part of speech, person, number, tense, mood, voice, gender, case, degree)]\n","  def decline(word):\n","    decliner = CollatinusDecliner()\n","    words = decliner.decline(word)\n","    return words\n","\n","  # takes in a declined word object (list of words in the format returned by the decliner function) and\n","  # formats it so that each word is with the correct type (nominative, genative, etc.)\n","  # currently only works with nouns that have one gender. needs to be expanded so it can make\n","  # dictionaries for multiple genders and word types.\n","  def fill_dicts(word, neutral = False, masc = False, fem = False):\n","      singular_dict = {}\n","      plural_dict = {}\n","      for i in range(0, len(word)):\n","\n","        #check if verb\n","        if word[i][1][0] == \"v\":\n","\n","          #check if singular verb\n","          if word[i][1][1:] == \"1spia---\":\n","            singular_dict[\"First Person\"] = (word[i][0])\n","          if word[i][1][1:] == \"2spia---\":\n","            singular_dict[\"Second Person\"] = (word[i][0])\n","          if word[i][1][1:] == \"3spia---\":\n","            singular_dict[\"Third Person\"] = (word[i][0])\n","          #check if plural verb\n","          if word[i][1][1:] == \"1ppia---\":\n","            plural_dict[\"First Person\"] = (word[i][0])\n","          if word[i][1][1:] == \"2ppia---\":\n","            plural_dict[\"Second Person\"] = (word[i][0])\n","          if word[i][1][1:] == \"3ppia---\":\n","            plural_dict[\"Third Person\"] = (word[i][0])\n","\n","        # check if singular\n","        elif word[i][1][0] == \"-\":\n","          if word[i][1][2] == \"s\":\n","            if word[i][1][7] == \"n\":\n","              singular_dict[\"Nominative\"] = (word[i][0])\n","            elif word[i][1][7] == \"g\":\n","              singular_dict[\"Genative\"] = (word[i][0])\n","            elif word[i][1][7] == \"d\":\n","              singular_dict[\"Dative\"] = (word[i][0])\n","            elif word[i][1][7] == \"a\":\n","              singular_dict[\"Accusative\"] = (word[i][0])\n","            elif word[i][1][7] == \"b\":\n","              singular_dict[\"Ablative\"] = (word[i][0])\n","            elif word[i][1][7] == \"v\":\n","              singular_dict[\"Vocative\"] = (word[i][0])\n","          # check if plural\n","          elif word[i][1][2] == \"p\":\n","            if word[i][1][7] == \"n\":\n","              plural_dict[\"Nominative\"] = (word[i][0])\n","            elif word[i][1][7] == \"g\":\n","              plural_dict[\"Genative\"] = (word[i][0])\n","            elif word[i][1][7] == \"d\":\n","              plural_dict[\"Dative\"] = (word[i][0])\n","            elif word[i][1][7] == \"a\":\n","              plural_dict[\"Accusative\"] = (word[i][0])\n","            elif word[i][1][7] == \"b\":\n","              plural_dict[\"Ablative\"] = (word[i][0])\n","            elif word[i][1][7] == \"v\":\n","              plural_dict[\"Vocative\"] = (word[i][0])\n","      return singular_dict, plural_dict\n","\n","\n","\n","\n","  def make_df(word_tup, noun = False):\n","    print(word_tup)\n","\n","    #gender = set()\n","    #for i in range(0,len(word)):\n","    #  gender.add(word[i][1][6])\n","    # runs if the word has no gendered attributes\n","    #if '-' in gender or 'n' in gender:\n","    singular_dict, plural_dict = fill_dicts(word_tup, True)\n","    print(singular_dict, plural_dict)\n","    print()\n","\n","    if word_tup[1][1][0] == \"v\":\n","        dict_verb = {\"Person\": [\"First Person\", \"Second Person\", \"Third Person\"], \"Singular\":[], \"Plural\":[]}\n","        dict_verb[\"Singular\"].append(singular_dict[\"First Person\"])\n","        dict_verb[\"Singular\"].append(singular_dict[\"Second Person\"])\n","        dict_verb[\"Singular\"].append(singular_dict[\"Third Person\"])\n","        dict_verb[\"Plural\"].append(singular_dict[\"First Person\"])\n","        dict_verb[\"Plural\"].append(singular_dict[\"Second Person\"])\n","        dict_verb[\"Plural\"].append(singular_dict[\"Third Person\"])\n","        df = pd.DataFrame(dict_verb)\n","        df = df.set_index(\"Person\")\n","        return df\n","\n","    if word_tup[1][1][0] == \"-\":\n","      dict_overall = {\"Case\":[\"Nominative\", \"Genative\", \"Dative\", \"Accusative\", \"Ablative\", \"Vocative\"], \"Singular\":[], \"Plural\":[]}\n","      dict_overall[\"Singular\"].append(singular_dict[\"Nominative\"])\n","      dict_overall[\"Singular\"].append(singular_dict[\"Genative\"])\n","      dict_overall[\"Singular\"].append(singular_dict[\"Dative\"])\n","      dict_overall[\"Singular\"].append(singular_dict[\"Accusative\"])\n","      dict_overall[\"Singular\"].append(singular_dict[\"Ablative\"])\n","      dict_overall[\"Singular\"].append(singular_dict[\"Vocative\"])\n","\n","      dict_overall[\"Plural\"].append(plural_dict[\"Nominative\"])\n","      dict_overall[\"Plural\"].append(plural_dict[\"Genative\"])\n","      dict_overall[\"Plural\"].append(plural_dict[\"Dative\"])\n","      dict_overall[\"Plural\"].append(plural_dict[\"Accusative\"])\n","      dict_overall[\"Plural\"].append(plural_dict[\"Ablative\"])\n","      dict_overall[\"Plural\"].append(plural_dict[\"Vocative\"])\n","      df = pd.DataFrame(dict_overall)\n","      return df\n","    if 'm' in gender:\n","          dict_masc = {\"Case\":[\"Nominative\", \"Genative\", \"Dative\", \"Accusative\", \"Ablative\", \"Vocative\"], \"Singular\":[], \"Plural\":[]}\n","\n","    else:\n","      print(\"not valid at the moment\")\n","      print(gender)\n","      return -1\n","\n","\n","  df = make_df(decline(word))\n","  return df\n","#print(make_df(word))\n","\n","\n","# formated by [word, (part of speech, person, number, tense, mood, voice, gender, case, degree)]\n","# https://github.com/bab2min/lamonpy\n","\n","      ##display(df)\n","      ##CREATE A FOLDER\n","  root = 'latin_variations_new'\n","\n","  # NAN ARE REPALCED BY SPACE CHARACTER\n","  df.to_csv(f'{root}/{word}.csv')\n","\n","\n","  if(show):display(df)\n","  ## READ\n","  #print('READ FROM FILE ------------------------------------')\n","  #display(pd.read_csv(f'{root}/{h3[i]}/{word}/{nav[i]}_{h4[i]}.csv'))\n","        #----------------------------------\n","#generate_variations('Ῥωμαῖος','greek',show=True)\n","generate_variations('ambulo','latin',show=True)\n","#test words\n","#'Ῥωμαῖος' romans\n","#'μισώ' hate\n","#'εἰμί' is/are to be\n","#ελληνικά greeks\n","#odium hatred\n","#romanus\n","#odio hate\n","#απεχθάνομαι\n","#lacedaemon\n","\n","# TO SHOW THE DATAFRAME PASS SHOW AS TRUE\n","\n","\n","\n"],"metadata":{"id":"AKRv09Mz2RNE","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1695927242308,"user_tz":240,"elapsed":47436,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}},"outputId":"63c3948d-17f4-4c95-f496-0588102905aa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting cltk\n","  Downloading cltk-1.1.6-py3-none-any.whl (845 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m845.0/845.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML<7.0.0,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (6.0.1)\n","Collecting boltons<22.0.0,>=21.0.0 (from cltk)\n","  Downloading boltons-21.0.0-py2.py3-none-any.whl (193 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (4.3.2)\n","Collecting gitpython<4.0,>=3.0 (from cltk)\n","  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting greek-accentuation<2.0.0,>=1.2.0 (from cltk)\n","  Downloading greek_accentuation-1.2.0-py2.py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: nltk<4.0,>=3.7 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.8.1)\n","Collecting python-Levenshtein<0.13.0,>=0.12.0 (from cltk)\n","  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (2.31.0)\n","Requirement already satisfied: scikit-learn<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.2.2)\n","Requirement already satisfied: scipy<2.0.0,>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.11.2)\n","Requirement already satisfied: spacy<4.0.0,>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.6.1)\n","Collecting stanza<2.0.0,>=1.3.0 (from cltk)\n","  Downloading stanza-1.5.1-py3-none-any.whl (865 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting stringcase<2.0,>=1.2 (from cltk)\n","  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from cltk) (4.66.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->cltk) (1.23.5)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->cltk) (6.4.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython<4.0,>=3.0->cltk)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (2023.6.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein<0.13.0,>=0.12.0->cltk) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.2->cltk) (3.2.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (0.10.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.10.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.1.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.3.0)\n","Collecting emoji (from stanza<2.0.0,>=1.3.0->cltk)\n","  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (3.20.3)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (2.0.1+cu118)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4.0,>=3.0->cltk)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.2.4->cltk) (4.5.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.2.4->cltk) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.2.4->cltk) (0.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.2.4->cltk) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (1.3.0)\n","Building wheels for collected packages: python-Levenshtein, stringcase\n","  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp310-cp310-linux_x86_64.whl size=159968 sha256=7f286e2a4c8f6dcdefa2f6f43c5aa9339a3b9186bb38701553712ae9420effdd\n","  Stored in directory: /root/.cache/pip/wheels/7b/c3/05/60b4747cf52e0f6b6ee52022088a4de07d755016493e86373d\n","  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3568 sha256=7e2d1e749f2e36471f050bb618952cc8010f073837670e8e40dfca9568ce5f1e\n","  Stored in directory: /root/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\n","Successfully built python-Levenshtein stringcase\n","Installing collected packages: stringcase, greek-accentuation, boltons, smmap, python-Levenshtein, emoji, gitdb, gitpython, stanza, cltk\n","Successfully installed boltons-21.0.0 cltk-1.1.6 emoji-2.8.0 gitdb-4.0.10 gitpython-3.1.37 greek-accentuation-1.2.0 python-Levenshtein-0.12.2 smmap-5.0.1 stanza-1.5.1 stringcase-1.2.0\n"]},{"output_type":"stream","name":"stderr","text":["INFO:CLTK:Cloning 'lat_models_cltk' from 'https://github.com/cltk/lat_models_cltk.git'\n"]},{"output_type":"stream","name":"stdout","text":["['lat_text_perseus', 'lat_treebank_perseus', 'lat_text_latin_library', 'phi5', 'phi7', 'latin_proper_names_cltk', 'lat_models_cltk', 'latin_pos_lemmata_cltk', 'latin_treebank_index_thomisticus', 'latin_lexica_perseus', 'latin_training_set_sentence_cltk', 'latin_word2vec_cltk', 'latin_text_antique_digiliblt', 'latin_text_corpus_grammaticorum_latinorum', 'latin_text_poeti_ditalia', 'lat_text_tesserae', 'cltk_lat_lewis_elementary_lexicon']\n","[('ambulo', 'v1spia---'), ('ambulas', 'v2spia---'), ('ambulat', 'v3spia---'), ('ambulamus', 'v1ppia---'), ('ambulatis', 'v2ppia---'), ('ambulant', 'v3ppia---'), ('ambulabam', 'v1siia---'), ('ambulabas', 'v2siia---'), ('ambulabat', 'v3siia---'), ('ambulabamus', 'v1piia---'), ('ambulabatis', 'v2piia---'), ('ambulabant', 'v3piia---'), ('ambulabo', 'v1sfia---'), ('ambulabis', 'v2sfia---'), ('ambulabit', 'v3sfia---'), ('ambulabimus', 'v1pfia---'), ('ambulabitis', 'v2pfia---'), ('ambulabunt', 'v3pfia---'), ('ambulavi', 'v1sria---'), ('ambulavisti', 'v2sria---'), ('ambulavit', 'v3sria---'), ('ambulavimus', 'v1pria---'), ('ambulavistis', 'v2pria---'), ('ambulaverunt', 'v3pria---'), ('ambulavere', 'v3pria---'), ('ambulaveram', 'v1slia---'), ('ambulaveras', 'v2slia---'), ('ambulaverat', 'v3slia---'), ('ambulaveramus', 'v1plia---'), ('ambulaveratis', 'v2plia---'), ('ambulaverant', 'v3plia---'), ('ambulavero', 'v1stia---'), ('ambulaveris', 'v2stia---'), ('ambulaverit', 'v3stia---'), ('ambulaverimus', 'v1ptia---'), ('ambulaveritis', 'v2ptia---'), ('ambulaverint', 'v3ptia---'), ('ambulem', 'v1spsa---'), ('ambules', 'v2spsa---'), ('ambulet', 'v3spsa---'), ('ambulemus', 'v1ppsa---'), ('ambuletis', 'v2ppsa---'), ('ambulent', 'v3ppsa---'), ('ambularem', 'v1sisa---'), ('ambulares', 'v2sisa---'), ('ambularet', 'v3sisa---'), ('ambularemus', 'v1pisa---'), ('ambularetis', 'v2pisa---'), ('ambularent', 'v3pisa---'), ('ambulaverim', 'v1srsa---'), ('ambulaveris', 'v2srsa---'), ('ambulaverit', 'v3srsa---'), ('ambulaverimus', 'v1prsa---'), ('ambulaveritis', 'v2prsa---'), ('ambulaverint', 'v3prsa---'), ('ambulavissem', 'v1slsa---'), ('ambulavisses', 'v2slsa---'), ('ambulavisset', 'v3slsa---'), ('ambulavissemus', 'v1plsa---'), ('ambulavissetis', 'v2plsa---'), ('ambulavissent', 'v3plsa---'), ('ambula', 'v2spma---'), ('ambulate', 'v2ppma---'), ('ambulato', 'v2sfma---'), ('ambulato', 'v3sfma---'), ('ambulatote', 'v2pfma---'), ('ambulanto', 'v3pfma---'), ('ambulare', 'v--pna---'), ('ambulasse', 'v--rna---'), ('ambulans', 'g-sppamn-'), ('ambulans', 'g-sppamv-'), ('ambulantem', 'g-sppama-'), ('ambulantis', 'g-sppamg-'), ('ambulanti', 'g-sppamd-'), ('ambulante', 'g-sppamb-'), ('ambulantes', 'g-pppamn-'), ('ambulantes', 'g-pppamv-'), ('ambulantes', 'g-pppama-'), ('ambulantium', 'g-pppamg-'), ('ambulantum', 'g-pppamg-'), ('ambulantibus', 'g-pppamd-'), ('ambulantibus', 'g-pppamb-'), ('ambulans', 'g-sppafn-'), ('ambulans', 'g-sppafv-'), ('ambulantem', 'g-sppafa-'), ('ambulantis', 'g-sppafg-'), ('ambulanti', 'g-sppafd-'), ('ambulante', 'g-sppafb-'), ('ambulantes', 'g-pppafn-'), ('ambulantes', 'g-pppafv-'), ('ambulantes', 'g-pppafa-'), ('ambulantium', 'g-pppafg-'), ('ambulantum', 'g-pppafg-'), ('ambulantibus', 'g-pppafd-'), ('ambulantibus', 'g-pppafb-'), ('ambulans', 'g-sppann-'), ('ambulans', 'g-sppanv-'), ('ambulans', 'g-sppana-'), ('ambulantis', 'g-sppang-'), ('ambulanti', 'g-sppand-'), ('ambulante', 'g-sppanb-'), ('ambulantia', 'g-pppann-'), ('ambulantia', 'g-pppanv-'), ('ambulantia', 'g-pppana-'), ('ambulantium', 'g-pppang-'), ('ambulantum', 'g-pppang-'), ('ambulantibus', 'g-pppand-'), ('ambulantibus', 'g-pppanb-'), ('ambulaturus', 'g-sfpamn-'), ('ambulature', 'g-sfpamv-'), ('ambulaturum', 'g-sfpama-'), ('ambulaturi', 'g-sfpamg-'), ('ambulaturo', 'g-sfpamd-'), ('ambulaturo', 'g-sfpamb-'), ('ambulaturi', 'g-pfpamn-'), ('ambulaturi', 'g-pfpamv-'), ('ambulaturos', 'g-pfpama-'), ('ambulaturorum', 'g-pfpamg-'), ('ambulaturis', 'g-pfpamd-'), ('ambulaturis', 'g-pfpamb-'), ('ambulatura', 'g-sfpafn-'), ('ambulatura', 'g-sfpafv-'), ('ambulaturam', 'g-sfpafa-'), ('ambulaturae', 'g-sfpafg-'), ('ambulaturae', 'g-sfpafd-'), ('ambulatura', 'g-sfpafb-'), ('ambulaturae', 'g-pfpafn-'), ('ambulaturae', 'g-pfpafv-'), ('ambulaturas', 'g-pfpafa-'), ('ambulaturarum', 'g-pfpafg-'), ('ambulaturis', 'g-pfpafd-'), ('ambulaturis', 'g-pfpafb-'), ('ambulaturum', 'g-sfpann-'), ('ambulaturum', 'g-sfpanv-'), ('ambulaturum', 'g-sfpana-'), ('ambulaturi', 'g-sfpang-'), ('ambulaturo', 'g-sfpand-'), ('ambulaturo', 'g-sfpanb-'), ('ambulatura', 'g-pfpann-'), ('ambulatura', 'g-pfpanv-'), ('ambulatura', 'g-pfpana-'), ('ambulaturorum', 'g-pfpang-'), ('ambulaturis', 'g-pfpand-'), ('ambulaturis', 'g-pfpanb-'), ('ambulandum', '----g--a-'), ('ambulandi', '----g--g-'), ('ambulando', '----g--d-'), ('ambulando', '----g--b-'), ('ambulatum', '----u----'), ('ambulatu', '----u----'), ('ambulor', 'v1spip---'), ('ambularis', 'v2spip---'), ('ambulare', 'v2spip---'), ('ambulatur', 'v3spip---'), ('ambulamur', 'v1ppip---'), ('ambulamini', 'v2ppip---'), ('ambulantur', 'v3ppip---'), ('ambulabar', 'v1siip---'), ('ambulabaris', 'v2siip---'), ('ambulabare', 'v2siip---'), ('ambulabatur', 'v3siip---'), ('ambulabamur', 'v1piip---'), ('ambulabamini', 'v2piip---'), ('ambulabantur', 'v3piip---'), ('ambulabor', 'v1sfip---'), ('ambulaberis', 'v2sfip---'), ('ambulabere', 'v2sfip---'), ('ambulabitur', 'v3sfip---'), ('ambulabimur', 'v1pfip---'), ('ambulabimini', 'v2pfip---'), ('ambulabuntur', 'v3pfip---'), ('ambuler', 'v1spsp---'), ('ambuleris', 'v2spsp---'), ('ambulere', 'v2spsp---'), ('ambuletur', 'v3spsp---'), ('ambulemur', 'v1ppsp---'), ('ambulemini', 'v2ppsp---'), ('ambulentur', 'v3ppsp---'), ('ambularer', 'v1sisp---'), ('ambulareris', 'v2sisp---'), ('ambularere', 'v2sisp---'), ('ambularetur', 'v3sisp---'), ('ambularemur', 'v1pisp---'), ('ambularemini', 'v2pisp---'), ('ambularentur', 'v3pisp---'), ('ambulare', 'v2spmp---'), ('ambulamini', 'v2ppmp---'), ('ambulator', 'v2sfmp---'), ('ambulator', 'v3sfmp---'), ('ambulantor', 'v3pfmp---'), ('ambulari', 'v--pnp---'), ('ambulatus', 'g-srppmn-'), ('ambulate', 'g-srppmv-'), ('ambulatum', 'g-srppma-'), ('ambulati', 'g-srppmg-'), ('ambulato', 'g-srppmd-'), ('ambulato', 'g-srppmb-'), ('ambulati', 'g-prppmn-'), ('ambulati', 'g-prppmv-'), ('ambulatos', 'g-prppma-'), ('ambulatorum', 'g-prppmg-'), ('ambulatis', 'g-prppmd-'), ('ambulatis', 'g-prppmb-'), ('ambulata', 'g-srppfn-'), ('ambulata', 'g-srppfv-'), ('ambulatam', 'g-srppfa-'), ('ambulatae', 'g-srppfg-'), ('ambulatae', 'g-srppfd-'), ('ambulata', 'g-srppfb-'), ('ambulatae', 'g-prppfn-'), ('ambulatae', 'g-prppfv-'), ('ambulatas', 'g-prppfa-'), ('ambulatarum', 'g-prppfg-'), ('ambulatis', 'g-prppfd-'), ('ambulatis', 'g-prppfb-'), ('ambulatum', 'g-srppnn-'), ('ambulatum', 'g-srppnv-'), ('ambulatum', 'g-srppna-'), ('ambulati', 'g-srppng-'), ('ambulato', 'g-srppnd-'), ('ambulato', 'g-srppnb-'), ('ambulata', 'g-prppnn-'), ('ambulata', 'g-prppnv-'), ('ambulata', 'g-prppna-'), ('ambulatorum', 'g-prppng-'), ('ambulatis', 'g-prppnd-'), ('ambulatis', 'g-prppnb-'), ('ambulandus', '--s-g-mn-'), ('ambulande', '--s-g-mv-'), ('ambulandum', '--s-g-ma-'), ('ambulandi', '--s-g-mg-'), ('ambulando', '--s-g-md-'), ('ambulando', '--s-g-mb-'), ('ambulandi', '--p-g-mn-'), ('ambulandi', '--p-g-mv-'), ('ambulandos', '--p-g-ma-'), ('ambulandorum', '--p-g-mg-'), ('ambulandis', '--p-g-md-'), ('ambulandis', '--p-g-mb-'), ('ambulanda', '--s-g-fn-'), ('ambulanda', '--s-g-fv-'), ('ambulandam', '--s-g-fa-'), ('ambulandae', '--s-g-fg-'), ('ambulandae', '--s-g-fd-'), ('ambulanda', '--s-g-fb-'), ('ambulandae', '--p-g-fn-'), ('ambulandae', '--p-g-fv-'), ('ambulandas', '--p-g-fa-'), ('ambulandarum', '--p-g-fg-'), ('ambulandis', '--p-g-fd-'), ('ambulandis', '--p-g-fb-'), ('ambulandum', '--s-g-nn-'), ('ambulandum', '--s-g-nv-'), ('ambulandum', '--s-g-na-'), ('ambulandi', '--s-g-ng-'), ('ambulando', '--s-g-nd-'), ('ambulando', '--s-g-nb-'), ('ambulanda', '--p-g-nn-'), ('ambulanda', '--p-g-nv-'), ('ambulanda', '--p-g-na-'), ('ambulandorum', '--p-g-ng-'), ('ambulandis', '--p-g-nd-'), ('ambulandis', '--p-g-nb-')]\n","{'First Person': 'ambulo', 'Second Person': 'ambulas', 'Third Person': 'ambulat', 'Nominative': 'ambulandum', 'Vocative': 'ambulandum', 'Accusative': 'ambulandum', 'Genative': 'ambulandi', 'Dative': 'ambulando', 'Ablative': 'ambulando'} {'First Person': 'ambulamus', 'Second Person': 'ambulatis', 'Third Person': 'ambulant', 'Nominative': 'ambulanda', 'Vocative': 'ambulanda', 'Accusative': 'ambulanda', 'Genative': 'ambulandorum', 'Dative': 'ambulandis', 'Ablative': 'ambulandis'}\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["              Singular   Plural\n","Person                         \n","First Person    ambulo   ambulo\n","Second Person  ambulas  ambulas\n","Third Person   ambulat  ambulat"],"text/html":["\n","  <div id=\"df-0cc7d665-339d-4fba-8ec5-f34900e035d9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Singular</th>\n","      <th>Plural</th>\n","    </tr>\n","    <tr>\n","      <th>Person</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>First Person</th>\n","      <td>ambulo</td>\n","      <td>ambulo</td>\n","    </tr>\n","    <tr>\n","      <th>Second Person</th>\n","      <td>ambulas</td>\n","      <td>ambulas</td>\n","    </tr>\n","    <tr>\n","      <th>Third Person</th>\n","      <td>ambulat</td>\n","      <td>ambulat</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cc7d665-339d-4fba-8ec5-f34900e035d9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-0cc7d665-339d-4fba-8ec5-f34900e035d9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-0cc7d665-339d-4fba-8ec5-f34900e035d9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-aeed95f2-d849-4e3b-be75-8b7e43a71d57\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-aeed95f2-d849-4e3b-be75-8b7e43a71d57')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-aeed95f2-d849-4e3b-be75-8b7e43a71d57 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["\n","\n","###### HELPER FUNCTIONS#######\n","\n","\n","def path_to_folder(path):\n","    # converts non folders into folders\n","    #aka just adds a slash to the end, if you dont give it a folder then it will probably produce an errror\n","    root = f\"{path}\"\n","    if root[-1] != \"/\":\n","        root+=\"/\"\n","    return root\n","\n","\n","def get_each_file_in_folder(path):\n","    # this only takes in a folder path\n","    #RETURNS A FILE LIST\n","    filelist =[]\n","    root =path_to_folder(path)\n","    for subdir, dirs, files in os.walk(root):\n","        for file in files:\n","            f = os.path.join(subdir, file)\n","            filelist.append(f)\n","    return filelist\n","\n","\n","\n","def get_each_csv_in_folder(path):\n","    # takes in Noun, Adjective,Verb\n","    #Returns All csv's in a folder\n","    filelist =[]\n","    root =path_to_folder(path)\n","    for subdir, dirs, files in os.walk(root):\n","        for file in files:\n","            f = os.path.join(subdir, file)\n","            if f.endswith('.csv'):\n","                filelist.append(f)\n","    return filelist\n","\n","def get_each_df_in_folder(path):\n","    #Takes in a folder\n","    #Gets every dataframe in a folder\n","    dflist =[]\n","    root =path_to_folder(path)\n","    for f in get_each_csv_in_folder(path):\n","        df = pd.read_csv(f).fillna('')\n","\n","        dflist.append(df)\n","    return dflist\n","'''\n","def get_greek_df(path):\n","    # takes in Noun, Adjective,Verb\n","    #print(\"test\")\n","    dflist =[]\n","    root =to_folder(path)\n","    for f in get_each_csv_in(path):\n","        df = pd.read_csv(f)\n","        greekdf=GreekCsv(f,df)\n","        dflist.append(greekdf)\n","    return dflist\n","'''\n","# get greek words from dictionary\n","def get_words_from_dict(path):\n","    wordlist = []\n","    for df in get_each_df_in_folder(path):\n","        for i in df.index:\n","             wordlist.append(df.iloc[i,1])\n","    return wordlist\n","\n","#reads all csv and intializes all words into variations\n","def init_variations(path):\n","    # takes in a folder path\n","    # goes though every word in a dictionary and generates the variations of that word\n","    for word in get_words_from_dict(path):\n","        #display(df.iloc[i,1])\n","        generate_variations(word)\n","\n","def match_words_in_folder(searchpath,wordpath):\n","#takes in a search path and a word path it will read a word from the word path and then find all\n","#files in each folder that contains that word\n","    filelist = []\n","    searchpath= path_to_folder(searchpath)\n","    for word in get_words_from_dict(wordpath):\n","        if(os.path.exists(searchpath + word)):\n","            filelist.append(get_each_file_in_folder(searchpath + word))\n","    flat_list = [item for sublist in filelist for item in sublist]\n","    return flat_list\n","\n","def get_each_df_from_path_list(filelist):\n","    #takes in a list of csv paths and outputs a list of dataframes\n","    dflist=[]\n","    for file in filelist:\n","        df = pd.read_csv(file).fillna('')\n","        dflist.append(df)\n","    return dflist\n","\n","def display_list(list):\n","# takes in a list of dataframes and displays the data\n","    for i in list:\n","        display(i)\n","\n","def display_all():\n","    init_variations(\"dictionary/\")\n","    matchedwords= match_words_in_folder(\"variations/\")\n","    display_list(get_each_df_from_path_list(matchedwords))\n","\n","\n","\n","\n","def get_indices(df,words):\n","    # returns a tuple of 2 lists that contain the row indexes and column inexes for each column and row that this word is present in\n","    # convert to take in a list of words\n","    columns=[]\n","    rows = []\n","    for columnIndex in range(df.shape[1]):\n","        if(isinstance(words, list)):\n","            for word in words:\n","                rowIndex = df.index[df.iloc[:, columnIndex].str.contains(word)].tolist()\n","                if(rowIndex):\n","                    columns.append(columnIndex)\n","                    rows.append(rowIndex)\n","        else:\n","            rowIndex = df.index[df.iloc[:, columnIndex].str.contains(words)].tolist()\n","            if(rowIndex):\n","                columns.append(columnIndex)\n","                rows.append(rowIndex)\n","\n","            #remove duplicates\n","\n","    rows = [item for sublist in rows for item in sublist]\n","    rows = [*set(rows)]\n","    #print(\"COLUMNINDEXES \",columns)\n","    #print(\"ROWINDEXES \",rows)\n","    return (rows,columns)\n","\n","\n","def get_items(dflist,row_words,column_words=''):\n","    # takes in a tuple of index arrays\n","    # empty strings are used to indicate not input standard input is either a string or array of strings\n","    dflist2=[]\n","    if (row_words=='' and column_words==''):\n","            print('please put a word to search in columns or rows')\n","            return []\n","\n","    for df in dflist:#\n","        row_indexes = get_indices(df,row_words)[0]\n","        column_indexes = get_indices(df,column_words)[1]\n","        if (row_words=='' and column_words!=''):\n","            df2 =df.iloc[:,column_indexes]\n","        if (row_words!='' and  column_words==''):\n","            df2 =df.iloc[row_indexes]\n","\n","        df2 =df.iloc[row_indexes,column_indexes]\n","\n","        dflist2.append(df2)\n","    return dflist2\n","\n","# a function that checks if a string contains a number\n","def contains_number(string):\n","    return any(char.isdigit() for char in string)\n","\n","\n","# not activley used will generate every possible combination kinda slow\n","def generate_all_accusative_scentences(verbs,nouns,sentiment):\n","    # takes in a verb dflist and noundf list aswell as asentiment\n","    sentiment = sentiment.lower()\n","    with open(f\"latin_accusative_{sentiment}.txt.atepc\",\"w+\",encoding=\"utf-8\") as f:\n","        for verbdf in verbs:\n","            for column_index in range(1,3):\n","                for row_index in range(verbdf.shape[0]):\n","                    verb =verbdf.iat[row_index,column_index]\n","                    if(verb=='' or contains_number(verb)):\n","                        continue\n","                    for noundf in nouns:\n","                        for column_index2 in range(1,noundf.shape[1]):\n","                            for row_index2 in range(noundf.shape[0]):\n","                                noun =noundf.iat[row_index2,column_index2]\n","\n","                                if(noun== '' or contains_number(noun)):\n","                                    continue\n","                                for word in verb.split(\" \"):\n","                                    f.write(word+\" O -100\\n\")\n","                                    #print(word+\" O -999\")\n","                                for i,word in enumerate(noun.split(\" \")):\n","                                    if(sentiment ==\"positive\" ):\n","                                        if(i==0):\n","                                            f.write(word +\" B-ASP Positive\\n\")\n","                                            #print(word +\" B-ASP Positive\")\n","                                        else:\n","                                            f.write(word +\" I-ASP Positive\\n\")\n","                                            #print(word +\" I-ASP Positive\")\n","                                    elif(sentiment ==\"negative\" ):\n","                                        if(i==0):\n","                                            f.write(word +\" B-ASP Negative\\n\")\n","                                            #print(word +\" B-ASP Negative\")\n","                                        else:\n","                                            f.write(word +\" I-ASP Negative\\n\")\n","                                            #print(word +\" B-ASP Negative\")\n","                                f.write(\". O -100\\n\\n\")\n","                                #print(\". O -999\")\n","\n","# WHATS MAINLY USED FOR NOW    -------------------------------------------------------------------------\n","def generate_random_accusative_dataset(verbs,nouns,sentiment,num):\n","    # takes in a verb dflist and noundf list aswell as asentiment\n","    # takes in number of examples to output\n","    #randomly selects a verb noun pair\n","    with open(f\"latin_accusative_{sentiment}.txt.atepc\",\"w+\",encoding=\"utf-8\") as f:\n","        random_verb_dfs = random.choices(verbs,k=num)\n","        random_noun_dfs = random.choices(nouns,k=num)\n","        #\n","        i = 0\n","        num_tries=0\n","        while i <num:\n","            random_verb_row = random.randrange(0,random_verb_dfs[i].shape[0])\n","            random_verb_column = random.randrange(1,random_verb_dfs[i].shape[1])\n","            random_noun_row = random.randrange(0,random_noun_dfs[i].shape[0])\n","            random_noun_column = random.randrange(1,random_noun_dfs[i].shape[1])\n","\n","            verb =strip_accents_and_lowercase(\n","            random_verb_dfs[i].iat[ random_verb_row,random_verb_column])\n","\n","            noun =strip_accents_and_lowercase(random_noun_dfs[i].iat[ random_noun_row,random_noun_column])\n","            #print(verb)\n","            if((noun== '' or contains_number(noun)or\n","            verb=='' or contains_number(verb))\n","            and num_tries<20 ):\n","                print(\"COULDNT FIND NUMBER OF TRIES:\", num_tries)\n","                num_tries+=1\n","                continue\n","            num_tries = 0\n","            f.write(\"[ O -999\\n\")# add brackets\n","            for word in verb.split(\" \"):\n","                f.write(word+\" O -999\\n\")\n","            for j,word in enumerate(noun.split(\" \")):\n","                if(sentiment ==\"positive\" ):\n","                    if(j==0):\n","                        f.write(word +\" B-ASP Positive\\n\")\n","                        #print(word +\" B-ASP Positive\")\n","                    else:\n","                        f.write(word +\" I-ASP Positive\\n\")\n","                        #print(word +\" I-ASP Positive\")\n","                elif(sentiment ==\"negative\" ):\n","                    if(j==0):\n","                        f.write(word +\" B-ASP Negative\\n\")\n","                        #print(word +\" B-ASP Negative\")\n","                    else:\n","                        f.write(word +\" I-ASP Negative\\n\")\n","                        #print(word +\" B-ASP Negative\")\n","            f.write(\"] O -999\\n\\n\")\n","            #randomly select item or randomly select row and column\n","            #check if its valid if its not randomly get another pai\n","            i+=1\n","\n","#WHATS MAINLY USED FOR NOW\n","def generate_random_nominative_dataset(noun_dfs,verb_dfs,adjective_dfs,sentiment,num):\n","    # takes in a verb dflist and noundf list aswell as asentiment\n","    # takes in number of examples to output\n","    #randomly selects a verb noun pair\n","    #mathc linking plurality\n","    with open(f\"latin_nominative_{sentiment}.txt.atepc\",\"w+\",encoding=\"utf-8\") as f:\n","        random_noun_dfs = random.choices(noun_dfs,k=num)\n","        if(isinstance(verb_dfs, list)):\n","            random_verb_dfs = random.choices(verb_dfs,k=num)\n","        random_adjective_dfs = random.choices(adjective_dfs,k=num)\n","        gramatical_number=[\"1 sg\",\"2 sg\",\"3 sg\",\"1 pl\",\"2 pl\",\"3 pl\"]\n","\n","        #\n","        i = 0\n","        num_tries=0\n","        while i <num:\n","            random_gramatical_number = random.choice(gramatical_number)\n","\n","\n","            random_noun_row = random.randrange(0,random_noun_dfs[i].shape[0])\n","            random_noun_column = random.randrange(1,random_noun_dfs[i].shape[1])\n","            #random_noun_column = get_indices(random_noun_dfs[i],random_gramatical_number)\n","\n","            #random_verb_row = get_indices(random_verb_dfs[i],random_gramatical_number)\n","            if(isinstance(verb_dfs, list)):\n","                random_verb_row = random.randrange(0,random_verb_dfs[i].shape[0])\n","                random_verb_column = random.randrange(1,random_verb_dfs[i].shape[1])\n","\n","            random_adjective_row = random.randrange(0,random_adjective_dfs[i].shape[0])\n","            random_adjective_column = random.randrange(1,random_adjective_dfs[i].shape[1])\n","\n","            noun =strip_accents_and_lowercase(\n","            random_noun_dfs[i].iat[ random_noun_row,random_noun_column])\n","\n","            if(isinstance(verb_dfs, list)):\n","                verb =random_verb_dfs[i].iat[ random_verb_row,random_verb_column]\n","            else:\n","                verb =verb_dfs\n","\n","            adjective =strip_accents_and_lowercase(\n","            random_adjective_dfs[i].iat[ random_adjective_row,random_adjective_column])\n","\n","            #print(verb)\n","            if((noun== '' or contains_number(noun)or\n","            verb=='' or contains_number(verb)or\n","            adjective=='' or contains_number(adjective))\n","            and num_tries<20 ):\n","                print(\"COULDNT FIND NUMBER OF TRIES:\", num_tries)\n","                num_tries+=1\n","                continue\n","            num_tries = 0\n","            f.write(\"[ O -999\\n\")# add brackets so that stuff doesnt break when put into the\n","            for j,word in enumerate(noun.split(\" \")):\n","                if(sentiment ==\"positive\" ):\n","                    if(j==0):\n","                        f.write(word +\" B-ASP Positive\\n\")\n","                        #print(word +\" B-ASP Positive\")\n","                    else:\n","                        f.write(word +\" I-ASP Positive\\n\")\n","                        #print(word +\" I-ASP Positive\")\n","                elif(sentiment ==\"negative\" ):\n","                    if(j==0):\n","                        f.write(word +\" B-ASP Negative\\n\")\n","                        #print(word +\" B-ASP Negative\")\n","                    else:\n","                        f.write(word +\" I-ASP Negative\\n\")\n","                        #print(word +\" B-ASP Negative\")\n","            for word in verb.split(\" \"):\n","                f.write(word+\" O -999\\n\")\n","            for word in adjective.split(\" \"):\n","                f.write(word+\" O -999\\n\")\n","            f.write(\"] O -999\\n\\n\")\n","            #randomly select item or randomly select row and column\n","            #check if its valid if its not randomly get another pai\n","            i+=1\n","# FUNCTION GETS A RANDOM SAMPLE OUTPUTS AN ARRAY OF STUFF\n","def random_sample(path,num):\n","#READS A RANDOM SAMPLE FROM A FILE AND APPENDS IT TO AN ARRAY\n","    sample = []\n","    with open(path,\"r\",encoding=\"utf-8\") as f:\n","\n","        data = f.read().split('\\n\\n')\n","        for i in range (num):\n","            sample.append(random.choice(data))\n","\n","        # read untill double new line\n","        # append string to array\n","    return sample\n","\n","def sample_to_dataset(sample,name):\n","# ideally every entry inside of the array should be a complete dataset entry ready to be outputted directly into a file one after another\n","    with open(name,\"+w\",encoding=\"utf-8\") as f:\n","        for s in(sample):\n","            f.write(f\"{s}\\n\\n\")\n","\n","def shuffle_datasets(paths,type='test',num=2000):\n","#shuffle all datasets in the paths given to it and then outputs it into a new file\n","    combined = []\n","    for p in paths:\n","        pos =random_sample(f\"{p[0]}\",int(num/(2*len(paths))))\n","        neg =random_sample(f\"{p[1]}\",int(num/(2*len(paths))))\n","        combined += pos+neg\n","    random.shuffle(combined)\n","    sample_to_dataset(combined,f\"latin.{type}.txt.atepc\")\n","\n","\n","\n","\n","#nomitiv\n","#negation\n","#conjunction\n","\n"],"metadata":{"id":"PgyQebji2VQO","executionInfo":{"status":"ok","timestamp":1695918779042,"user_tz":240,"elapsed":38,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["#get_each_file_in(\"dictionary/nouns\")\n","#get_each_csv_in(\"dictionary/nouns\")\n","#init_variations_in(\"dictionary/nouns\")\n","#matchedwords= match_words_in(\"variations/noun\",\"dictionary/nouns\")\n","\n","# THIS FUNCTION PIECES TOGETHER A RANDOM SAMPLE OF DECLINED WORDS TO CREATE A ACCUSATIVE SCENTENCE\n","def test_create_accusative(init=False,randomize=True,debug=False):\n","    # INIIT VARAIATIONS CREATES THE GREEK VARIATIONS FOLDER\n","    if (init==True or not os.path.exists(\"latin_variations_new/\")):\n","        init_variations(\"latin_dictionary/nouns\")\n","        init_variations(\"latin_dictionary/verbs/actionverbs/negative\")\n","        init_variations(\"latin_dictionary/verbs/actionverbs/positive\")\n","        #init_variations_in(\"greek_dictionary/adjectives/positive\")\n","        #init_variations_in(\"greek_dictionary/adjectives/negative\")\n","\n","\n","    #'''\n","    # searches the words in the greek varations folder for nouns\n","    matchednouns= match_words_in_folder(\"latin_variations_new/noun\",\"latin_dictionary/nouns\")\n","\n","    #display_list(get_accustive_nouns(path_to_df(matchednouns)))\n","\n","\n","    matchedverbs_pos= match_words_in_folder(\"latin_variations_new/verb\",\"latin_dictionary/verbs/actionverbs/positive\")\n","    print(matchedverbs_pos)\n","    matchedverbs_neg= match_words_in_folder(\"latin_variations_new/verb\",\"latin_dictionary/verbs/actionverbs/negative\")\n","    print(matchedverbs_neg)\n","    ##display_list(path_to_df(matchedwords))\n","\n","    #display_list(get_present_singular_verbs(path_to_df(matchedverbs_neg)))\n","\n","    #if debug:display_list(path_to_df(matchedverbs_pos))\n","    #verbs_pos=get_present_singular_verbs(path_to_df(matchedverbs_pos))\n","    verbs_pos = get_items(get_each_df_from_path_list(matchedverbs_pos),\"Third Person\", \"Singular\")\n","   # print(f\"get_each_df_from_path_list {get_each_df_from_path_list(matchedverbs_pos)}\")\n","    print(f\"Verbs_Pos {verbs_pos}\")\n","    verbs_neg=get_items(get_each_df_from_path_list(matchedverbs_neg),\"Third Person\",\"Singular\")\n","   # print(f\"get_each_df_from_path_list {get_each_df_from_path_list(matchedverbs_neg)}\")\n","    print(f\"Verbs_neg {verbs_neg}\")\n","    #nouns=get_accustive_nouns(path_to_df(matchednouns))\n","    #nouns=return_rows_that_contain(['accusative','nominative'],path_to_df(matchednouns))\n","    nouns=get_items(get_each_df_from_path_list(matchednouns),'Accusative')\n","    #display(nouns)\n","    if debug:display_list(nouns)\n","    if debug:display_list(verbs_pos)\n","    if(randomize == True):\n","\n","      print('test')\n","      generate_random_accusative_dataset(verbs_pos,nouns,\"positive\",1000)\n","      generate_random_accusative_dataset(verbs_neg,nouns,\"negative\",1000)\n","        #generate_all_accusative_scentences(verbs_pos,nouns,\"positive\")\n","        #generate_all_accusative_scentences(verbs_neg,nouns,\"negative\")\n","      '''\n","      neg =random_sample(\"accusative_negative.txt.atepc\",1000)\n","      pos=random_sample(\"accusative_positive.txt.atepc\",1000)\n","      combined = neg+pos\n","      random.shuffle(combined)\n","      sample_to_dataset(combined,\"greek.test.txt.atepc\")\n","\n","\n","      generate_random_accusative_dataset(verbs_pos,nouns,\"positive\",1000)\n","      generate_random_accusative_dataset(verbs_neg,nouns,\"negative\",1000)\n","\n","      neg2 =random_sample(\"accusative_negative.txt.atepc\",1000)\n","      pos2=random_sample(\"accusative_positive.txt.atepc\",1000)\n","      combined2 = neg2+pos2\n","      random.shuffle(combined2)\n","      sample_to_dataset(combined2,\"greek.train.txt.atepc\")\n","      '''\n","    else:\n","      generate_all_accusative_scentences(verbs_pos,nouns,\"positive\")\n","      generate_all_accusative_scentences(verbs_neg,nouns,\"negative\")\n","\n","\n","#---------------------------\n","# THIS FUNCTION PIECES TOGETHER A RANDOM SAMPLE OF DECLINED WORDS TO CREATE A NOMINATIVCE SCENTENCE\n","def test_create_nominative(init=False,randomize=True,debug=False):\n","    if (init==True or not os.path.exists(\"latin_variations_new/\")):\n","        init_variations(\"latin_dictionary/nouns\")\n","        init_variations(\"latin_dictionary/verbs/linkingverbs/\")\n","        init_variations(\"latin_dictionary/adjectives/positive\")\n","        init_variations(\"latin_dictionary/adjectives/negative\")\n","\n","\n","    #'''\n","\n","    matchednouns= match_words_in_folder(\"latin_variations_new/noun\",\"latin_dictionary/nouns\")\n","\n","    #display_list(get_accustive_nouns(path_to_df(matchednouns)))\n","\n","\n","    matchedverbs= match_words_in_folder(\"latin_variations_new/verb\",\"latin_dictionary/verbs/linkingverbs/\")\n","    matched_adj_pos= match_words_in_folder(\"latin_variations_new/adjective\",\"latin_dictionary/adjectives/positive\")\n","    matched_adj_neg= match_words_in_folder(\"latin_variations_new/adjective\",\"latin_dictionary/adjectives/negative\")\n","\n","\n","    ##display_list(path_to_df(matchedwords))\n","\n","    #display_list(get_present_singular_verbs(path_to_df(matchedverbs_neg)))\n","\n","    #if debug:display_list(path_to_df(matchedverbs_pos))\n","    #verbs_pos=get_present_singular_verbs(path_to_df(matchedverbs_pos))\n","    nouns=get_items(get_each_df_from_path_list(matchednouns),'nominative')\n","    #verbs=get_items(path_to_df(matchedverbs),[\"sg\",\"pl\"],\"present\")\n","    #display_list(path_to_df(matchedverbs))\n","    #verbs=get_items(path_to_df(matchedverbs),\"infinitive\",\"active\")\n","    verbs= strip_accents_and_lowercase(\"sum\")\n","\n","    adjectives_pos=get_items(get_each_df_from_path_list(matched_adj_pos),\"nominative\")\n","    adjectives_neg=get_items(get_each_df_from_path_list(matched_adj_neg),\"nominative\")\n","    #nouns=get_accustive_nouns(path_to_df(matchednouns))\n","    #nouns=return_rows_that_contain(['accusative','nominative'],path_to_df(matchednouns))\n","\n","    #display(nouns)\n","    #if debug:display_list(nouns)\n","    #if debug:display_list(verbs)\n","    #if debug:display_list(adjectives_pos)\n","    if(randomize == True):\n","        generate_random_nominative_dataset(nouns,verbs,adjectives_pos,\"positive\",1000)\n","        generate_random_nominative_dataset(nouns,verbs,adjectives_neg,\"negative\",1000)\n","        #generate_all_accusative_scentences(verbs_pos,nouns,\"positive\")\n","        #generate_all_accusative_scentences(verbs_neg,nouns,\"negative\")\n","        '''\n","        neg =random_sample(\"nominative_negative.txt.atepc\",1000)\n","        pos=random_sample(\"nominative_positive.txt.atepc\",1000)\n","        combined = neg+pos\n","        random.shuffle(combined)\n","        sample_to_dataset(combined,\"greek.test.txt.atepc\")\n","        '''\n","        '''\n","        generate_random_nominative_dataset(nouns,verbs,adjectives_pos,\"positive\",1000)\n","        generate_random_nominative_dataset(nouns,verbs,adjectives_neg,\"negative\",1000)\n","        #generate_all_accusative_scentences(verbs_pos,nouns,\"positive\")\n","        #generate_all_accusative_scentences(verbs_neg,nouns,\"negative\")\n","\n","        neg =random_sample(\"nominative_negative.txt.atepc\",1000)\n","        pos=random_sample(\"nominative_positive.txt.atepc\",1000)\n","        combined = neg+pos\n","        random.shuffle(combined)\n","        sample_to_dataset(combined,\"greek.train.txt.atepc\")\n","        '''\n","\n","    else:\n","        pass\n","        #generate_all_accusative_scentences(verbs_pos,nouns,\"positive\")\n","        #generate_all_accusative_scentences(verbs_neg,nouns,\"negative\")\n"],"metadata":{"id":"dyiEVc3o2Yw5","executionInfo":{"status":"ok","timestamp":1695918779043,"user_tz":240,"elapsed":36,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# TEST GENERATE IS A FUNCTION THAT CREATE\n","def testGenerate(filename ='test', initialize = False):\n","    # init is a parmeter that generates all of the declension csv's\n","    #right now its a little innefficiant if the csv's are already initialized then you do not need to initialize so you can set to false and it will generate almost immediatley\n","     test_create_accusative(\n","     init=initialize,\n","     randomize=True,debug=True)\n","\n","     test_create_nominative(\n","     init = initialize,\n","     randomize=True,debug=True)\n","\n","     dataset_paths=[\n","          [\"latin_accusative_positive.txt.atepc\",\"latin_accusative_negative.txt.atepc\"],\n","          [\"latin_nominative_positive.txt.atepc\",\"latin_nominative_negative.txt.atepc\"]\n","          ]\n","     shuffle_datasets(dataset_paths,type=filename,num= 2000)\n","\n","testGenerate(filename ='test',initialize = False) # set initialize to false if this is not your first time running for it to generate fast\n","testGenerate(filename ='train',initialize = False)\n","\n","#this generates atepc files just open in a text editor download a text editor plugin for google drive to make it easier\n"],"metadata":{"id":"AtfZjecW3Ea-","colab":{"base_uri":"https://localhost:8080/","height":477},"executionInfo":{"status":"error","timestamp":1695918784738,"user_tz":240,"elapsed":5730,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}},"outputId":"bbe52f08-02df-4987-e4be-e0bcb855f24f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[]\n","[]\n","Verbs_Pos []\n","Verbs_neg []\n","test\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-e44693aa4a05>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m      \u001b[0mshuffle_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtestGenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitialize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# set initialize to false if this is not your first time running for it to generate fast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtestGenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitialize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-e44693aa4a05>\u001b[0m in \u001b[0;36mtestGenerate\u001b[0;34m(filename, initialize)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# init is a parmeter that generates all of the declension csv's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#right now its a little innefficiant if the csv's are already initialized then you do not need to initialize so you can set to false and it will generate almost immediatley\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m      test_create_accusative(\n\u001b[0m\u001b[1;32m      6\u001b[0m      \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m      randomize=True,debug=True)\n","\u001b[0;32m<ipython-input-7-862be2a3c097>\u001b[0m in \u001b[0;36mtest_create_accusative\u001b[0;34m(init, randomize, debug)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       \u001b[0mgenerate_random_accusative_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbs_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"positive\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m       \u001b[0mgenerate_random_accusative_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbs_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"negative\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#generate_all_accusative_scentences(verbs_pos,nouns,\"positive\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-6ce9fae3f864>\u001b[0m in \u001b[0;36mgenerate_random_accusative_dataset\u001b[0;34m(verbs, nouns, sentiment, num)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m#randomly selects a verb noun pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"latin_accusative_{sentiment}.txt.atepc\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mrandom_verb_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0mrandom_noun_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnouns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mfloor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_floor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.0\u001b[0m    \u001b[0;31m# convert to float for a small speed improvement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mfloor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_floor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.0\u001b[0m    \u001b[0;31m# convert to float for a small speed improvement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["pd.read_csv(\"latin_variations_new/verb/diligo/_conjugation.csv\")"],"metadata":{"id":"PeC2lFjU5unN","executionInfo":{"status":"aborted","timestamp":1695918784740,"user_tz":240,"elapsed":6,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.read_csv(\"latin_variations_new/noun/aegyptus/_declension.csv\")"],"metadata":{"id":"0cIbjshTZNOA","executionInfo":{"status":"aborted","timestamp":1695918784740,"user_tz":240,"elapsed":6,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["matchednouns= match_words_in_folder(\"latin_variations_new/noun\",\"latin_dictionary/nouns\")\n","nouns=get_items(get_each_df_from_path_list(matchednouns), \"accusative\", \"\")\n","count = 0\n","for item in nouns:\n","  count += 1\n","print('\\n')\n","for item in nouns:\n","  try:\n","    print(item[\"Singular\"])\n","  except:\n","      print(item[\"1\"])\n","      print(item[\"2\"])\n","      print(item[\"3\"])\n","\n","  #print(item[\"Singular\"])\n","\n","#print(nouns[0].iloc[0])\n","#print(nouns[0].accusative['Singular'])\n","for match in lookup.match_word('casus'): # Match possible words\n","    print(lookup.get_dictionary_string(match))  # Start with the definition of the word\n","    for i in lookup.get_word_inflections(match,less=False):\n","        print(i) # Then print each inflection's meaning\n","        print()\n","\n"],"metadata":{"id":"Gb6GF-4WFLVf","executionInfo":{"status":"aborted","timestamp":1695918784740,"user_tz":240,"elapsed":5,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cltk\n","import pandas as pd\n","from cltk.morphology.lat import CollatinusDecliner\n","from cltk.data.fetch import FetchCorpus\n","corpus_importer = FetchCorpus(\"lat\")\n","print(corpus_importer.list_corpora)\n","corpus_importer.import_corpus(\"lat_models_cltk\")\n","\n","def decline(word):\n","  decliner = CollatinusDecliner()\n","  words = decliner.decline(word)\n","  return words\n","def make_df(word):\n","  gender = set()\n","  for i in range(0,len(word)):\n","    gender.add(word[i][1][6])\n","\n","\n","  # runs if the word has no gendered attributes\n","  if gender == {\"-\"}:\n","    singular_dict = {}\n","    plural_dict = {}\n","    for i in range(0,len(word)):\n","      # check if noun\n","      if word[i][1][0] == \"-\":\n","        # check if singular\n","        if word[i][1][2] == \"s\":\n","          if word[i][1][7] == \"n\":\n","            singular_dict[\"Nominative\"] = (word[i][0])\n","          elif word[i][1][7] == \"g\":\n","            singular_dict[\"Genative\"] = (word[i][0])\n","          elif word[i][1][7] == \"d\":\n","            singular_dict[\"Dative\"] = (word[i][0])\n","          elif word[i][1][7] == \"a\":\n","            singular_dict[\"Accusative\"] = (word[i][0])\n","          elif word[i][1][7] == \"b\":\n","            singular_dict[\"Ablative\"] = (word[i][0])\n","          elif word[i][1][7] == \"v\":\n","            singular_dict[\"Vocative\"] = (word[i][0])\n","        # check if plural\n","        elif word[i][1][2] == \"p\":\n","          if word[i][1][7] == \"n\":\n","            plural_dict[\"Nominative\"] = (word[i][0])\n","          elif word[i][1][7] == \"g\":\n","            plural_dict[\"Genative\"] = (word[i][0])\n","          elif word[i][1][7] == \"d\":\n","            plural_dict[\"Dative\"] = (word[i][0])\n","          elif word[i][1][7] == \"a\":\n","            plural_dict[\"Accusative\"] = (word[i][0])\n","          elif word[i][1][7] == \"b\":\n","            plural_dict[\"Ablative\"] = (word[i][0])\n","          elif word[i][1][7] == \"v\":\n","            plural_dict[\"Vocative\"] = (word[i][0])\n","\n","    dict = {\"Case\":[\"Nominative\", \"Genative\", \"Dative\", \"Accusative\", \"Ablative\", \"Vocative\"], \"Singular\":[], \"Plural\":[]}\n","    dict[\"Singular\"].append(singular_dict[\"Nominative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Genative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Dative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Accusative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Ablative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Vocative\"])\n","\n","    dict[\"Plural\"].append(plural_dict[\"Nominative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Genative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Dative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Accusative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Ablative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Vocative\"])\n","    df = pd.DataFrame(dict)\n","    return df\n","  else:\n","    print(\"not valid at the moment\")\n","    return -1\n","\n","\n","for item in decline(\"puella\"):\n","  print(item)\n","\n","word = decline(\"puella\")\n","print(word)\n","# formated by [word, (part of speech, person, number, tense, mood, voice, gender, case, degree)]\n","# https://github.com/bab2min/lamonpy\n","print(make_df(word))\n","\n","\n"],"metadata":{"id":"B1YlW80tR3La","executionInfo":{"status":"aborted","timestamp":1695918784740,"user_tz":240,"elapsed":5,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install cltk\n","import pandas as pd\n","from cltk.morphology.lat import CollatinusDecliner\n","from cltk.data.fetch import FetchCorpus\n","corpus_importer = FetchCorpus(\"lat\")\n","print(corpus_importer.list_corpora)\n","corpus_importer.import_corpus(\"lat_models_cltk\")\n","\n","def decline(word):\n","  decliner = CollatinusDecliner()\n","  words = decliner.decline(word)\n","  return words\n","\n","def fill_dicts(word):\n","    singular_dict = {}\n","    plural_dict = {}\n","    for i in range(0, len(word)):\n","\n","        # check if singular\n","        if word[i][1][2] == \"s\":\n","          if word[i][1][7] == \"n\":\n","            singular_dict[\"Nominative\"] = (word[i][0])\n","          elif word[i][1][7] == \"g\":\n","            singular_dict[\"Genative\"] = (word[i][0])\n","          elif word[i][1][7] == \"d\":\n","            singular_dict[\"Dative\"] = (word[i][0])\n","          elif word[i][1][7] == \"a\":\n","            singular_dict[\"Accusative\"] = (word[i][0])\n","          elif word[i][1][7] == \"b\":\n","            singular_dict[\"Ablative\"] = (word[i][0])\n","          elif word[i][1][7] == \"v\":\n","            singular_dict[\"Vocative\"] = (word[i][0])\n","        # check if plural\n","        elif word[i][1][2] == \"p\":\n","          if word[i][1][7] == \"n\":\n","            plural_dict[\"Nominative\"] = (word[i][0])\n","          elif word[i][1][7] == \"g\":\n","            plural_dict[\"Genative\"] = (word[i][0])\n","          elif word[i][1][7] == \"d\":\n","            plural_dict[\"Dative\"] = (word[i][0])\n","          elif word[i][1][7] == \"a\":\n","            plural_dict[\"Accusative\"] = (word[i][0])\n","          elif word[i][1][7] == \"b\":\n","            plural_dict[\"Ablative\"] = (word[i][0])\n","          elif word[i][1][7] == \"v\":\n","            plural_dict[\"Vocative\"] = (word[i][0])\n","    return singular_dict, plural_dict\n","\n","\n","\n","\n","def make_df(word):\n","  gender = set()\n","  for i in range(0,len(word)):\n","    gender.add(word[i][1][6])\n","  print(gender)\n","  # runs if the word has no gendered attributes\n","  if gender == {\"-\"}:\n","    singular_dict, plural_dict = fill_dicts(word)\n","    print(singular_dict, plural_dict)\n","    print()\n","    dict = {\"Case\":[\"Nominative\", \"Genative\", \"Dative\", \"Accusative\", \"Ablative\", \"Vocative\"], \"Singular\":[], \"Plural\":[]}\n","    dict[\"Singular\"].append(singular_dict[\"Nominative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Genative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Dative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Accusative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Ablative\"])\n","    dict[\"Singular\"].append(singular_dict[\"Vocative\"])\n","\n","    dict[\"Plural\"].append(plural_dict[\"Nominative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Genative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Dative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Accusative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Ablative\"])\n","    dict[\"Plural\"].append(plural_dict[\"Vocative\"])\n","    df = pd.DataFrame(dict)\n","    return df\n","\n","  elif gender == {'f','n','m'}:\n","    # fills each list with the respective words\n","    masculine, feminine, neutral = [], [], []\n","    for item in word:\n","      if item[1][6] == 'm':\n","        masculine.append(item)\n","      elif item[1][6] == 'f':\n","        feminine.append(item)\n","      else:\n","        neutral.append(item)\n","\n","  else:\n","    print(\"not valid at the moment\")\n","    return -1\n","\n","\n","# for item in decline(\"Romanus\"):\n","#   print(item)\n","\n","word = decline(\"Romanus\")\n","print(word)\n","print(type(word))\n","# formated by [word, (part of speech, person, number, tense, mood, voice, gender, case, degree)]\n","# https://github.com/bab2min/lamonpy\n","print(make_df(word))\n","print('hi')\n","\n"],"metadata":{"id":"Zv0ghnNeDm2v","executionInfo":{"status":"aborted","timestamp":1695918784740,"user_tz":240,"elapsed":5,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install cltk\n","import pandas as pd\n","from cltk.morphology.lat import CollatinusDecliner\n","from cltk.morphology.akk import get_bound_form\n","from cltk.morphology.akk import decline_noun\n","\n","from cltk.data.fetch import FetchCorpus\n","corpus_importer = FetchCorpus(\"lat\")\n","print(corpus_importer.list_corpora)\n","corpus_importer.import_corpus(\"lat_models_cltk\")\n","\n","# takes in a word as a string and uses the collatinus decliner to return\n","# the different forms of the word. returns words which is a list that contains\n","# tuples formated like so: [word, (part of speech, person, number, tense, mood, voice, gender, case, degree)]\n","def decline(word):\n","  decliner = CollatinusDecliner()\n","  words = decliner.decline(word)\n","  return words\n","\n","# takes in a declined word object (list of words in the format returned by the decliner function) and\n","# formats it so that each word is with the correct type (nominative, genative, etc.)\n","# currently only works with nouns that have one gender. needs to be expanded so it can make\n","# dictionaries for multiple genders and word types.\n","def fill_dicts(word, neutral = False, masc = False, fem = False):\n","    singular_dict = {}\n","    plural_dict = {}\n","    for i in range(0, len(word)):\n","\n","      #check if verb\n","      if word[i][1][0] == \"v\":\n","\n","        #check if singular verb\n","        if word[i][1][1:] == \"1spia---\":\n","          singular_dict[\"First Person\"] = (word[i][0])\n","        if word[i][1][1:] == \"2spia---\":\n","          singular_dict[\"Second Person\"] = (word[i][0])\n","        if word[i][1][1:] == \"3spia---\":\n","          singular_dict[\"Third Person\"] = (word[i][0])\n","        #check if plural verb\n","        if word[i][1][1:] == \"1ppia---\":\n","          plural_dict[\"First Person\"] = (word[i][0])\n","        if word[i][1][1:] == \"2ppia---\":\n","          plural_dict[\"Second Person\"] = (word[i][0])\n","        if word[i][1][1:] == \"3ppia---\":\n","          plural_dict[\"Third Person\"] = (word[i][0])\n","\n","      # check if singular\n","      elif word[i][1][0] == \"-\":\n","        if word[i][1][2] == \"s\":\n","          if word[i][1][7] == \"n\":\n","            singular_dict[\"Nominative\"] = (word[i][0])\n","          elif word[i][1][7] == \"g\":\n","            singular_dict[\"Genative\"] = (word[i][0])\n","          elif word[i][1][7] == \"d\":\n","            singular_dict[\"Dative\"] = (word[i][0])\n","          elif word[i][1][7] == \"a\":\n","            singular_dict[\"Accusative\"] = (word[i][0])\n","          elif word[i][1][7] == \"b\":\n","            singular_dict[\"Ablative\"] = (word[i][0])\n","          elif word[i][1][7] == \"v\":\n","            singular_dict[\"Vocative\"] = (word[i][0])\n","        # check if plural\n","        elif word[i][1][2] == \"p\":\n","          if word[i][1][7] == \"n\":\n","            plural_dict[\"Nominative\"] = (word[i][0])\n","          elif word[i][1][7] == \"g\":\n","            plural_dict[\"Genative\"] = (word[i][0])\n","          elif word[i][1][7] == \"d\":\n","            plural_dict[\"Dative\"] = (word[i][0])\n","          elif word[i][1][7] == \"a\":\n","            plural_dict[\"Accusative\"] = (word[i][0])\n","          elif word[i][1][7] == \"b\":\n","            plural_dict[\"Ablative\"] = (word[i][0])\n","          elif word[i][1][7] == \"v\":\n","            plural_dict[\"Vocative\"] = (word[i][0])\n","    return singular_dict, plural_dict\n","\n","\n","\n","\n","def make_df(word, noun = False):\n","\n","  gender = set()\n","  for i in range(0,len(word)):\n","    gender.add(word[i][1][6])\n","  # runs if the word has no gendered attributes\n","  #if '-' in gender or 'n' in gender:\n","  singular_dict, plural_dict = fill_dicts(word, True)\n","  print(singular_dict, plural_dict)\n","  print()\n","\n","  if word[1][1][0] == \"v\":\n","      dict_verb = {\"Person\": [\"First Person\", \"Second Person\", \"Third Person\"], \"Singular\":[], \"Plural\":[]}\n","      dict_verb[\"Singular\"].append(singular_dict[\"First Person\"])\n","      dict_verb[\"Singular\"].append(singular_dict[\"Second Person\"])\n","      dict_verb[\"Singular\"].append(singular_dict[\"Third Person\"])\n","      dict_verb[\"Plural\"].append(singular_dict[\"First Person\"])\n","      dict_verb[\"Plural\"].append(singular_dict[\"Second Person\"])\n","      dict_verb[\"Plural\"].append(singular_dict[\"Third Person\"])\n","      df = pd.DataFrame(dict_verb)\n","      df = df.set_index(\"Person\")\n","      return df\n","\n","  if word[1][1][0] == \"-\":\n","    dict_overall = {\"Case\":[\"Nominative\", \"Genative\", \"Dative\", \"Accusative\", \"Ablative\", \"Vocative\"], \"Singular\":[], \"Plural\":[]}\n","    dict_overall[\"Singular\"].append(singular_dict[\"Nominative\"])\n","    dict_overall[\"Singular\"].append(singular_dict[\"Genative\"])\n","    dict_overall[\"Singular\"].append(singular_dict[\"Dative\"])\n","    dict_overall[\"Singular\"].append(singular_dict[\"Accusative\"])\n","    dict_overall[\"Singular\"].append(singular_dict[\"Ablative\"])\n","    dict_overall[\"Singular\"].append(singular_dict[\"Vocative\"])\n","\n","    dict_overall[\"Plural\"].append(plural_dict[\"Nominative\"])\n","    dict_overall[\"Plural\"].append(plural_dict[\"Genative\"])\n","    dict_overall[\"Plural\"].append(plural_dict[\"Dative\"])\n","    dict_overall[\"Plural\"].append(plural_dict[\"Accusative\"])\n","    dict_overall[\"Plural\"].append(plural_dict[\"Ablative\"])\n","    dict_overall[\"Plural\"].append(plural_dict[\"Vocative\"])\n","    df = pd.DataFrame(dict_overall)\n","    return df\n","  if 'm' in gender:\n","        dict_masc = {\"Case\":[\"Nominative\", \"Genative\", \"Dative\", \"Accusative\", \"Ablative\", \"Vocative\"], \"Singular\":[], \"Plural\":[]}\n","\n","  else:\n","    print(\"not valid at the moment\")\n","    print(gender)\n","    return -1\n","\n","\n","for item in decline(\"ambulo\"):\n","  print(item)\n","\n","#print(make_df(word))\n","word = decline(\"rex\")\n","print(make_df(word))\n","df = make_df(word)\n","# formated by [word, (part of speech, person, number, tense, mood, voice, gender, case, degree)]\n","# https://github.com/bab2min/lamonpy\n","\n","# print('hi')\n","\n"],"metadata":{"id":"Oz7cn5WuJU3Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1695919399046,"user_tz":240,"elapsed":7953,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}},"outputId":"8d6dee06-5fc4-4a1e-bc69-a168721047c0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cltk in /usr/local/lib/python3.10/dist-packages (1.1.6)\n","Requirement already satisfied: PyYAML<7.0.0,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (6.0.1)\n","Requirement already satisfied: boltons<22.0.0,>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (21.0.0)\n","Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (4.3.2)\n","Requirement already satisfied: gitpython<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.1.37)\n","Requirement already satisfied: greek-accentuation<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.2.0)\n","Requirement already satisfied: nltk<4.0,>=3.7 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.8.1)\n","Requirement already satisfied: python-Levenshtein<0.13.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (0.12.2)\n","Requirement already satisfied: requests<3.0.0,>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (2.31.0)\n","Requirement already satisfied: scikit-learn<2.0.0,>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.2.2)\n","Requirement already satisfied: scipy<2.0.0,>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.11.2)\n","Requirement already satisfied: spacy<4.0.0,>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from cltk) (3.6.1)\n","Requirement already satisfied: stanza<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.5.1)\n","Requirement already satisfied: stringcase<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from cltk) (1.2.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from cltk) (4.66.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->cltk) (1.23.5)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.1.2->cltk) (6.4.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4.0,>=3.0->cltk) (4.0.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0,>=3.7->cltk) (2023.6.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein<0.13.0,>=0.12.0->cltk) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.22.0->cltk) (2023.7.22)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.0.2->cltk) (3.2.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (0.10.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (1.10.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.1.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.2.4->cltk) (3.3.0)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (2.8.0)\n","Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (3.20.3)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza<2.0.0,>=1.3.0->cltk) (2.0.1+cu118)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4.0,>=3.0->cltk) (5.0.1)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.2.4->cltk) (4.5.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.2.4->cltk) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.2.4->cltk) (0.1.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.1)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (3.27.4.1)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.2.4->cltk) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza<2.0.0,>=1.3.0->cltk) (1.3.0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:CLTK:Pulling latest 'lat_models_cltk' from 'https://github.com/cltk/lat_models_cltk.git'.\n"]},{"output_type":"stream","name":"stdout","text":["['lat_text_perseus', 'lat_treebank_perseus', 'lat_text_latin_library', 'phi5', 'phi7', 'latin_proper_names_cltk', 'lat_models_cltk', 'latin_pos_lemmata_cltk', 'latin_treebank_index_thomisticus', 'latin_lexica_perseus', 'latin_training_set_sentence_cltk', 'latin_word2vec_cltk', 'latin_text_antique_digiliblt', 'latin_text_corpus_grammaticorum_latinorum', 'latin_text_poeti_ditalia', 'lat_text_tesserae', 'cltk_lat_lewis_elementary_lexicon']\n","('ambulo', 'v1spia---')\n","('ambulas', 'v2spia---')\n","('ambulat', 'v3spia---')\n","('ambulamus', 'v1ppia---')\n","('ambulatis', 'v2ppia---')\n","('ambulant', 'v3ppia---')\n","('ambulabam', 'v1siia---')\n","('ambulabas', 'v2siia---')\n","('ambulabat', 'v3siia---')\n","('ambulabamus', 'v1piia---')\n","('ambulabatis', 'v2piia---')\n","('ambulabant', 'v3piia---')\n","('ambulabo', 'v1sfia---')\n","('ambulabis', 'v2sfia---')\n","('ambulabit', 'v3sfia---')\n","('ambulabimus', 'v1pfia---')\n","('ambulabitis', 'v2pfia---')\n","('ambulabunt', 'v3pfia---')\n","('ambulavi', 'v1sria---')\n","('ambulavisti', 'v2sria---')\n","('ambulavit', 'v3sria---')\n","('ambulavimus', 'v1pria---')\n","('ambulavistis', 'v2pria---')\n","('ambulaverunt', 'v3pria---')\n","('ambulavere', 'v3pria---')\n","('ambulaveram', 'v1slia---')\n","('ambulaveras', 'v2slia---')\n","('ambulaverat', 'v3slia---')\n","('ambulaveramus', 'v1plia---')\n","('ambulaveratis', 'v2plia---')\n","('ambulaverant', 'v3plia---')\n","('ambulavero', 'v1stia---')\n","('ambulaveris', 'v2stia---')\n","('ambulaverit', 'v3stia---')\n","('ambulaverimus', 'v1ptia---')\n","('ambulaveritis', 'v2ptia---')\n","('ambulaverint', 'v3ptia---')\n","('ambulem', 'v1spsa---')\n","('ambules', 'v2spsa---')\n","('ambulet', 'v3spsa---')\n","('ambulemus', 'v1ppsa---')\n","('ambuletis', 'v2ppsa---')\n","('ambulent', 'v3ppsa---')\n","('ambularem', 'v1sisa---')\n","('ambulares', 'v2sisa---')\n","('ambularet', 'v3sisa---')\n","('ambularemus', 'v1pisa---')\n","('ambularetis', 'v2pisa---')\n","('ambularent', 'v3pisa---')\n","('ambulaverim', 'v1srsa---')\n","('ambulaveris', 'v2srsa---')\n","('ambulaverit', 'v3srsa---')\n","('ambulaverimus', 'v1prsa---')\n","('ambulaveritis', 'v2prsa---')\n","('ambulaverint', 'v3prsa---')\n","('ambulavissem', 'v1slsa---')\n","('ambulavisses', 'v2slsa---')\n","('ambulavisset', 'v3slsa---')\n","('ambulavissemus', 'v1plsa---')\n","('ambulavissetis', 'v2plsa---')\n","('ambulavissent', 'v3plsa---')\n","('ambula', 'v2spma---')\n","('ambulate', 'v2ppma---')\n","('ambulato', 'v2sfma---')\n","('ambulato', 'v3sfma---')\n","('ambulatote', 'v2pfma---')\n","('ambulanto', 'v3pfma---')\n","('ambulare', 'v--pna---')\n","('ambulasse', 'v--rna---')\n","('ambulans', 'g-sppamn-')\n","('ambulans', 'g-sppamv-')\n","('ambulantem', 'g-sppama-')\n","('ambulantis', 'g-sppamg-')\n","('ambulanti', 'g-sppamd-')\n","('ambulante', 'g-sppamb-')\n","('ambulantes', 'g-pppamn-')\n","('ambulantes', 'g-pppamv-')\n","('ambulantes', 'g-pppama-')\n","('ambulantium', 'g-pppamg-')\n","('ambulantum', 'g-pppamg-')\n","('ambulantibus', 'g-pppamd-')\n","('ambulantibus', 'g-pppamb-')\n","('ambulans', 'g-sppafn-')\n","('ambulans', 'g-sppafv-')\n","('ambulantem', 'g-sppafa-')\n","('ambulantis', 'g-sppafg-')\n","('ambulanti', 'g-sppafd-')\n","('ambulante', 'g-sppafb-')\n","('ambulantes', 'g-pppafn-')\n","('ambulantes', 'g-pppafv-')\n","('ambulantes', 'g-pppafa-')\n","('ambulantium', 'g-pppafg-')\n","('ambulantum', 'g-pppafg-')\n","('ambulantibus', 'g-pppafd-')\n","('ambulantibus', 'g-pppafb-')\n","('ambulans', 'g-sppann-')\n","('ambulans', 'g-sppanv-')\n","('ambulans', 'g-sppana-')\n","('ambulantis', 'g-sppang-')\n","('ambulanti', 'g-sppand-')\n","('ambulante', 'g-sppanb-')\n","('ambulantia', 'g-pppann-')\n","('ambulantia', 'g-pppanv-')\n","('ambulantia', 'g-pppana-')\n","('ambulantium', 'g-pppang-')\n","('ambulantum', 'g-pppang-')\n","('ambulantibus', 'g-pppand-')\n","('ambulantibus', 'g-pppanb-')\n","('ambulaturus', 'g-sfpamn-')\n","('ambulature', 'g-sfpamv-')\n","('ambulaturum', 'g-sfpama-')\n","('ambulaturi', 'g-sfpamg-')\n","('ambulaturo', 'g-sfpamd-')\n","('ambulaturo', 'g-sfpamb-')\n","('ambulaturi', 'g-pfpamn-')\n","('ambulaturi', 'g-pfpamv-')\n","('ambulaturos', 'g-pfpama-')\n","('ambulaturorum', 'g-pfpamg-')\n","('ambulaturis', 'g-pfpamd-')\n","('ambulaturis', 'g-pfpamb-')\n","('ambulatura', 'g-sfpafn-')\n","('ambulatura', 'g-sfpafv-')\n","('ambulaturam', 'g-sfpafa-')\n","('ambulaturae', 'g-sfpafg-')\n","('ambulaturae', 'g-sfpafd-')\n","('ambulatura', 'g-sfpafb-')\n","('ambulaturae', 'g-pfpafn-')\n","('ambulaturae', 'g-pfpafv-')\n","('ambulaturas', 'g-pfpafa-')\n","('ambulaturarum', 'g-pfpafg-')\n","('ambulaturis', 'g-pfpafd-')\n","('ambulaturis', 'g-pfpafb-')\n","('ambulaturum', 'g-sfpann-')\n","('ambulaturum', 'g-sfpanv-')\n","('ambulaturum', 'g-sfpana-')\n","('ambulaturi', 'g-sfpang-')\n","('ambulaturo', 'g-sfpand-')\n","('ambulaturo', 'g-sfpanb-')\n","('ambulatura', 'g-pfpann-')\n","('ambulatura', 'g-pfpanv-')\n","('ambulatura', 'g-pfpana-')\n","('ambulaturorum', 'g-pfpang-')\n","('ambulaturis', 'g-pfpand-')\n","('ambulaturis', 'g-pfpanb-')\n","('ambulandum', '----g--a-')\n","('ambulandi', '----g--g-')\n","('ambulando', '----g--d-')\n","('ambulando', '----g--b-')\n","('ambulatum', '----u----')\n","('ambulatu', '----u----')\n","('ambulor', 'v1spip---')\n","('ambularis', 'v2spip---')\n","('ambulare', 'v2spip---')\n","('ambulatur', 'v3spip---')\n","('ambulamur', 'v1ppip---')\n","('ambulamini', 'v2ppip---')\n","('ambulantur', 'v3ppip---')\n","('ambulabar', 'v1siip---')\n","('ambulabaris', 'v2siip---')\n","('ambulabare', 'v2siip---')\n","('ambulabatur', 'v3siip---')\n","('ambulabamur', 'v1piip---')\n","('ambulabamini', 'v2piip---')\n","('ambulabantur', 'v3piip---')\n","('ambulabor', 'v1sfip---')\n","('ambulaberis', 'v2sfip---')\n","('ambulabere', 'v2sfip---')\n","('ambulabitur', 'v3sfip---')\n","('ambulabimur', 'v1pfip---')\n","('ambulabimini', 'v2pfip---')\n","('ambulabuntur', 'v3pfip---')\n","('ambuler', 'v1spsp---')\n","('ambuleris', 'v2spsp---')\n","('ambulere', 'v2spsp---')\n","('ambuletur', 'v3spsp---')\n","('ambulemur', 'v1ppsp---')\n","('ambulemini', 'v2ppsp---')\n","('ambulentur', 'v3ppsp---')\n","('ambularer', 'v1sisp---')\n","('ambulareris', 'v2sisp---')\n","('ambularere', 'v2sisp---')\n","('ambularetur', 'v3sisp---')\n","('ambularemur', 'v1pisp---')\n","('ambularemini', 'v2pisp---')\n","('ambularentur', 'v3pisp---')\n","('ambulare', 'v2spmp---')\n","('ambulamini', 'v2ppmp---')\n","('ambulator', 'v2sfmp---')\n","('ambulator', 'v3sfmp---')\n","('ambulantor', 'v3pfmp---')\n","('ambulari', 'v--pnp---')\n","('ambulatus', 'g-srppmn-')\n","('ambulate', 'g-srppmv-')\n","('ambulatum', 'g-srppma-')\n","('ambulati', 'g-srppmg-')\n","('ambulato', 'g-srppmd-')\n","('ambulato', 'g-srppmb-')\n","('ambulati', 'g-prppmn-')\n","('ambulati', 'g-prppmv-')\n","('ambulatos', 'g-prppma-')\n","('ambulatorum', 'g-prppmg-')\n","('ambulatis', 'g-prppmd-')\n","('ambulatis', 'g-prppmb-')\n","('ambulata', 'g-srppfn-')\n","('ambulata', 'g-srppfv-')\n","('ambulatam', 'g-srppfa-')\n","('ambulatae', 'g-srppfg-')\n","('ambulatae', 'g-srppfd-')\n","('ambulata', 'g-srppfb-')\n","('ambulatae', 'g-prppfn-')\n","('ambulatae', 'g-prppfv-')\n","('ambulatas', 'g-prppfa-')\n","('ambulatarum', 'g-prppfg-')\n","('ambulatis', 'g-prppfd-')\n","('ambulatis', 'g-prppfb-')\n","('ambulatum', 'g-srppnn-')\n","('ambulatum', 'g-srppnv-')\n","('ambulatum', 'g-srppna-')\n","('ambulati', 'g-srppng-')\n","('ambulato', 'g-srppnd-')\n","('ambulato', 'g-srppnb-')\n","('ambulata', 'g-prppnn-')\n","('ambulata', 'g-prppnv-')\n","('ambulata', 'g-prppna-')\n","('ambulatorum', 'g-prppng-')\n","('ambulatis', 'g-prppnd-')\n","('ambulatis', 'g-prppnb-')\n","('ambulandus', '--s-g-mn-')\n","('ambulande', '--s-g-mv-')\n","('ambulandum', '--s-g-ma-')\n","('ambulandi', '--s-g-mg-')\n","('ambulando', '--s-g-md-')\n","('ambulando', '--s-g-mb-')\n","('ambulandi', '--p-g-mn-')\n","('ambulandi', '--p-g-mv-')\n","('ambulandos', '--p-g-ma-')\n","('ambulandorum', '--p-g-mg-')\n","('ambulandis', '--p-g-md-')\n","('ambulandis', '--p-g-mb-')\n","('ambulanda', '--s-g-fn-')\n","('ambulanda', '--s-g-fv-')\n","('ambulandam', '--s-g-fa-')\n","('ambulandae', '--s-g-fg-')\n","('ambulandae', '--s-g-fd-')\n","('ambulanda', '--s-g-fb-')\n","('ambulandae', '--p-g-fn-')\n","('ambulandae', '--p-g-fv-')\n","('ambulandas', '--p-g-fa-')\n","('ambulandarum', '--p-g-fg-')\n","('ambulandis', '--p-g-fd-')\n","('ambulandis', '--p-g-fb-')\n","('ambulandum', '--s-g-nn-')\n","('ambulandum', '--s-g-nv-')\n","('ambulandum', '--s-g-na-')\n","('ambulandi', '--s-g-ng-')\n","('ambulando', '--s-g-nd-')\n","('ambulando', '--s-g-nb-')\n","('ambulanda', '--p-g-nn-')\n","('ambulanda', '--p-g-nv-')\n","('ambulanda', '--p-g-na-')\n","('ambulandorum', '--p-g-ng-')\n","('ambulandis', '--p-g-nd-')\n","('ambulandis', '--p-g-nb-')\n","{'Nominative': 'rex', 'Vocative': 'rex', 'Accusative': 'regem', 'Genative': 'regis', 'Dative': 'regi', 'Ablative': 'rege'} {'Nominative': 'reges', 'Vocative': 'reges', 'Accusative': 'reges', 'Genative': 'regum', 'Dative': 'regibus', 'Ablative': 'regibus'}\n","\n","         Case Singular   Plural\n","0  Nominative      rex    reges\n","1    Genative    regis    regum\n","2      Dative     regi  regibus\n","3  Accusative    regem    reges\n","4    Ablative     rege  regibus\n","5    Vocative      rex    reges\n","{'Nominative': 'rex', 'Vocative': 'rex', 'Accusative': 'regem', 'Genative': 'regis', 'Dative': 'regi', 'Ablative': 'rege'} {'Nominative': 'reges', 'Vocative': 'reges', 'Accusative': 'reges', 'Genative': 'regum', 'Dative': 'regibus', 'Ablative': 'regibus'}\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ql5zssS3O0V8","executionInfo":{"status":"aborted","timestamp":1695918784741,"user_tz":240,"elapsed":6,"user":{"displayName":"Jordan Y","userId":"03897633224741231742"}}},"execution_count":null,"outputs":[]}]}